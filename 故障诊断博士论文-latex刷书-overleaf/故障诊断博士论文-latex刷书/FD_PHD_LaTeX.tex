% This LaTeX document needs to be compiled with XeLaTeX.
\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\graphicspath{ {./images/} }
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, filecolor=magenta, urlcolor=cyan,}
\urlstyle{same}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{caption}
\usepackage{bbold}
\usepackage{fixltx2e}
\usepackage[fallback]{xeCJK}
\usepackage{polyglossia}
\usepackage{fontspec}
\usepackage{newunicodechar}
% % 加载你自己的笔记宏包，这一版会导致图像不显示，所以弃用
% \usepackage{notebook}  % 直接用 .sty 名字，不用写扩展名
% 加载你自己的笔记宏包
\usepackage{notes}  % 直接用 .sty 名字，不用写扩展名
\IfFontExistsTF{Noto Serif CJK TC}
{\setCJKmainfont{Noto Serif CJK TC}}
{\IfFontExistsTF{STSong}
  {\setCJKmainfont{STSong}}
  {\IfFontExistsTF{Droid Sans Fallback}
    {\setCJKmainfont{Droid Sans Fallback}}
    {\setCJKmainfont{SimSun}}
}}

\setmainlanguage{english}
\IfFontExistsTF{CMU Serif}
{\setmainfont{CMU Serif}}
{\IfFontExistsTF{DejaVu Sans}
  {\setmainfont{DejaVu Sans}}
  {\setmainfont{Georgia}}
}

\title{Fault Diagnosis for Uncertain Systems in Closed Loop }

\author{Applied to Semiconductor Equipment}
\date{}


%New command to display footnote whose markers will always be hidden
\let\svthefootnote\thefootnote
\newcommand\blfootnotetext[1]{%
  \let\thefootnote\relax\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \let\thefootnote\svthefootnote%
}

%Overriding the \footnotetext command to hide the marker if its value is `0`
\let\svfootnotetext\footnotetext
\renewcommand\footnotetext[2][?]{%
  \if\relax#1\relax%
    \ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
  \else%
    \if?#1\ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
    \else\svfootnotetext[#1]{#2}\fi%
  \fi
}

\newunicodechar{ı}{\ifmmode\imath\else{$\imath$}\fi}

\begin{document}
\maketitle
\captionsetup{singlelinecheck=false}
Applied to Semiconductor Equipment\\
\includegraphics[max width=\textwidth, center]{2025_10_13_d0faf158053d0f7500c0g-001}

\section*{Fault Diagnosis for Uncertain Systems in Closed Loop \\
 Applied to Semiconductor Equipment }


\section*{disc}
The author successfully completed the educational program of the Graduate School of the Dutch Institute for Systems and Control (DISC).

\section*{ASML}
The work described in this thesis has been carried out at the Eindhoven University of Technology, and was supported and funded by ASML, Veldhoven, The Netherlands.

A catalogue record is available from the Eindhoven University of Technology Library.\\
ISBN: 978-90-386-6146-9\\
Cover design: Koen Classens. Render back cover of TWINSCAN EXE ©ASML.\\
Reproduction: Ipskamp printing, Enschede, the Netherlands.\\
(C) 2024 by Koen Classens. All rights reserved.

\section*{Fault Diagnosis for Uncertain Systems in Closed Loop }


\section*{PROEFSCHRIFT}
ter verkrijging van de graad van doctor aan de Technische Universiteit Eindhoven, op gezag van de rector magnificus prof.dr. S.K. Lenaerts, voor een commissie aangewezen door het College voor Promoties, in het openbaar te verdedigen op vrijdag 4 oktober 2024 om 13:30 uur\\
door

Koen Hendrik Johan Classens\\
geboren te Venray

Dit proefschrift is goedgekeurd door de promotoren en de samenstelling van de promotiecommissie is als volgt:\\
voorzitter: \href{http://prof.dr.ir}{prof.dr.ir}. P.D. Anderson\\
promotoren: \href{http://prof.dr.ir}{prof.dr.ir}. T.A.E. Oomen\\
\href{http://prof.dr.ir}{prof.dr.ir}. W.P.M.H. Heemels\\
leden: \href{http://prof.dr.ir}{prof.dr.ir}. P.M.J. Van den Hof \href{http://prof.dr.ir}{prof.dr.ir}. B. Jayawardhana (Rijksuniversiteit Groningen) \href{http://dr.ir}{dr.ir}. R.M.G. Ferrari (Delft University of Technology) \href{http://dr.ir}{dr.ir}. D. Jung (Linköping University)\\
adviseur: \href{http://dr.ir}{dr.ir}. J.J.M. van de Wijdeven (ASML)

Het onderzoek dat in dit proefschrift wordt beschreven is uitgevoerd in overeenstemming met de TU/e Gedragscode Wetenschapsbeoefening.

\section*{Summary}
\section*{Fault Diagnosis for Uncertain Systems in Closed Loop Applied to Semiconductor Equipment}
The economic value of high-tech production equipment is predominantly determined by its productivity. In integrated circuit (IC) production factories, for example, any downtime of the wafer exposure equipment directly halts production. Therefore, unexpected system faults directly deteriorate productivity. These unforeseen faults, which may arise from factors such as defects, aging system components, and wear and tear, among others, underscore the imperative need for effective maintenance strategies.

In the absence of maintenance, it is not a question whether a machine will fail, but rather when it will fail. Ideally, the inevitable downtime for maintenance is optimally scheduled to minimize negative effects on productivity. This scheduling can be achieved through predictive maintenance, in which unexpected faults are detected in an early phase and addressed proactively. The implementation of predictive maintenance is pursued via the process of predicting and detecting faults in equipment and pinpointing its origin, which is called fault detection and isolation. After detection, faults are repaired either through software reconfigurations or through hardware maintenance.

Digital twins are increasingly being developed for industrial high-tech systems. These digital counterparts offer promising solutions for fault diagnosis, predictive maintenance, and controller reconfiguration. Properly designed digital counterparts possess the ability to accurately predict the machine behavior. Consequently, detection of an increasing mismatch between the model and the machine can indicate an upcoming failure. This presents the opportunity for scheduled instead of unscheduled service actions. As such, they provide major opportunities for enhancing uptime.

Interestingly, models of the system are typically created before commissioning a machine, ranging from simple first-principles models to those identified during system integration. Surprisingly, after system integration and controller design, the developed models are often left unused. In sharp contrast, these models can be repurposed to form a digital counterpart that is continuously informed with real-time data stemming from sensors and actuators of the physical machine.

Hence, the models developed during the machine's design and integration phases, are at the heart of the proposed digital counterparts, emphasizing their pivotal role in optimizing operational efficiency.

The aim of this thesis is the development of systematic design approaches for model-based fault diagnosis systems for mechatronics by leveraging prior information, i.e., available models, and posterior information in the form of input/output data. This thesis includes a number of contributions towards improving the performance of model-based fault diagnosis systems for mechatronics.

Inherently, there is uncertainty in the obtained data and in the available models. First, methods are developed which are able to distinguish faulty system behavior from unmodeled system dynamics and disturbances, while providing robust performance guarantees. These methods use uncertain models, particularly known from precision mechatronics, where they are integral to robust controller design.

Second, assuming no modeling uncertainty, a systematic procedure for fault detection and isolation in multivariate systems in closed-loop configuration is presented. This procedure deals with numerous potential fault scenarios, modeling them as additive contributions and considering closed-loop aspects and subsystem interactions.

Third, the system identification framework is adopted. Both online and offline estimation methods are developed to estimate additive (modal) systems. These methods track parameters related to each mode shape, effectively pinpointing the origin of anomalous behavior. Additionally a frequency-domain method is presented to detect, locate, and quantify anomalous nonlinear behavior. To achieve this, measurement locations in mechanical systems are interpreted as an arrangement of nodes in a dynamic network, and linearisation techniques are applied to the frequency response functions constructed from node to node. This allows to pinpoint the origin of anomalous nonlinearities in mechanical systems.

The approaches presented in this thesis contribute to both practically relevant and theoretical advancements, facilitating the implementation of sophisticated diagnostic systems. Notably, the approaches are effectively validated on industrial systems such as prototype lithography systems and academic examples, showcasing their effectiveness in detecting faults and pinpointing their origins amidst complex dynamics. These contributions hold significant potential to elevate the economic value of of current and beyond future state-of-the-art manufacturing equipment.

\section*{Contents}
Summary\\
I Background ..... 1\\
1 Introduction ..... 3\\
1.1 The role of mathematical models and data in modern society ..... 4\\
1.2 The role of mathematical models and data in mechatronic sys- tem design ..... 7\\
1.3 Next-generation mechatronic systems ..... 10\\
1.4 Approaches to fault diagnosis ..... 11\\
1.5 Present challenges for fault diagnosis in mechatronic systems ..... 14\\
1.6 Contributions ..... 17\\
1.7 Outline of the thesis ..... 21\\
II Fault Diagnosis for Uncertain Systems ..... 25\\
2 Closed-loop Optimal Fault Detection for Uncertain Systems ..... 27\\
2.1 Introduction ..... 28\\
2.2 Notation and preliminaries ..... 30\\
2.3 Problem formulation ..... 32\\
2.4 Robust closed-loop fault detection solution ..... 38\\
2.5 Design ..... 44\\
2.6 Numerical example ..... 51\\
2.7 Conclusion ..... 58\\
Appendices ..... 59\\
2.A Alternative representation residual dynamics for uncertain closed- loop systems ..... 59\\
3 Robust Fault Detection with Application to a Prototype Wafer Stage ..... 61\\
3.1 Introduction ..... 62\\
3.2 Notation and preliminaries ..... 64\\
3.3 System description and problem formulation ..... 66\\
3.4 Solution ..... 71\\
3.5 Experimental results ..... 73\\
3.6 Conclusion ..... 83\\
4 Robust $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ Fault Detection for Closed-Loop Systems with Application to a Prototype Wafer Stage ..... 85\\
4.1 Introduction ..... 86\\
4.2 Notation and preliminaries ..... 88\\
4.3 Problem formulation ..... 90\\
4.4 Solution: An $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ to an $\mathcal{H}_{\infty}$ approach ..... 92\\
4.5 Structured robust performance analysis and synthesis ..... 95\\
4.6 Robust performance via fixed structure synthesis ..... 98\\
4.7 Robust performance in view of $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ conditions ..... 100\\
4.8 Experimental results on a prototype wafer stage ..... 103\\
4.9 Conclusion ..... 112\\
Appendices ..... 113\\
4.A Proof of theorem 4.15 ..... 113\\
4.B Proof of theorem 4.19 ..... 114\\
4.C Proof of theorem 4.21 ..... 117\\
4.D Proof of theorem 4.24 ..... 117\\
III Fault Diagnosis for Nominal Systems ..... 119\\
5 Closed-loop Aspects in MIMO Fault Diagnosis with Applica- tion to Precision Mechatronics ..... 121\\
5.1 Introduction ..... 122\\
5.2 Problem formulation ..... 123\\
5.3 Procedure for closed-loop fault detection based on identified models ..... 126\\
5.4 Illustrative case study ..... 132\\
5.5 Conclusion ..... 134\\
Appendices ..... 135\\
5.A Proof of theorem 5.2 ..... 135\\
6 Nullspace-based Fault Diagnosis for Closed-Loop Mechatronic Systems with Application to Semiconductor Equipment ..... 137\\
6.1 Introduction ..... 138\\
6.2 Problem formulation ..... 139\\
6.3 Solution to the AFDIP ..... 143\\
6.4 Design for actuator and sensor faults ..... 145\\
6.5 Experimental results ..... 148\\
6.6 Conclusion ..... 152\\
7 Direct Shaping of Minimum and Maximum Singular Values: An $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ Synthesis Approach for Fault Detection Filters ..... 155\\
7.1 Introduction ..... 156\\
7.2 Preliminaries ..... 157\\
$7.3 \mathcal{H}_{-} / \mathcal{H}_{\infty}$ approach to fault detection ..... 158\\
7.4 Design specifications and synthesis for fault diagnosis ..... 160\\
7.5 Numerical example ..... 165\\
7.6 Conclusion ..... 166\\
Appendices ..... 168\\
7.A Synthesis inequalities ..... 168\\
7.B Auxiliary results ..... 169\\
7.C Derivation of the maximum gain synthesis LMI ..... 170\\
7.D Derivation of the minimum gain synthesis BMI ..... 173\\
IV New Perspectives on Identification for Fault De- tection and Isolation ..... 179\\
8 Recursive Identification of Structured Systems: An Instrumental- Variable Approach Applied to Mechanical Systems ..... 181\\
8.1 Introduction ..... 182\\
8.2 System setup and problem formulation ..... 184\\
8.3 Parsimony and interpretability in continuous-time system iden- tification ..... 187\\
8.4 Recursive estimation of continuous-time systems ..... 189\\
8.5 Numerical Simulation ..... 200\\
8.6 Experimental validation on an overactuated and oversensed flex- ible beam setup ..... 203\\
8.7 Conclusions ..... 205\\
Appendices ..... 208\\
8.A Derivation of the refined instrumental variable iterations ..... 208\\
8.B Recursive computation of the SRIVC iterations ..... 209\\
8.C Proof of theorem 8.14 ..... 211\\
9 Identification of Additive Continuous-time Systems in Open and Closed Loop ..... 217\\
9.1 Introduction ..... 218\\
9.2 System and model setup ..... 220\\
9.3 Stationary points for additive continuous-time system identifi- cation ..... 222\\
9.4 Additive system identification: An instrumental variable solution ..... 227\\
9.5 Simulations ..... 235\\
9.6 Experimental validation ..... 239\\
9.7 Conclusions ..... 241\\
Appendices ..... 242\\
9.A Technical lemma ..... 242\\
10 Locating Nonlinearities in Mechanical Systems: A Frequency- Domain Dynamic Network Perspective ..... 245\\
10.1 Introduction ..... 246\\
10.2 System setup and problem formulation ..... 248\\
10.3 Best linear approximation ..... 252\\
10.4 Nonlinearity location methodology ..... 256\\
10.5 Simulation study ..... 262\\
10.6 Experimental validation ..... 265\\
10.7 Conclusion ..... 270\\
V Closing ..... 271\\
11 Conclusions and Recommendations ..... 273\\
11.1 Conclusions ..... 273\\
11.2 Recommendations ..... 276\\
Bibliography ..... 279\\
List of publications ..... 303\\
Dankwoord ..... 309\\
Curriculum Vitae ..... 313

\section*{Part I}
\section*{Background}
\section*{Санртве 1}
\section*{Introduction ${ }^{1}$}
Imagine a world where industrial machinery never fails, production lines operate seamlessly, and costly downtime is a thing of the past. In industries with large financial stakes such as the semiconductor industry, maximizing throughput and minimizing unexpected downtime are paramount. Fault diagnosis and monitoring systems have the potential to play a pivotal role by operating behind the scenes to predict and pinpoint emerging faults, ensuring that the production process runs smoothly. The underlying mathematical models allow to transform data into actionable insights, giving engineers the means to foresee problems and find optimal solutions even before issues arise. This thesis focuses on the development of these advanced model-based fault diagnosis methods, specifically tailored to high-precision mechatronic systems.

First the concept of a mathematical model and their relevance in today's society are introduced in Section 1.1. Section 1.2 discusses the role of mathematical models and data in the development of mechatronic systems. Following this, Section 1.3 explores the missed opportunities for diagnostics and explains how to leverage these models for fault diagnosis in mechatronic systems, emphasizing that high-quality models and data lie at the heart of effective fault diagnosis systems. Section 1.4 provides an overview of current trends and state-of-the-art techniques in fault diagnosis. The research objectives addressed in this thesis are outlined in Section 1.5, outline the challenges addressed in this thesis. The contributions of this thesis are discussed in Section 1.6. Finally,

\footnotetext{${ }^{1}$ The chapter is based on: [38] K. Classens, W. P. M. H. Heemels, and T. Oomen, "Digital Twins in Mechatronics: From Model-based Control to Predictive Maintenance," in 2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI), Beijing, China, 2021, pp. 336-339, and [40] K. Classens, J. van de Wijdeven, W. P. M. H. Heemels, and T. Oomen, "Opportunities of Digital Twins for High-tech Systems: From Fault Diagnosis and Predictive Maintenance to Control Reconfiguration," Mikroniek, vol. 63, no. 5, pp. 5-12, 2023.
}Section 1.7 provides an overview of the thesis structure.

\subsection*{1.1 The role of mathematical models and data in modern society}
Throughout history, humans have aspired to comprehend the world around them. This comprehension is achieved through models that are developed and refined through interactions with the environment. Initially, these models have been qualitative, allowing the prediction of outcomes based on observations. For example, the sun rising in the east and setting in the west, dark clouds indicating impending rain, ground shaking during an earthquake, or tides following the phases of the moon. With the development of physics and mathematics, these models have been replaced by quantitative models expressed through mathematical equations.

\subsection*{1.1.1 Advancements in mathematical modeling}
A mathematical model is an abstract representation of a real-world system which involves equations, functions, and variables to simulate behavior, dynamics, and interactions of the components within a system. Mathematical models offer a deep understanding of the world and are fundamental to advancements in science, technology, and society. These models can be developed using fundamental laws such as Newton's laws of motion [179], which describe the relationship between the motion of an object and the forces acting on it; the Navier-Stokes equations for fluid dynamics [176, 177], which explain the behavior of fluids and are crucial for understanding weather patterns and aerodynamics; Maxwell's equations for electromagnetism [118, 123, 173], which describe how electric and magnetic fields interact; and Einstein's theory of general relativity [71, 72], which explains gravitational interactions and is essential for understanding astronomical observations and the universe. As engineering systems grow more intricate and the need for precise modeling increases, these fundamental relations often become insufficient and first-principle become models exceedingly complex. Consequently, their complexity makes these models impractical for system design, necessitating the development of simplified or approximate models that balance accuracy and manageability.

Understanding and interpreting historical phenomena has greatly benefited from mathematical models, which quantify complex systems and reveal underlying patterns. For instance, demographic trends and population growth over centuries can be analyzed through mathematical models [152, 231], helping historians predict societal changes and resource needs. In the field of epidemiology, models on disease outbreaks have been used to understand the spread and impact of illnesses, aiding in the development of preventive measures and public\\[0pt]
health strategies [27, 113, 169]. In physics, the motion of celestial bodies, such as the planets in our solar system, has been accurately described through mathematical models since the time of Newton, allowing for precise predictions of planetary positions and interactions [131]. Additionally, the fields of economics and finance have benefited from mathematical modeling by explaining trends in trade, market dynamics, and financial crises, offering insights into historical economic behaviors and help to inform future economic policies [ $2,16,117,217$ ]. These models are based on historical data, transforming qualitative observations into quantitative predictive models.

In present times, mathematical models drive innovation and efficiency through their predictive capabilities. In medicine, for example, predictive models are used to simulate the progression of diseases and the potential impact of treatments, aiding in the development of personalized medicine and improving patient outcomes [103, 172]. In environmental science, models predict impact on climate change, guiding policymakers to mitigate adverse effects [122, 221]. Additionally, in logistics and supply chain management, models optimize routes and inventory levels, enhancing efficiency and reducing costs. This includes modeling of warehouses and package delivery systems, and predicting time-to-arrival for packages. In the automotive industry, models predict vehicle arrival times [1], improving traffic management and reducing congestion. These applications leverage vast amounts of data, which is transformed into actionable insights. Whether improving healthcare, financial stability, environmental sustainability, or operational efficiency, mathematical models and data are at the heart of new advancements, highlighting their crucial role in addressing complex real-world challenges.

\subsection*{1.1.2 Mathematical models in engineering systems}
In today's technologically driven world, mathematical models enable prediction and optimization which is crucial to continuously improve complex engineering systems. In the automotive industry, for example, models enable to simulate vehicle dynamics and improve safety features, accelerating the development of advanced driver-assistance systems (ADAS) and autonomous vehicles, thus enhancing road safety and efficiency. In aerospace engineering, predictive models facilitate the design of airplanes and spacecraft, optimizing aerodynamics, structural integrity, fuel efficiency, and safety, exemplified by SpaceX's successful vertical landing of the Falcon 9 rocket [230], a milestone in reusable rocket technology. Civil engineers employ models to predict the behavior of structures under various loads and environmental conditions, ensuring the safety and longevity of infrastructures. In robotics, mathematical models enable the design and control of complex systems like the da Vinci surgical robots [271], which significantly improve surgical precision by extending surgeons' capabilities beyond natural human limits. In quantum computing, mathematical models are essen-\\[0pt]
tial for developing algorithms that leverage quantum mechanics to solve complex problems far more efficiently than classical computers, promising breakthroughs in cryptography, materials science, and medicine. Additionally, in astronomy, models played a crucial role in the first observation of gravitational waves in 2015 by the Advanced Laser Interferometer Gravitational-Wave Observatory (LIGO) [155], opening a new frontier in our understanding of the universe. All these advancements heavily rely on vast amounts of data, which feed into the models, allowing engineers to simulate real-world scenarios accurately and optimize performance within various engineering disciplines. The integration of data and mathematical modeling underscores their impact in driving innovation and efficiency.

\subsection*{1.1.3 Digital twins}
A major emerging trend that has significantly advanced the development of engineering systems over the past decade is the creation of digital twins. A digital twin is a virtual representation of a physical object or system that is continuously updated with real-time data and uses mathematical models to simulate its physical counterpart's behavior and performance [175, 209]. Digital twins can be used to monitor and analyze the health and performance of systems, predict failures before they occur, and optimize operations in real-time. For instance, in manufacturing systems, digital twins can simulate production processes, enabling engineers to identify inefficiencies and test modifications without disrupting actual production. In urban planning, digital twins of cities can model traffic flows, energy usage, and environmental impacts, aiding in the development of smarter, more sustainable urban environments, and in healthcare, digital twins of patients can provide personalized treatment plans and simulate the effects of interventions [145]. The integration of digital twins into various industries highlights the potential of combining large data streams with sophisticated mathematical models, showcasing the continuous advancement in engineers' ability to understand and optimize the processes in the world around us.

\subsection*{1.1.4 A self-reinforcing cycle}
All these advancements have been significantly driven by the exponential growth in computational power, enabled by the development of increasingly miniaturized transistors in semiconductor chips. Improved processing capabilities have facilitated the handling of large datasets and the execution of complex simulation models that were once impractical. This computational revolution has enabled the creation of more sophisticated mathematical models and the generation of more precise data, thereby enhancing predictive capabilities. In turn, these improved models and data contribute to the creation of even more advanced chips, creating a virtuous cycle of technological progress, see Figure 1.1.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-018}
\captionsetup{labelformat=empty}
\caption{Figure 1.1: Illustration of the self-reinforcing cycle of technological progress due to the increasing computational power. Increased computational power yields more detailed models which empowers more accurate predictions. More accurate predictions yield better machine design which in turn allows to miniaturize chips further which yields increased computational power.}
\end{center}
\end{figure}

\subsection*{1.2 The role of mathematical models and data in mechatronic system design}
Mechatronic system design is the interdisciplinary engineering process of designing, developing, and optimizing systems that integrate mechanical components, electrical components, and software to achieve a specific functional objective. This approach combines principles from mechanical engineering, electrical engineering, computer science, and control engineering to create complex systems that perform precise, reproducible, and automated tasks [143]. This relatively young field has gone through a major development over the past few decades. As a result of a synergistic combination of precision mechanical engineering, control theory, and system thinking, new concepts and architectures have been developed for machines and scientific instruments. In recent years, due to increasing demands, there is an increasing need for further integration of several adjacent fields, like physics, optics, heat and flow, tribology, materials science, mathematics and software. This growing complexity underscores the important role that models of various types and at different stages play in the ongoing advancement of these systems, from initial system design to post-commissioning optimization.

\subsection*{1.2.1 Conceptualization and initial design}
In the early stages of mechatronic system design, mathematical models play an important role in shaping initial concepts, guiding early decision-making, and\\
revealing potential challenges. The process starts with a definition of the system requirements and objectives, which involves understanding the desired basic functionalities, performance criteria, and design constraints. This step is critical as it sets the direction for the entire design process. Engineers utilize back-of-theenvelope calculations to perform quick, rough estimates that determine whether concepts are feasible or not. These preliminary calculations, which may include basic equations of motion, energy balances, or simple electrical circuit analyses, allow engineers to swiftly discard unviable concepts and focus on promising ones, thereby conserving time and valuable resources.

Once system requirements are defined, a typical development approach starts with a high-level design or model of the system, followed by iterative development of a detailed design. Computer-Aided Design (CAD) tools further enhance the conceptual design and accelerate the design cycle and reduces costs, enabling faster and more efficient development of innovative mechatronic solutions. These early insights help in making informed decisions about the overall system architecture and component selection.

\subsection*{1.2.2 Simulation and detailed design}
As the design process progresses, engineers move from rough calculations to detailed and sophisticated modeling efforts. CAD models with integrated Finite Element Method (FEM) [186] provide the foundation for further refinement. These models allow for comprehensive structural analyses, predicting how components respond to stresses, strains, and thermal loads. FEM simulations ensure the mechanical integrity and reliability of the design before building physical prototypes by identifying potential failure points and enabling adjustments to enhance robustness and durability. This iterative process optimizes design parameters such as material selection, thickness, and geometric configurations to achieve desired performance characteristics. Additionally, the design requirements are continuously validated using these models to ensure compliance with specifications.

To understand time-domain behavior of closed-loop controlled mechatronic systems, dynamic simulations are essential. While the Finite Element Method (FEM) provides detailed structural analysis, it is often too complex and impractical for these simulations. Therefore, simpler models are used to accurately represent system dynamics. These models enable engineers to predict how systems will respond to various inputs and disturbances over time. By using these advanced modeling techniques, engineers can fine-tune control system parameters to achieve desired performance characteristics such as stability, and accuracy.

\subsection*{1.2.3 Prototyping and validation}
Once the design reaches a certain level of maturity, rapid prototyping techniques come into play. Rapid prototyping allows for the quick creation of physical prototypes that can be tested and further optimized. Once a prototype is available, the most accurate models are obtained from measurement of the physical system itself such as Frequency Response Functions (FRFs) [202] or estimated parametric models [161]. These empirical approaches refine the initial models, capturing real-world behavior and intricacies that first-principles models typically overlook. This enhances the precision and reliability of simulations and control strategies in mechatronic systems.

Analysis of these models provides critical insights into the dynamic behavior of the system, which is essential for fine-tuning control algorithms and ensuring that the system performs as expected under real-world conditions. The data gathered during prototyping and testing are fed back into the models, enabling further refinement and validation. Prototyping not only accelerates the development cycle but also allows for early detection and correction of design flaws, reducing the overall time and cost to market.

\subsection*{1.2.4 Post-commissioning and continuous development}
After commissioning, mathematical models and data continue to play a role in maintaining and optimizing the performance of mechatronic systems. These models and data are primarily used for further development and performance enhancement once the machine is operational at the client's site. During the initial stage of the design process, a digital twin is often created, that includes updated models and serves as a dynamic testing environment for ongoing optimization. Additionally, this digital counterpart replaces a large volume of documents containing specifications and requirements. By embedding knowledge within these models rather than in extensive documentation, engineers can more effectively monitor and improve system performance.

Advanced system design, accurate models, and precise control have led to the creation of superior high-tech production equipment with extreme accuracy and incredible speeds. This is particularly evident in lithography systems, which are critical in the manufacturing of semiconductor devices. Advanced lithography machines have enabled the production of highly intricate and dense microchips, driving the exponential growth of computational power and the miniaturization of electronic components. These advancements fuel innovations across various industries, from consumer electronics to advanced computing. However, to fully capitalize on these advancements, models and operational data can be more effectively utilized throughout the lifecycle of the equipment, as explored in the next section.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-021}
\captionsetup{labelformat=empty}
\caption{Figure 1.2: Illustration of traditional types of maintenance, where (a) is an illustration of reactive maintenance and (b) is an illustration of preventive maintenance. Responding reactively leads to unexpected downtime and typically a long repair time. Preventive strategies lead to more interruptions than strictly necessary.}
\end{center}
\end{figure}

\subsection*{1.3 Next-generation mechatronic systems}
Despite the advancements in high-tech production equipment, characterized by extreme accuracy and remarkable speed, many models developed during the design phase are not utilized after commissioning and the vast majority of the operational data is discarded. Concurrently, unexpected downtime continues to be a significant challenge, resulting in considerable financial losses. The underutilization of models and data represents a missed opportunity for further optimization and reliability improvements.

To adress the challenge of downtime, predictive maintenance through Fault Detection and Isolation (FDI) offers a promising opportunity. Unlike traditional maintenance approaches, see Figure 1.2, such as reactive maintenance, which addresses failures after they occur, and preventive maintenance, which involves regular and scheduled servicing, predictive maintenance uses real-time data and advanced algorithms to foresee potential failures before they happen [287]. This approach not only minimizes unexpected downtime but also extends the lifespan of the equipment, see Figure 1.3. This claim is reinforced by what is known as the Waddington effect [30], suggesting that more maintenance does not inherently improve reliability or prevent emergency breakdowns. The vision for the future mechatronic systems includes self-healing machines, which can autonomously detect and deal with faults [95]. For instance, in the case of an actuator failure, by reallocating the control effort among the remaining healthy actuators, socalled control reconfiguration [164]. By leveraging predictive maintenance and advancing towards self-healing machines, the reliability and economic value of mechatronic systems can be significantly enhanced.

The models measured from the machine and the vast amounts of input and output data generated during fault-free operation provide the foundation for

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-022}
\captionsetup{labelformat=empty}
\caption{Figure 1.3: Illustration of the machine health of future mechatronic systems, where (a) illustrates predictive maintenance and (b) illustrates self-healing machines. Predictive maintenance is preferred over traditional maintenance as it leads to a small amount of downtime and minimal interruptions. Self-healing machines are envisioned, where the machine autonomously detects and deals with faults.}
\end{center}
\end{figure}

a digital counterpart, used as fault diagnosis system to predict potential issues and to continuously monitor system performance. The key is to use cost-effective models and data measured directly from the machine. High-fidelity models such as FEM are typically too complex for online monitoring, necessitating simpler models such as identified state-space systems or transfer functions. In the hightech production industry, interpretability is key, ensuring that the insights derived from these digital counterparts are understandable and actionable. This transparency allows for precise adjustments and informed decision-making. By exploiting these models, readily available for any high-tech mechatronic system, overall system reliability and efficiency can be significantly enhanced.

\subsection*{1.4 Approaches to fault diagnosis}
This section offers an overview of the origins of fault diagnosis systems and various approaches that are used in the field. Fault diagnosis systems are widely integrated in many safety-critical engineering systems including aerospace systems, spacecraft, vehicle control systems, railway systems, medical systems, the chemical and process industry, nuclear power plants and power systems, among others. The aerospace sector, in particular, has a rich history in this field, dating back to the early days of aviation where stringent legislative requirements drove advancements in the field of fault diagnosis to guarantee the safety and reliability of flight systems [170, 288].

Common types of faults in mechatronic systems include the loss of efficiency or blocking of an actuator, the loss or drift of a sensor, a disconnection of a system component, and mechanical failures due to wear and fatigue. Additionally,\\[0pt]
moving resonances [39] which may cause excessive vibrations or even lead to closed-loop instability, can severely impact system. Detecting and appropriately handling these faults is essential to maintain performance and prevent downtime. Fault diagnosis includes three main tasks: fault detection, fault isolation and fault identification. These concepts are defined as follows [63].

Definition 1.1. Fault detection: detection the occurrence of faults within a process, which result in undesired behavior of the entire system.

Definition 1.2. Fault isolation: pinpointing the origin of the faults.\\
Definition 1.3. Fault identification: determination of the type, magnitude and root-case of the fault.

A traditional approach to improve system reliability is to enhance the quality and robustness of individual system components like sensors, actuators, or controllers. Despite following this approach cannot guarantee fault-free components. Redundant hardware is often included to avoid relying on a single component that is susceptible to faults. The basic idea is to use identical sensors such that the duplicated output signals can be compared and used as a diagnostic system in the form of limit checking or majority voting. Redundant actuators can be included such that multiple actuators can execute the same task. The main advantage of redundant hardware is its high reliability and direct fault isolation. On the other hand, redundant hardware occupies additional space and depending on the application, costs can be high.

A large number of survey papers $[79,84,85,94,137,138,265,266,267]$ were written on different fault diagnosis methods and numerous books $[17,21,35$, 61, 63, 94, 137, 197, 232, 264] attempt to classify these approaches. With the development of modern control theory, analytical redundancy, i.e., redundancy through software, has become a mainstream field. Besides hardware redundancy, analytical fault diagnosis methods are generally subdivided into signal-based methods, model-based methods, and knowledge-based methods. Figure 1.4 shows a closed-loop controlled system that is augmented by a fault diagnosis system.

Signal fault diagnosis relies on the assumption that the process signals carry information about faults in the form of symptoms. Consequently, fault diagnosis can be achieved by suitable signal processing. These techniques range from simple time-domain methods, such as checking if a magnitude threshold is exceeded, to more complex frequency-domain methods, such as analyzing power spectral densities. These signal processing based diagnostics systems are primarily used in steady-state operation of a process.

The main idea of model-based fault diagnosis is to include a process model in the machine's software. In this way, the process behavior is reconstructed on-line. Similar to hardware redundancy, the process model runs in parallel to the machine and is driven by the same process inputs. During fault-free operation, the reconstructed process variables follow the corresponding variables of

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-024}
\captionsetup{labelformat=empty}
\caption{Figure 1.4: Schematic overview of a closed-loop controlled mechatronic system subjected to faults and disturbances. Data is extracted from the controlled system which serves as input to the fault diagnosis system which generates signals revealing knowledge about potential faults. Depending on the approach, the system's setpoint (task) is included as an input for the fault diagnosis system.}
\end{center}
\end{figure}

the real machine. In case a fault is present, the variables show a deviation. The difference between the measured process variables and their estimates is called a residual. Hence, the residual signal carries an important message for a successful fault diagnosis: if the residual nonzero, a fault is present, otherwise the system is fault-free. The system that creates the estimates and builds the difference between the process outputs and their estimates is called the residual generator, see Figure 1.5. This generator is typically designed to achieve decoupling of the faults from disturbances and modeling uncertainties, present in any practical system. Extracting information about the fault of interest and converting this information into a decision is done by post-processing the residuals. This procedure is called residual evaluation.

Knowledge-based fault diagnosis, also known as intelligent fault diagnosis, is gaining increasingly more interest by academics. This concept refers mainly to artificial intelligence (AI) and machine learning theories for fault diagnosis. Contrary to model-based methods and signal-based approaches that require either an a priori known model or signal patterns, knowledge-based approaches start from a vast amount of historical data. By processing this large volume of data, underlying knowledge, which implicitly represents the dependence of system variables is extracted. The consistency between the observed behavior of the machine and the knowledge base is then checked, leading to fault diagnosis with the aid of classifiers. Knowledge-based schemes can be applied in realtime, but require training on a large amount of historical data. From this point of view, knowledge-based fault diagnosis approaches are sometimes referred to as data-driven approaches.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-025}
\captionsetup{labelformat=empty}
\caption{Figure 1.5: Schematic overview of a model-based fault diagnosis system that is split into three parts. First, residual signals are generated by a residual generator. Subsequently, these are processed and transformed into a decision. These two actions are typically referred to as residual evaluation.}
\end{center}
\end{figure}

A step beyond fault diagnosis involves converting the obtained knowledge directly into actionable responses, as illustrated in Figure 1.6. This is known as fault tolerant control, which integrates the fault diagnosis system into the closed-loop controlled system. Multiple surveys and books describe various fault-tolerant control approaches and its applications [21, 61, 85, 139]. In faulttolerant control, the fault diagnosis system detects faults in real-time and assesses the severity of the malfunction. Based on this information, the system takes appropriate fault-tolerant actions, such as compensating the faulty signals, tuning or reconfiguring the controller, and, if redundant hardware is present, switching to a configuration that bypasses the faulty component. This seamless integration enhances system reliability and ensures continuous operation despite the presence of faults.

The simple yet accurate models measured from mechatronic equipment, paired with vast input and output data, present major opportunities for fault diagnosis. Recognizing the requirement that the insights derived from the digital counterparts are based on solid fundamentals and are understandable, this thesis pursues model-based approaches. The majority of the approaches presented in this thesis fit into the model-based class and relate directly to particular subclasses within this domain, to be specific, observer-based approaches, directfiltering, parameter estimators, and more general identification related schemes. Tailoring these fault diagnosis methods to closed-loop mechatronic systems yields several unsolved challenges that are described next.

\subsection*{1.5 Present challenges for fault diagnosis in mechatronic systems}
Future high-tech systems necessitate tailored diagnostics systems that can handle the inherent complexity of these machines. Existing fault diagnosis approaches cannot always be applied to these complex systems, and need fur-

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-026}
\captionsetup{labelformat=empty}
\caption{Figure 1.6: Schematic overview of a fault-tolerant closed-loop controlled mechatronic system subjected to faults and disturbances, and augmented with a fault diagnosis system that is directly linked to the controller.}
\end{center}
\end{figure}

ther development. A total of five challenges are considered in this thesis which are related to three topics, further examined in the following three subsections. First, fault diagnosis approaches need to be able to discriminate faults from disturbances and modeling uncertainty in a multi-input multi-output (MIMO) system and closed-loop control setting. Second, a proof-of-concept is required to illustrate the applicability to high-precision industrial systems. Third, new perspectives on system identification give new insights and yield new approaches for fault detection, isolation, and estimation. In this section, a list of objectives is formulated to address these open challenges, which together constitute the overarching research objective.

Develop robust advanced fault diagnosis systems for closed-loop MIMO systems, while demonstrating practical applicability through proofs-ofconcept and exploring new perspectives on identification.

\subsection*{1.5.1 Robust fault diagnosis for closed-loop MIMO systems}
A key challenge in fault detection (FD) involves distinguishing faults from unknown disturbances and modeling uncertainty. It is widely acknowledged that achieving satisfactory performance in model-based fault diagnosis systems requires a balanced tradeoff between fault sensitivity and disturbance rejection [62]. Several optimal fault diagnosis methods have been developed for linear time-invariant (LTI) systems [62, 134, 142, 156, 157, 248, 270]. These methods result in optimal FD systems in the sense that the residual is as sensitive to\\
faults as possible provided that the disturbance and plant model are exactly known.

Robust methods have been developed to explicitly address modeling uncertainty. Among these, many utilize the $\mathcal{H}_{\infty}$ criterion [216, 242, 285]. Addition- ally, $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ criteria are employed and addressed with LMI solutions [125, 269]. Contrary to methods for LTI systems without uncertainty [62, 157, 248], robust methods found in existing literature tend to be either conservative or complicated to apply. For example, methods that rely solely on an $\mathcal{H}_{\infty}$ criterion do not directly account for $\mathcal{H}_{-}$fault sensitivity, which has to be analyzed a posteriori. Although several important steps have been taken towards fault diagnosis for complex systems, at present a method for optimal detection of faults in uncertain closed-loop systems is lacking. Hence, it is aimed to obtain an optimal solution to the fault detection filter design problem in the $\mathcal{H}_{i} / \mathcal{H}_{\infty}$ framework. In contrast to maximizing the sensitivity of faults via the lowest singular value through the $\mathcal{H}_{-}$index, $\mathcal{H}_{i}$ refers to optimizing the sensitivity in all the directions related the singular values.

Research Objective I. Develop optimal fault diagnosis filter synthesis algorithms that allow to discriminate faults from disturbances and modeling uncertainty.

In general, many fault diagnosis methods are developed for open-loop systems and therefore not tailored to closed-loop systems. This results in a discrepancy between the expected performance and the achieved performance. Hence, it is shown that taking the closed-loop and MIMO aspects into account is instrumental for achieving reliable results.

Research Objective II. Investigate the closed-loop and MIMO aspects of fault detection filters.

\subsection*{1.5.2 Application to high-precision motion systems}
Fault diagnosis systems provide a major opportunity to decrease downtime of high-precision production equipment, increasing its economic value. To illustrate this potential towards application in industrial environments, it is desired to\\
provide a proof-of-concept.

Research Objective III. Demonstrate proof-of-concept for robust fault diagnosis methods in next-generation precision equipment.

\subsection*{1.5.3 Identification for fault detection and isolation}
A key indicator for an increased risk for failure in mechanical systems is the shifting of resonances. While a single non-additive transfer function is commonly employed in linear system identification, practical applications in flexible motion systems [184, 268], and vibration analysis are more effectively conceptualized as a sum of transfer functions with distinct denominators [91]. Since these additive model parametrizations bring benefits such as enhanced physical insight for fault diagnosis [39], it is desired to directly estimate systems in this interpretable model class.

Research Objective IV. Develop estimation algorithms tailored to additive (modal) systems

Faults often induce additional nonlinearities in a system. To this end, a method to detect, locate, and quantify nonlinearities in a system is desired. Ideally, a framework is desired that fits in the commonly used non-parametric framework.

Research Objective V. Develop a non-parametric method to detect, locate, and quantify nonlinearities in mechanical systems.

\subsection*{1.6 Contributions}
This thesis presents new diagnostics and identification techniques and is subdivided into three parts, each having a distinct focus. The first part mainly contributes to the development of robust fault diagnosis methods that allow to distinguish faults from disturbances and modeling uncertainty. The proposed approaches consider MIMO uncertain systems in closed loop. The second part contributes to the development of fault diagnosis systems considering nominal systems, i.e., without uncertainty. Again, MIMO systems are considered in\\
closed loop with a particular focus on the implications of these aspects and application to a large MIMO industrial system. The third part, focusing on estimation, contributes to the field of fault diagnosis indirectly. In this part, offline and online methods are proposed to estimate additive (modal) systems and to detect, isolate, and quantify nonlinearities in a system. In this section, the contributions of this thesis are summarized for each individual part. A graphical overview of this thesis is included at the end of this section in Figure 1.7.

\subsection*{1.6.1 Uncertain systems}
Part II consists of Chapters 2, 3, and 4. In Chapter 2 a new approach is introduced to distinguish faults from disturbances and modeling uncertainties. The robust $\mathcal{H}_{i} / \mathcal{H}_{\infty}$ fault detection filter design problem is considered for continuoustime linear time-invariant uncertain systems operating in open or closed-loop configurations. The proposed framework offers a unified approach to handle various types of uncertainties by solving a single Riccati equation, based on a worst-case disturbance and uncertainty model. This worst-case model is obtained by nonlinear optimization and interpolation via application of the boundary Nevanlinna-Pick method. The efficacy of the proposed approach is demonstrated using an uncertain model of a next-generation prototype reticle stage. The results illustrate that an optimal compromise is achieved between fault sensitivity and rejection of modelling uncertainties and disturbances. This constitutes the first contribution of this thesis.

Contribution I. An optimal solution is provided to the $\mathcal{H}_{i} / \mathcal{H}_{\infty}$ fault detection problem for closed-loop MIMO LTI uncertain systems.

In Chapter 3 this method is applied to a high-precision wafer stage in closedloop operation, specifically used for lithography in the semiconductor industry. Application of the proposed framework is shown starting from identification of the closed-loop system using multisine excitation, allowing the development of a quality certificate for the model in the form of uncertainty bounds. Subsequently, an uncertainty-independent upper bound on disturbances and modeling errors is calculated to synthesize the optimal fault detection filter. This marks the first implementation of such a robust fault detection technique in high-precision production equipment, which constitutes the second contribution of this thesis.

Contribution II. A user-friendly design framework is provided by applying the optimal $\mathcal{H}_{i} / \mathcal{H}_{\infty}$ fault detection solution to a prototype wafer stage.

In Chapter 4, the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ paradigm for uncertain linear time invariant systems is adapted to fit $\mu$-synthesis and $\mu_{g}$-analysis tools, offering a framework for robust fault detection and isolation by effectively managing disturbances and modeling uncertainties. This $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ design and analysis framework is successfully applied to an experimental next-generation wafer stage prototype. This constitutes the third contribution of this thesis.

Contribution III. The robust $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ problem is formulated to accommodate an $\mathcal{H}_{\infty}$ solution with structured complex perturbation, enabling optimization through $\mu$-synthesis.

\subsection*{1.6.2 Nominal systems}
Part III consists of Chapters 5, 6, and 7. In Chapter 5 a systematic procedure is presented for identification of a nominal model to accurate nullspace-based fault diagnosis, accounting for the influence of noise and interaction in multivariable closed-loop control configurations. The influence of noise and interaction on the model estimate and fault diagnosis system are investigated through the use of closed-loop operators and by means of an illustrative case study. This constitutes the fourth contribution of this thesis.

Contribution IV. A systematic procedure is established for fault detection for MIMO systems in closed-loop configuration, beginning with the identification of an accurate model.

Chapter 6 applies the nullspace-based fault diagnosis framework to a MIMO prototype wafer stage with 13 inputs and 4 outputs. A model of the system is identified and used to synthesize a fault detection and isolation filter capable of distinguishing 17 distinct fault scenarios. This marks the first implementation of a fault detection and isolation filter in a large MIMO motion system, which\\
constitutes the fifth contribution of this thesis.

Contribution V. A proof-of-principle is provided by synthesizing a nullspace-based fault detection and isolation filter and experimentally val- idating the filter on a prototype wafer stage with a large number of possible fault scenarios.

In Chapter 7, an approach is developed to directly shape the sensitivities from fault and disturbance to residual, expressed in terms of minimum and maximum singular values. The developed method offers an alternative solution to the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ synthesis problem, building upon traditional multiobjective synthesis results. The result is an optimal filter synthesized via iterative convex optimization. This constitutes the sixth contribution of this thesis.

Contribution VI. An alternative $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ synthesis algorithm is developed to design fault detection and isolation filters, building upon traditional multiobjective synthesis and convex optimization.

\subsection*{1.6.3 Identification for Fault Detection and Isolation}
Part IV consists of Chapters 8, 9, and 10. Chapter 8 presents a new approach for real-time estimation of time-varying dynamics within an interpretable model structure. To achieve this, an additive model structure is adopted, offering enhanced parsimony, particularly suitable for mechanical systems. The proposed approach integrates the recursive simplified refined instrumental variable method with block-coordinate descent to minimize an exponentially-weighted output error cost function. This novel recursive identification method yields parametric continuous-time additive models applicable in both open-loop and closed-loop controlled systems. The approach is validated using experimental data to detect time-varying resonance mechanic as a result of a fault in an overactuated and oversensed flexible beam system. This constitutes the seventh contribution of this thesis.

Contribution VII. A recursive continuous-time system identification algorithm is derived for systems in additive form, for both open and closed-loop systems.

Building upon the idea to estimate additive models, Chapter 9 presents a\\
novel offline identification method that delivers additive models for both open and closed-loop setups, known as the Simplified Refined Instrumental Variable Approach (SRIVC) for additive systems. The closed-loop variant is named the Closed-Loop Simplified Refined Instrumental Variable Approach (CLSRIVC). The estimators that are derived are shown to be generically consistent, and admit the identification of marginally stable additive systems. Numerical simulations show the efficacy of the proposed approach, and its performance in identifying a modal representation of a flexible beam is verified using experimental data. This constitutes the eight contribution of this thesis.

Contribution VIII. Open and closed-loop estimators are derived, extending the SRIVC and CLSRIVC estimators for additive continuoustime models.

Chapter 10 introduces a data-driven approach for nonlinearity location and quantification by analyzing nonparametric frequency response functions. To achieve this objective, measurement locations in mechanical systems are interpreted as nodes arranged in a dynamic network, and linearization techniques are employed on the frequency response functions formed from node to node. The efficacy of the proposed approach and the concept of nonlinearity localization and quantification are illustrated by numerical simulations and experiments on a flexible beam setup. This constitutes the ninth contribution of this thesis.

Contribution IX. A detailed step-by-step procedure is established, utilizing the equivalences between mechanical systems and dynamic networks, to identify the location of nonlinearities in MIMO systems and quantify their magnitude.

\subsection*{1.7 Outline of the thesis}
Including this introduction, this thesis consists of 11 chapters. Chapters 2-10, constitute Part II to IV, are based on articles in preparation, submitted articles, or published articles, and consequently are self-contained and can be read independently. This may lead to some repetition with respect to the introductions and/or considered systems in the individual chapters. However, the relevant chapters do provide some continuity and relevant information linking them with each other. The chapters and how they relate to the contributions and research objectives are listed below. A reference to the corresponding research paper is included at the beginning of each chapter. The conclusions and recommendations of this thesis are presented in Chapter 11.

\section*{Part II: Fault Diagnosis for Uncertain Systems}
Chapter 2 corresponds to Contribution I and the publication:

\begin{itemize}
  \item \hspace{0pt} [46] Koen Classens, Tjeerd Ickenroth, Jeroen van de Wijdeven, Maurice Heemels, and Tom Oomen, "Closed-Loop Optimal Fault Detection for Uncertain Systems," submitted for journal publication.
\end{itemize}

Related preliminary results are presented in the following publication:

\begin{itemize}
  \item \hspace{0pt} [47] Koen Classens, Tjeerd Ickenroth, Jeroen van de Wijdeven, Maurice Heemels, and Tom Oomen, "Optimal Fault Detection for Closed-Loop Linear Uncertain Systems," in 63rd IEEE Conference on Decision and Control, Milan, Italy, 2024.
\end{itemize}

Chapter 3 corresponds to Contribution II and the publication:

\begin{itemize}
  \item \hspace{0pt} [45] Koen Classens, Tjeerd Ickenroth, Koen Tiels, Paul Tacx, Jeroen van de Wijdeven, Maurice Heemels, and Tom Oomen, "Robust Fault Detection with Application to a Prototype Wafer Stage," submitted for journal publication.
\end{itemize}

Related preliminary results are presented in the following publication:

\begin{itemize}
  \item \hspace{0pt} [47] Koen Classens, Tjeerd Ickenroth, Jeroen van de Wijdeven, Maurice Heemels, and Tom Oomen, "Optimal Fault Detection for Closed-Loop Linear Uncertain Systems," in 63rd IEEE Conference on Decision and Control, Milan, Italy, 2024.
\end{itemize}

Chapter 4 corresponds to Contribution III and the publication:

\begin{itemize}
  \item \hspace{0pt} [41] Koen Classens, Stan de Rijk, Jeroen van de Wijdeven, Maurice Heemels, and Tom Oomen, "Robust $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ Fault Detection with Application to a Prototype Wafer Stage," submitted for journal publication.
\end{itemize}

\section*{Part III: Fault Diagnosis for Nominal Systems}
Chapter 5 corresponds to Contribution IV and the publication:

\begin{itemize}
  \item \hspace{0pt} [37] Koen Classens, Maurice Heemels, and Tom Oomen, "Closed-loop Aspects in MIMO Fault Diagnosis with Application to Precision Mechatronics," in 2021 IEEE American Control Conference, New Orleans, Louisiana, USA, 2021.
\end{itemize}

Related preliminary results are presented in the following publications:

\begin{itemize}
  \item \hspace{0pt} [43] Koen Classens, Maurice Heemels, and Tom Oomen, "A Closed-Loop Perspective on Fault Detection for Precision Motion Control: With Application to an Overactuated System," in 2021 IEEE International Conference on Mechatronics, Kashiwa, Japan, 2021.
\end{itemize}

Chapter 6 corresponds to Contribution V and the publication:

\begin{itemize}
  \item \hspace{0pt} [49] Koen Classens, Jeroen van de Wijdeven, Maurice Heemels, and Tom Oomen, "Nullspace-based Fault Diagnosis for Closed-Loop Mechatronic Systems with Application to Semiconductor Equipment," submitted for journal publication.
\end{itemize}

Chapter 7 corresponds to Contribution VI and the publication:

\begin{itemize}
  \item \hspace{0pt} [44] Koen Classens, Maurice Heemels, and Tom Oomen, "Direct Shaping of Minimum and Maximum Singular Values: An $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ Synthesis Approach for Fault Detection Filters," in Proceedings of the IFAC 22st Triennial World Congress, Yokohama, Japan, 2023.
\end{itemize}

\section*{Part IV: New Perspectives on Identification for Fault Detection and Isolation}
Chapter 8 corresponds to Contribution VII and the publication:

\begin{itemize}
  \item \hspace{0pt} [42] Koen Classens, Rodrigo González, and Tom Oomen, "Recursive Identification of Structured Systems: An Instrumental-Variable Approach applied to Mechanical Systems," submitted for journal publication.
\end{itemize}

Chapter 9 corresponds to Contribution VIII and the publication:

\begin{itemize}
  \item \hspace{0pt} [112] Rodrigo González, Koen Classens, Cristian Rojas, James Welsh, and Tom Oomen, "Identification of Additive Continuous-time Systems in Open and Closed Loop," submitted for journal publication.
\end{itemize}

Chapter 10 corresponds to Contribution IX and the publication:

\begin{itemize}
  \item \hspace{0pt} [48] Koen Classens, Maarten Schoukens, Tom Oomen, and Jean-Philippe Noël, "Locating Nonlinearities in Mechanical Systems: A FrequencyDomain Dynamic Network Approach," submitted for journal publication.
\end{itemize}

\section*{Part I: Background}
\section*{Chapter 1 - Introduction}
\begin{figure}[h]
\begin{center}
\captionsetup{labelformat=empty}
\caption{Part II: Fault Diagnosis for Uncertain Systems}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-035(3)}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\captionsetup{labelformat=empty}
\caption{Part III: Fault Diagnosis for Nominal Systems}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-035(4)}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\captionsetup{labelformat=empty}
\caption{Part IV: New Perspectives on Identification for Fault Detection and Isolation}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-035(2)}
\end{center}
\end{figure}

\includegraphics[max width=\textwidth, center]{2025_10_13_d0faf158053d0f7500c0g-035(1)}\\
\includegraphics[max width=\textwidth, center]{2025_10_13_d0faf158053d0f7500c0g-035}

\section*{Part V: Closing}
\section*{Chapter 11 - Conclusions and Recommendations}
Figure 1.7: Schematic overview of this thesis, featuring illustrations that represent the working principles or motivating applications of each chapter.

\section*{Part II}
\section*{Fault Diagnosis for Uncertain Systems}
\section*{Chapter 2}
\section*{Closed-loop Optimal Fault Detection for Uncertain Systems ${ }^{1,2}$}
\begin{abstract}
Faults jeopardize the reliability and safety of complex engineering systems. Distinguishing these faults from disturbances and inherent modeling uncertainties in practical systems is a challenging task. This chapter addresses the robust fault detection filter design problem for continuous-time linear timeinvariant uncertain systems in open or closed-loop configurations. The proposed framework offers a unified approach to handle parametric and dynamic uncertainties by solving a single Riccati equation, based on a worst-case disturbance and uncertainty model. This worst-case model is obtained by nonlinear optimization and application of the boundary Nevanlinna-Pick method. The efficacy of the proposed approach is demonstrated using an uncertain model of a nextgeneration prototype reticle stage, used for lithography. The results illustrate that an optimal compromise is achieved between fault sensitivity and rejection of modelling uncertainties and disturbances. This capability enables the clear differentiation between faults and undesired effects in the residuals, thereby enhancing fault detection reliability, ultimately contributing to improved safety and performance.
\end{abstract}

\footnotetext{${ }^{1}$ The results in this chapter constitute Contribution I in Section 1.6. The chapter is based on: [46] K. Classens, T. Ickenroth, J. van de Wijdeven, W. P. M. H. Heemels, and T. Oomen, "Closed-Loop Optimal Fault Detection for Uncertain Systems," submitted for journal publication.\\
${ }^{2}$ Paul Tacx is gratefully acknowledged for providing the model of the prototype reticle stage.
}\subsection*{2.1 Introduction}
Fault diagnosis systems play an essential role in modern engineering systems, which are continuously growing in complexity. Examples include high-precision production equipment [38, 40] and aerospace applications [170, 288], all of which require increased focus on improving system safety and reliability. Given the inherent variability and uncertainty in real-world systems, the question is not whether a machine will fail without proper monitoring and maintenance, but rather, when it will fail. Therefore, timely detection and identification of faults is crucial to mitigate the risk of performance degradation, damage, and threats to human safety. The knowledge gained from diagnostic systems can be used to schedule maintenance optimally, thereby reducing both risk and downtime. In this context, effective fault detection methodologies play a vital role in ensuring the reliability and safety of complex engineering systems.

Modern complex engineering systems typically operate in closed-loop configurations, where feedback mechanisms are essential for achieving the desired performance. The models of these systems are inherently uncertain due to factors such as limited estimation accuracy, simplifications and assumptions, or system variability, which can significantly impact fault diagnosis performance. Consequently, there is need for advanced fault diagnosis techniques that account for the closed-loop dynamics and can effectively manage these uncertainties.

Driven by the growing demand for more safe and reliable systems, fault diagnosis approaches have been substantially developed. Both data-driven methods [61, 94, 137] and model-based methods [35, 63, 94, 137, 262] have shown considerable progress. Among these, observer-based methods as considered in this chapter have gained substantial attention for their effectiveness in detecting various types of faults [35, 79]. Building on these methods, comprehensive strategies have been developed for fault detection and isolation (FDI) [35, 63, 262].

The key challenge in fault detection (FD) involves distinguishing faults from unknown disturbances. It is widely acknowledged that achieving satisfactory performance in model-based fault diagnosis systems requires a delicate balance between fault sensitivity and disturbance rejection [62]. Several optimal fault diagnosis methods have been developed for linear time-invariant (LTI) systems, including factorization-based techniques [62, 142], often implemented through the solution of a Riccati equation [157]. Alternatively, $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ techniques have been utilized, employing LMI synthesis [134, 156, 248, 270]. These methods are optimal in the sense that the residual is as sensitive to faults as possible provided that the disturbance and plant model are exactly known.

Robust methods have been developed to explicitly address modeling uncertainty. Among these, many utilize the $\mathcal{H}_{\infty}$ criterion, which is optimized using $\mu$-synthesis [216,242]. Other methods involve $\mathcal{H}_{\infty}$ model-matching techniques, solved through Linear Matrix Inequality (LMI) optimization [285]. Additionally, $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ criteria are employed and addressed with LMI solutions [125, 269],\\
sometimes in combination with $\mu_{g}$-analysis [178].\\[0pt]
Contrary to methods for LTI systems without uncertainty [62, 157, 248], robust methods found in existing literature tend to be either conservative or complicated to apply. For example, methods that rely solely on an $\mathcal{H}_{\infty}$ criterion do not directly account for $\mathcal{H}_{-}$fault sensitivity, which has to be analyzed a posteriori. Model-matching techniques often fail to guarantee optimality in terms of $\mathcal{H}_{-}$fault sensitivity, as their effectiveness heavily depends on the reference model. Additionally, these methods are often difficult to extend for fault isolation purposes. In general, many fault diagnosis methods are developed for open-loop systems and therefore not tailored to closed-loop systems. By means of a motivating example, it is shown that taking the closed loop into account is instrumental for achieving reliable results.

Although several important steps have been taken towards fault diagnosis for complex systems, at present a method for optimal detection of faults in uncertain closed-loop systems is lacking. This chapter aims to develop an optimal $\mathcal{H}_{i} / \mathcal{H}_{\infty}$ solution to the fault detection filter design problem for LTI multi-input multioutput (MIMO) uncertain closed-loop systems. The scope is confined to the fault detection task, however, yet can be readily expanded for isolation purposes. The solution, solved using a single Riccati equation, is based on an upper bound of the uncertainty and disturbance model and achieves an optimal compromise between the rejection of disturbances and modeling uncertainty with respect to fault sensitivity.

The upper-bound model is derived from worst-case gain analysis for systems with mixed uncertainties, encompassing both dynamic and parametric uncertainties. This problem is known to be NP-hard [26]. To address this, lower and upper bounds are calculated using techniques such as skewed- $\mu$ power iterations [12, 129, 212] and convex optimization employing D-G scaling [12, 130, 188, 212]. Skewed- $\mu$ power iterations use a heuristic to determine the parameter or complex matrix value corresponding to the worst-case lower bound at a specific frequency. A stable Linear Time-Invariant (LTI) sample is constructed via interpolation [286]. Extending this concept, the method proposed in [195, 196] constructs worst-case mixed uncertainty samples that maximize gain across multiple frequencies, utilizing nonlinear optimization and boundary NevanlinnaPick (BNP) interpolation [14]. This approach yields a stable, norm-bounded LTI uncertainty sample that interpolates a collection of matrix samples, providing a worst-case upper bound for the uncertainty and disturbance model.

In summary, the key contributions are outlined as follows.\\
C1 An optimal solution is provided to the $\mathcal{H}_{i} / \mathcal{H}_{\infty}$ fault detection problem for closed-loop MIMO LTI uncertain systems. To this end, the uncertainty is extracted and a worst-case upper bound is constructed which allows to solve the problem using a single Riccati equation.

C2 The efficacy of the proposed approach is shown using a simulation model\\
of a next-generation prototype reticle stage.\\
The subsequent sections of this chapter are organized as follows: After the preliminaries, the fault detection filter optimization problem is formulated for closed-loop uncertain LTI systems. Additionally, a motivating example is examined that illustrates the need for a closed-loop formulation. Subsequently, a solution is proposed that optimally addresses the filter optimization problem. Following this, a numerical example is presented, demonstrating the applica- tion of the proposed solution on next-generation prototype reticle stage used in lithography. Finally, the chapter concludes by summarizing key findings.

\subsection*{2.2 Notation and preliminaries}
The sets of real numbers and nonnegative real numbers are indicated by $\mathbb{R}$ and $\mathbb{R}_{\geq 0}$. By $\|\cdot\|_{2}$ the Euclidean norm is defined. The maximum and minimum singular values of the matrix $A$ are denoted by $\bar{\sigma}(A)$ and $\underline{\sigma}(A)$, respectively. The real-rational subspace of $\mathcal{H}_{\infty}$ is denoted by $\mathcal{R} \mathcal{H}_{\infty}$. The signal $y \in \mathcal{L}_{2}$ if $\|y\|_{2}^{2}=\int_{0}^{\infty} y^{\top}(t) y(t) \mathrm{d} t<\infty$. The signal $y \in \mathcal{L}_{2 e}$ if $\|y\|_{2 T}^{2}=\int_{0}^{T} y^{\top}(t) y(t) \mathrm{d} t< \infty, T \in \mathbb{R}_{\geq 0}$. A transfer function $N$ is called inner if $N \in \mathcal{R} \mathcal{H}_{\infty}$ and $N^{H} N=I$ and co-inner if $N \in \mathcal{R} \mathcal{H}_{\infty}$ and $N N^{H}=I$. A transfer function $M$ is called outer if $M \in \mathcal{R} \mathcal{H}_{\infty}$ and has full row normal rank and has no open right half plane zeros.

Definition 2.1. (Linear fractional transformation) For matrices $N$ and $M= \left[\begin{array}{ll}M_{11} & M_{12} \\ M_{21} & M_{22}\end{array}\right]$ of appropriate partitioning, the lower linear fractional transformation (LFT) is defined as $\mathcal{F}_{l}(M, N)=M_{11}+M_{12} N\left(I-M_{22} N\right)^{-1} M_{21}$ and the upper LFT as $\mathcal{F}_{u}(M, N)=M_{22}+M_{21} N\left(I-M_{11} N\right)^{-1} M_{12}$, under the assumption that the involved matrix inverses exists.

Consider uncertainties whose values on the imaginary axis admit the structure $\boldsymbol{\Delta}_{s}=\left\{\operatorname{diag}\left(p_{1} I, \ldots, p_{n_{r}} I, \delta_{1} I, \ldots, \delta_{n_{c}} I, \Delta_{1}, \ldots, \Delta_{n_{z}}\right) \in \mathbb{C}^{p \times q}\right.$ whose blocks satisfy $p_{j} \in \mathbb{R}$ with $\left|p_{j}\right| \leq 1$ for $j=1, \ldots, n_{r}, \delta_{j} \in \mathbb{C}$ with $\left|\delta_{j}\right| \leq 1$ for $j=1, \ldots, n_{c}$, and $\Delta_{j} \in \mathbb{C}^{p_{j} \times q_{j}}$ with $\left\|\Delta_{j}\right\|_{2} \leq 1$ for $j=1, \ldots, n_{z}$. The actual set of uncertainties $\boldsymbol{\Delta}:=\left\{\Delta(s) \in \mathcal{R} \mathcal{H}_{\infty} \mid \Delta(i \omega) \in \boldsymbol{\Delta}_{s}\right.$ for all $\left.\omega \in \mathbb{R} \cup\{\infty\}\right\}$.

The structured singular value of a matrix $P$ with respect to the set $\boldsymbol{\Delta}$ is defined as


\begin{equation*}
\mu_{\boldsymbol{\Delta}}(P)=\frac{1}{\sup \{r \mid \operatorname{det}(I-P \Delta) \neq 0 \text { for all } \Delta \in r \boldsymbol{\Delta}\}} \tag{2.1}
\end{equation*}


The following definitions are used in this chapter [63, 157, 233, 286].\\
Definition 2.2. (Minimum gain) The smallest gain of the continuous-time LTI system $G$ : $\mathcal{L}_{2 e} \rightarrow \mathcal{L}_{2 e}$, that is the $\mathcal{H}_{-}$index, is defined as


\begin{equation*}
\|G\|_{-}=\inf _{\omega \in \mathbb{R}_{\geq 0}} \underline{\sigma}(G(j \omega)) . \tag{2.2}
\end{equation*}


The minimum gain is not a norm and therefore named the $\mathcal{H}_{-}$index.\\
Definition 2.3. (Maximum gain) The $\mathcal{H}_{\infty}$ norm of the continuous-time LTI system $G: \mathcal{L}_{2 e} \rightarrow \mathcal{L}_{2 e}$, denoted as $\|G\|_{\infty}$, is given by


\begin{equation*}
\|G\|_{\infty}=\sup _{\omega \in \mathbb{R}_{\geq 0}} \bar{\sigma}(G(j \omega)) \tag{2.3}
\end{equation*}


Lemma 2.4. (Robust Stability) Consider the system $N=\left[\begin{array}{l|l}N_{11} & N_{12} \\ \hline N_{21} & N_{22}\end{array}\right]$ in the $N \Delta$-structure, i.e., such that $\left[\begin{array}{c}u_{\Delta} \\ z\end{array}\right]=N\left[\begin{array}{c}y_{\Delta} \\ w\end{array}\right]$ and $y_{\Delta}=\Delta u_{\Delta}$ with $\Delta \in \Delta$. Assume that the nominal system $N_{22}$ and the perturbations $\Delta$ are stable. Then, the uncertain system is stable for all plants in the model set, i.e., $\mathcal{F}_{u}(N, \Delta)$ is stable $\forall \Delta \in \boldsymbol{\Delta}$, if and only if $\Leftrightarrow \mu_{\Delta}\left(N_{11}\right)<1, \forall \omega$.

Proof. The proof is provided in [233, Chapter 8].\\
Lemma 2.5. (Left Coprime Factorization) The transfer matrices $M, N \in \mathcal{R} \mathcal{H}_{\infty}$ are called left coprime over $\mathcal{R} \mathcal{H}_{\infty}$ if there exist two transfer matrices $X, Y \in \mathcal{R} \mathcal{H}_{\infty}$ such that the Bezout Identity holds, i.e.,

$$
M X+N Y=I
$$

Let $G(s)$ be a proper real rational transfer matrix. A left coprime factorization (LCF) of system $G$ is a factorization $G=M^{-1} N$, where $M$ and $N$ are leftcoprime over $\mathcal{R} \mathcal{H}_{\infty}$. Let $G=\left[\begin{array}{c|c}A & B \\ \hline C & D\end{array}\right]$ be a detectable state-space realization of $G$ and $L$ be a matrix with appropriate dimensions such that $A+L C$ is stable, then a left coprime factorization of $G$ is given by state space representation

$$
\left[\begin{array}{cc}
M & N
\end{array}\right]:=\left[\begin{array}{c|cc}
A+L C & L & B+L D \\
\hline C & I & D
\end{array}\right] \in \mathcal{R} \mathcal{H}_{\infty} .
$$

Proof. The proof is provided in [286, Chapter 13].\\
Lemma 2.6. (Co-inner-outer factorization) Let $G \in \mathcal{R} \mathcal{H}_{\infty}^{p \times m}$ and assume $p \leq m$. Then, there exists a $L C F G=\tilde{M}^{-1} \tilde{N}$ such that $\tilde{N}$ is a co-inner and $\tilde{M}$ is coouter if and only if $G G^{H}>0$ on the $j \omega$-axis, including at $\infty$. This factorization is unique up to a constant unitary multiple. Assume that $G=\left[\begin{array}{c|c}A-j \omega I & B \\ \hline C & D\end{array}\right]$ is detectable and that $G=\left[\begin{array}{c|c}A & B \\ \hline C & D\end{array}\right]$ has full row rank for all $\omega \in \mathbb{R}_{\geq 0}$. Then, a particular realization of the desired co-inner-outer factorization (CIOF) is

\[
\left[\begin{array}{cc}
\tilde{M} & \tilde{N}
\end{array}\right]:=\left[\begin{array}{c|cc}
A+L C & L & B+L D  \tag{2.4}\\
\hline R^{-1 / 2} C & R^{-1 / 2} & R^{-1 / 2} D
\end{array}\right] \in \mathcal{R} \mathcal{H}_{\infty},
\]

where $R=D D^{\top}>0$, and $L=-\left(B D^{\top}+Y C^{\top}\right)^{\top} R^{-1}$ and $Y \geq 0$ be the stabilizing solution to the Riccati equation

$$
\begin{array}{r}
\left(A-B D^{\top} R^{-1} C\right) Y+Y\left(A-B D^{\top} R^{-1} C\right)^{\top}- \\
Y C^{\top} R^{-1} C Y+B\left(I-D^{\top} R^{-1} D\right) B^{\top}=0,
\end{array}
$$

such that $A-B D^{\top} R^{-1} C-Y C^{T} R^{-1} C$ is stable.

Proof. The proof is provided in [286, Chapter 13]. \(\square\)

\subsection*{2.3 Problem formulation}
First, the optimal fault detection problem is formulated for uncertain closed-loop systems. Subsequently, a motivating example is given to show the necessity for closed-loop formulations.

\subsection*{2.3.1 $\mathcal{H}_{i} / \mathcal{H}_{\infty}$-problem formulation}
Consider the input-output representation of the uncertain LTI processes described by


\begin{equation*}
y=G_{u}(s, \Delta) u+G_{d}(s, \Delta) d+G_{f}(s, \Delta) f \tag{2.5}
\end{equation*}


where $G_{u}(s, \Delta), G_{d}(s, \Delta)$ and $G_{f}(s, \Delta)$ are uncertain transfer function matrices from the Laplace transformed time-dependent control input $u$, disturbance $d$, and fault $f$, to the output $y$. The time-dependent vectors are $y(t) \in \mathbb{R}^{n_{y}}$, $u(t) \in \mathbb{R}^{n_{u}}, d(t) \in \mathbb{R}^{n_{d}}$, and $f(t) \in \mathbb{R}^{n_{f}}$, respectively. The modelling uncertainty is denoted by $\Delta \in \Delta$ and can be parametric or dynamic with suitable dimensions. Throughout, the Laplace operator $s$ is omitted when clear from context. The system (2.5) is controlled to follow a reference $r(t) \in \mathbb{R}^{n_{y}}$ by means of a robustly stabilizing feedback controller $C(s)$, i.e., $u=C(r-y)$, see Figure 2.1. Substitution of the feedback relation into (2.5) results in the closed-loop input-output relation for linear uncertain systems, given by


\begin{equation*}
y=S_{\Delta}\left(G_{u} C r+G_{d} d+G_{f} f\right) \tag{2.6}
\end{equation*}


where $S_{\Delta}=\left(I+G_{u} C\right)^{-1}$ is the uncertain sensitivity function.\\
The closed-loop system is augmented with a fault detection system which takes as inputs the control input $u$ and output $y$. The fault detection system generates residual signals $\epsilon(t) \in \mathbb{R}^{n_{y}}$ that allow to detect faults $f$, despite the influence of the external disturbances $r$ and $d$. All residual generators can be parameterized as [64]


\begin{equation*}
\epsilon=R(s)\left(\tilde{M}_{u}(s) y-\tilde{N}_{u}(s) u\right), \tag{2.7}
\end{equation*}


\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-044}
\captionsetup{labelformat=empty}
\caption{Figure 2.1: Generic fault detection configuration for uncertain closed-loop controlled systems. The control input $u$ and output $y$ form the inputs for the fault detection (FD) system which generates the residual signal $\epsilon$.}
\end{center}
\end{figure}

where $R(s) \in \mathcal{R} \mathcal{H}_{\infty}^{n_{y} \times n_{y}}$ is a post-filter of the pre-residual $\tilde{\epsilon}=\tilde{M}_{u}(s) y- \tilde{N}_{u}(s) u$, which in time-domain is $\tilde{\epsilon}(t) \in \mathbb{R}^{n_{y}}$. The transfer function matrices $\tilde{M}_{u}(s), \tilde{N}_{u}(s) \in \mathcal{R} \mathcal{H}_{\infty}$ form a left coprime factorization of the nominal plant $G_{u}(s, 0)$, i.e., $G_{u}(s, 0)=\tilde{M}_{u}^{-1} \tilde{N}_{u}$. The uncertain feedback system (2.6) augmented with the fault detection system (2.7) is graphically depicted in Figure 2.1.

Let the uncertain part of the plant $G_{u}(s, \Delta)$ be defined as


\begin{equation*}
\tilde{G}_{u}(s, \Delta):=G_{u}(s, \Delta)-G_{u}(s, 0) . \tag{2.8}
\end{equation*}


Substitution of (2.6), the control law $u=C(r-y)$, and (2.8) into (2.7), gives the residual dynamics for uncertain closed-loop systems as

$$
\begin{aligned}
\epsilon= & R \tilde{M}_{u}\{\underbrace{\tilde{G}_{u}(s, \Delta) C S_{\Delta}}_{T_{\tilde{\epsilon} r}^{\Delta}} r+\underbrace{\left(G_{d}(s, \Delta)-\tilde{G}_{u}(s, \Delta) C S_{\Delta} G_{d}(s, \Delta)\right.}_{T_{\tilde{\epsilon} d}^{\Delta}}) d \\
& +\underbrace{\left(G_{f}(s, \Delta)-\tilde{G}_{u}(s, \Delta) C S_{\Delta} G_{f}(s, \Delta)\right.}_{T_{\tilde{\epsilon} f}^{\Delta}}) f\}
\end{aligned}
$$

To enhance readability, let $T_{\tilde{\epsilon} r}(s, \Delta)=\tilde{M}_{u} T_{\tilde{\epsilon} r}^{\Delta}, T_{\tilde{\epsilon} d}(s, \Delta)=\tilde{M}_{u} T_{\tilde{\epsilon} d}^{\Delta}$ and $T_{\tilde{\epsilon} f}(s, \Delta)=\tilde{M}_{u} T_{\tilde{\epsilon} f}^{\Delta}$ be defined as the uncertain transfers from the reference\\
$r$, the disturbances $d$, and the faults $f$, to the pre-residual $\tilde{\epsilon}$. Then, the residual dynamics for uncertain closed-loop systems can be written as

\[
\epsilon=R \underbrace{\left[\begin{array}{cc}
T_{\tilde{\epsilon} r}(s, \Delta) & T_{\tilde{\epsilon} d}(s, \Delta)
\end{array}\right]}_{\tilde{G}_{d}(s, \Delta)} \underbrace{\left[\begin{array}{c}
r  \tag{2.9}\\
d
\end{array}\right]}_{\tilde{d}}+R T_{\tilde{\epsilon} f}(s, \Delta) f
\]

where $\tilde{G}_{d}(s, \Delta)$ describes the transfer from the extended disturbance input $\tilde{d}= \begin{array}{ll}{[r} & d]^{\top}\end{array}$ to the pre-residual $\tilde{\epsilon}$.

The residual dynamics clearly show the inherent trade-off between sensitivity to faults and robustness against external disturbances and the effect of modelling uncertainties. It is desirable to minimize $R \tilde{G}_{d}(s, \Delta)$ for all $\Delta \in \Delta$ in order to reduce the impact of $r$ and $d$ on the residual. Conversely, it is also desirable to maximize $R T \tilde{\epsilon} f(s, \Delta)$ for all $\Delta \in \Delta$ to enhance fault sensitivity.

Remark 2.7. The residual dynamics in (2.9) are equal to, see Appendix 2.A,

$$
\epsilon=R \tilde{M}_{u} S^{-1} S_{\Delta}\left(\tilde{G}_{u}(s, \Delta) C S r+G_{d}(s, \Delta) d+G_{f}(s, \Delta) f\right)
$$

where $S=(I+G(s, 0) C)^{-1}$ is the nominal sensitivity. This expression is insightful since $S^{-1} S_{\Delta}$ can be factored out for $r, d$, and $f$.

A natural way to evaluate the robustness against disturbances $\tilde{d}$ is through the $\mathcal{H}_{\infty}$ norm, whereas characterizing the sensitivity of faults necessitates a more intricate approach. The singular values of a matrix give a measure for the amplification in the direction of maximum action among all directions orthogonal to the singular vectors of any larger singular value. In essence, these give a measure for amplification in the principal directions of a system. In this context, all singular values $\sigma_{i}\left(R(j \omega) T_{\tilde{\epsilon} f}(j \omega, \Delta)\right), \omega \in[0, \infty), \Delta \in \boldsymbol{\Delta}$, where $i=1, \ldots, n_{\sigma}$ and $n_{\sigma}=\min \left(n_{y}, n_{f}\right)$, together form a measure of the fault sensitivity and the corresponding singular values cover all directions of the subspace spanned by $R(j \omega) T_{\tilde{\epsilon} f}(j \omega, \Delta)$.

Given the measure for worst-case disturbance amplification and the measure for fault sensitivity, different robust $\mathcal{H}_{i} / \mathcal{H}_{\infty}$ performance indices are defined. First of all, consider the index based on the worst-case fault sensitivity as


\begin{equation*}
J_{\omega}(R)=\frac{\inf _{\Delta \in \Delta}\left(\underline{\sigma}\left(R(j \omega) T_{\tilde{\epsilon} f}(j \omega, \Delta)\right)\right)}{\left\|R(s) \tilde{G}_{d}(s, \Delta)\right\|_{\infty}} \tag{2.10}
\end{equation*}


Instead of covering only the worst-case principle direction, focusing on the worstcase in all principle directions gives a set of $n_{\sigma}$ functions per frequency $\omega$ as


\begin{equation*}
J_{i, \omega}(R)=\frac{\inf _{\Delta \in \Delta}\left(\sigma_{i}\left(R(j \omega) T_{\tilde{\epsilon} f}(j \omega, \Delta)\right)\right)}{\left\|R(s) \tilde{G}_{d}(s, \Delta)\right\|_{\infty}} \tag{2.11}
\end{equation*}


Alternatively, instead of covering the worst-case fault sensitivity in each principle direction, the entire set $\bar{\Delta} \in \boldsymbol{\Delta}$ is considered in


\begin{equation*}
J_{i, \omega, \bar{\Delta}}(R)=\frac{\sigma_{i}\left(R(j \omega) T_{\tilde{\epsilon} f}(j \omega, \bar{\Delta})\right)}{\left\|R(s) \tilde{G}_{d}(s, \Delta)\right\|_{\infty}} \tag{2.12}
\end{equation*}


Here, $\left\|R(s) \tilde{G}_{d}(s, \Delta)\right\|_{\infty}$ for all $\Delta \in \boldsymbol{\Delta}$, whereas the fault sensitivity is based on a single $\bar{\Delta} \in \Delta$. Both stem from the same set.

The objective is to find the fault detection filter that maximizes the ratio $J_{i, \omega, \bar{\Delta}}$ in (2.12) for all singular values $i$, at every frequency $\omega$, and for every realization $\bar{\Delta} \in \boldsymbol{\Delta}$.

Problem 2.8. Consider the residual dynamics (2.9) of a closed-loop uncertain system described by (2.6) and let $\gamma>0$ be a user-defined combined disturbance and uncertainty rejection level. Find a single stable transfer function matrix (TFM) $R_{\text {opt }}(s) \in \mathcal{R} \mathcal{H}_{\infty}^{n_{y} \times n_{y}}$ such that


\begin{equation*}
R_{\mathrm{opt}}(s)=\underset{R(s) \in \mathcal{R} \mathcal{H}_{\infty}}{\arg \sup }\left\{\sigma_{i}\left(R(j \omega) T_{\tilde{\epsilon} f}(j \omega, \bar{\Delta})\right) \mid\left\|R(s) \tilde{G}_{d}(s, \Delta)\right\|_{\infty} \leq \gamma\right\} \tag{2.13}
\end{equation*}


for all $i=1, \ldots, n_{\sigma}$, for all $\bar{\Delta} \in \boldsymbol{\Delta}$, and for all $\omega \in[0, \infty)$, where $\Delta \in \boldsymbol{\Delta}$.\\
Remark 2.9. Note that the introduction of $\gamma>0$ has no effect on the performance indices in (2.10) to (2.12). Since the filter $R(j \omega)$ can be scaled arbitrarily, the bound $\gamma$ merely serves as a scaling parameter that has no influence on the optimal ratio, but ensures that the optimal solution for optimization problem (2.13) is unique.

Remark 2.10. Note that the optimization problem (2.13) is multiobjective since the solution is a single $R(s)$ that solves the problem for all $i=1, \ldots, n_{\sigma}$. Hence, the optimization problem includes the special $\mathcal{H}_{\infty} / \mathcal{H}_{\infty}$ and the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ objectives


\begin{equation*}
\sup _{R(s) \in \mathcal{R} \mathcal{H}_{\infty}}\left\{\left\|R(s) T_{\tilde{\epsilon} f}(s, \Delta)\right\|_{\infty} \mid\left\|R(s) \tilde{G}_{d}(s, \Delta)\right\|_{\infty} \leq \gamma\right\}, \tag{2.14}
\end{equation*}


for all $\Delta \in \boldsymbol{\Delta}$ and


\begin{equation*}
\sup _{R(s) \in \mathcal{R} \mathcal{H}_{\infty}}\left\{\left\|R(j \omega) T_{\tilde{\epsilon} f}(j \omega, \Delta)\right\|_{-} \mid\left\|R(s) \tilde{G}_{d}(s, \Delta)\right\|_{\infty} \leq \gamma\right\}, \tag{2.15}
\end{equation*}


for all $\Delta \in \boldsymbol{\Delta}$, respectively.\\
Remark 2.11. (Special case: open loop) Although the framework presented in this study focuses on uncertain closed-loop systems, the same methodology and solution are equally applicable to open-loop uncertain systems. This requires deriving the residual dynamics in (2.9) for uncertain open-loop systems, yielding a residual $\epsilon$ that depends on the input $u$ rather than the reference $r$.

Remark 2.12. (Special case: nominal systems) Note that when there is no uncertainty, i.e., $\Delta=0$ and thus $T_{\tilde{\epsilon} r}^{\Delta}=0, T_{\tilde{\epsilon} d}^{\Delta}=0$, and $T_{\tilde{\epsilon} f}^{\Delta}=0$, the residual dynamics in (2.9) simplify to the same expression for closed (and open)-loop nominal systems, that is

$$
\epsilon=R \tilde{M}_{u}\left(G_{d} d+G_{f} f\right)
$$

Hence, the residual only depends on disturbances $d$ and possible faults $f$, and is independent of the reference $r$ [37]. In this case, the FD filter design problem is equivalent for open and closed-loop nominal systems.

\subsection*{2.3.2 Motivating example}
The importance of adopting a closed-loop formulation for fault diagnosis becomes evident if there is a discrepancy between the actual system and the system used for fault diagnosis filter synthesis. This necessity is illustrated by the following example where the optimal solution to (2.13) for the open loop is applied in a closed-loop scenario. In Section 2.4 it is shown how to obtain the optimal solution to (2.13).

Consider the single-input single-output (SISO) system from [154], subject to the additive disturbance $d$ and fault $f$ with output


\begin{equation*}
y=\frac{1}{s+2+\alpha} u+\frac{s+1}{s+2+\alpha} d+\frac{15}{s+2+\alpha} f \tag{2.16}
\end{equation*}


where $\alpha \in[-1,1]$ is an uncertain parameter. The optimal open-loop fault diagnosis filter $R_{\mathrm{ol}}$ in the sense of (2.13), achieves $\bar{\sigma}\left(R(s) \tilde{G}_{d_{\mathrm{ol}}}(\omega, \Delta)\right)=1$ over the entire frequency axis, with $\tilde{G}_{d_{\mathrm{ol}}}(s, \Delta)$ the transfer function from $d_{\mathrm{ol}}=\left[\begin{array}{ll}u & d\end{array}\right]^{\top}$ to $\tilde{\varepsilon}$, and is given by

$$
R_{\mathrm{ol}}(s)=\frac{s^{2}+3 s+2}{s^{2}+3.078 s+2.236}
$$

In the closed-loop scenario, with robustly stabilizing controller $C=\frac{s+0.2 \pi}{s}$, the optimal post-filter $R_{\mathrm{cl}}$ in the sense of (2.13), is

$$
R_{\mathrm{cl}}(s)=\frac{s^{3}+4 s^{2}+4.628 s+1.257}{s^{3}+4.105 s^{2}+4.054 s+0.8886}
$$

To illustrate that this filter optimally solves the closed-loop uncertain fault detection problem, Figures 2.2 and 2.3 show the relations between inputs $\left[\begin{array}{ll}r & d\end{array}\right]^{\top}$ and residual $\epsilon$ when the optimal closed-loop filter $R_{\mathrm{cl}}$ is used ( - ) and when the open-loop optimal filter $R_{\mathrm{ol}}$ (一) is used in the closed-loop scenario. Figure 2.4 displays the singular values, i.e., $\bar{\sigma}\left(R\left[T_{\bar{\epsilon} r}(s, \Delta) T_{\bar{\epsilon} d}(s, \Delta)\right]\right)$ with $R=R_{\mathrm{cl}}$ in (—) and $R=R_{\mathrm{ol}}$ in (—). Clearly, the open-loop filter $R_{\mathrm{ol}}$ does not satisfy the

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-048}
\captionsetup{labelformat=empty}
\caption{Figure 2.2: Closed-loop transfer \$r \textbackslash rightarrow \textbackslash epsilon: R(s) T\_\{\textbackslash tilde\{\textbackslash epsilon} r\}(s, \textbackslash Delta)\$ with the optimal $R$ filter based on the closed-loop (-) and open-loop (-) formulation. Both filters are then applied to system (2.16) operating in closed loop. The design constraint $\gamma=1$ is shown in (---).\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-048(1)}
\captionsetup{labelformat=empty}
\caption{Figure 2.3: Closed-loop transfer \$d \textbackslash rightarrow \textbackslash epsilon: R(s) T\_\{\textbackslash tilde\{\textbackslash epsilon} d\}(s, \textbackslash Delta)\$ with the optimal $R$-filter based on the closed-loop (-) and open-loop (-) formulation. Both filters are then applied to system (2.16) operating in closed loop. The design constraint $\gamma=1$ is shown in (---) which is clearly violated for the filter based on the open-loop approach.\}\end{center}
\end{figure}

bound $\gamma=1$ in the closed-loop scenario, hence, using the open-loop formulation in a closed-loop setting results in violated specifications, which clearly shows the necessity for closed-loop synthesis for uncertain systems operating in closed loop.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-049}
\captionsetup{labelformat=empty}
\caption{Figure 2.4: Singular value \$\textbackslash bar\{\textbackslash sigma}\textbackslash left(R(s) \textbackslash tilde\{G\}\_\{d\}(s, \textbackslash Delta)\textbackslash right)\$ with the optimal $R$-filter based on the closed-loop (-) formulation and open-loop (-) formulation. Both filters are then applied to system (2.16) operating in closed loop. The design constraint $\gamma=1$ is shown in (---) which is clearly violated for the filter based on the open-loop approach. The effect of exogenous disturbances $r$ and $d$ is homogenized for the entire frequency axis with the closed-loop approach.\}\end{center}
\end{figure}

\subsection*{2.4 Robust closed-loop fault detection solution}
In this section, the solution to the robust fault detection filter optimization problem (2.13) is presented, which is contribution 2.1. First, an upper-bound envelope is established to encapsulate the worst-case scenario of the uncertain exogenous disturbances. Subsequently, this envelope is used to solve the optimal robust fault detection filter design problem, followed by several remarks.

Lemma 2.13. Let $\tilde{d}_{1}$ and $\tilde{d}_{2}$ be unitary input vectors, i.e., $\left\|\tilde{d}_{1}\right\|_{2}=\left\|\tilde{d}_{2}\right\|_{2}=1$, then $\bar{G}_{d}(s) \in \mathcal{R} \mathcal{H}_{\infty}^{n_{y} \times\left(n_{y}+n_{d}\right)}$ is defined as the upper-bound envelope of $\tilde{G}_{d}(s, \Delta)$ if for all vectors with $\left\|\tilde{d}_{2}\right\|_{2}=1$ the following condition is satisfied


\begin{equation*}
\left\|\tilde{G}_{d}(j \omega, \Delta) \tilde{d}_{1}\right\|_{2} \leq\left\|\bar{G}_{d}(j \omega) \tilde{d}_{2}\right\|_{2} \quad \forall \omega, \forall \Delta \in \boldsymbol{\Delta}, \tag{2.17}
\end{equation*}


where $\tilde{d}_{1}$ a unitary vector such that the output of the particular realization of $\tilde{G}_{d}(j \omega, \Delta) \tilde{d}_{1}$ matches the direction of $\bar{G}_{d}(j \omega) \tilde{d}_{2}$. The upper-bound envelope condition (2.17) is satisfied if $\bar{\sigma}\left(\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta)\right) \leq 1$ for all $\omega$ and all $\Delta \in \boldsymbol{\Delta}$, where $\bar{G}_{\text {do }}$ is the outer of $\bar{G}_{d}$.

Proof. If

$$
\bar{\sigma}\left(\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta)\right) \leq 1, \quad \forall \omega, \forall \Delta \in \boldsymbol{\Delta},
$$

then

$$
\sup _{\left\|d_{1}(j \omega)\right\|_{2}=1}\left\|\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta) d_{1}(j \omega)\right\|_{2} \leq 1, \quad \forall \omega, \forall \Delta \in \boldsymbol{\Delta} .
$$

Since any $d_{2}$ satisfies $\left\|\tilde{d}_{2}\right\|_{2}=1$,

$$
\begin{array}{r}
\sup _{\left\|d_{1}(j \omega)\right\|_{2}=1}\left\|\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta) d_{1}(j \omega)\right\|_{2} \leq\left\|\tilde{d}_{2}(j \omega)\right\|_{2}, \\
\forall\left\|\tilde{d}_{2}(j \omega)\right\|_{2}=1, \forall \omega, \forall \Delta \in \boldsymbol{\Delta} .
\end{array}
$$

Using the inner of $\bar{G}_{d}(j \omega)$,

$$
\begin{array}{r}
\sup _{\left\|d_{1}(j \omega)\right\|_{2}=1}\left\|\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta) d_{1}(j \omega)\right\|_{2} \leq\left\|\bar{G}_{d i}(j \omega) \tilde{d}_{2}(j \omega)\right\|_{2}, \\
\forall\left\|\tilde{d}_{2}(j \omega)\right\|_{2}=1, \forall \omega, \forall \Delta \in \boldsymbol{\Delta} .
\end{array}
$$

Hence, for all $\left\|d_{1}(j \omega)\right\|_{2}=1$,

$$
\begin{array}{r}
\left\|\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta) d_{1}(j \omega)\right\|_{2} \leq\left\|\bar{G}_{d i}(j \omega) \tilde{d}_{2}(j \omega)\right\|_{2} \\
\forall\left\|\tilde{d}_{2}(j \omega)\right\|_{2}=1, \forall \omega, \forall \Delta \in \boldsymbol{\Delta}
\end{array}
$$

Since for each $\Delta, d_{1}(j \omega)$ is such that $\tilde{G}_{d}(j \omega, \Delta) d_{1}(j \omega)$ points in the same direction as $\bar{G}_{d}(j \omega) d_{2}(j \omega)$, a scaling factor $g$ is introduced such that

$$
\tilde{G}_{d}(j \omega, \Delta) d_{1}(j \omega)=g \bar{G}_{d}(j \omega) d_{2}(j \omega), \quad \forall\left\|\tilde{d}_{2}(j \omega)\right\|_{2}=1, \forall \omega, \forall \Delta \in \boldsymbol{\Delta}
$$

with $g \leq 1$. Since these point in the same direction, clearly

$$
\begin{array}{r}
\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta) d_{1}(j \omega)=g \bar{G}_{d o}^{-1}(j \omega) \bar{G}_{d}(j \omega) d_{2}(j \omega), \\
\forall\left\|\tilde{d}_{2}(j \omega)\right\|_{2}=1, \forall \omega, \forall \Delta \in \boldsymbol{\Delta},
\end{array}
$$

point in the same direction. Thus,

$$
\left\|\tilde{G}_{d}(j \omega, \Delta) d_{1}(j \omega)\right\|_{2} \leq\left\|\bar{G}(j \omega) \tilde{d}_{2}(j \omega)\right\|_{2}, \quad \forall\left\|\tilde{d}_{2}(j \omega)\right\|_{2}=1, \forall \omega, \forall \Delta \in \boldsymbol{\Delta}
$$

Lemma 2.14. Let $\tilde{d}_{1}$ and $\tilde{d}_{2}$ be unitary input vectors, then $\bar{G}_{d}(s) \in \mathcal{R} \mathcal{H}_{\infty}^{n_{y} \times\left(n_{y}+n_{d}\right)}$ is defined as a tight upper-bound envelope of $\tilde{G}_{d}(s, \Delta)$ if for all $\left\|\tilde{d}_{2}\right\|=1$ the following condition is satisfied


\begin{equation*}
\sup _{\Delta \in \Delta}\left\|\tilde{G}_{d}(j \omega, \Delta) \tilde{d}_{1}\right\|_{2}=\left\|\bar{G}_{d}(j \omega) \tilde{d}_{2}\right\|_{2} \quad \forall \omega, \tag{2.18}
\end{equation*}


where $\tilde{d}_{1}$ a unitary vector such that the output of the particular realization of $\tilde{G}_{d}(j \omega, \Delta) \tilde{d}_{1}$ matches the direction of $\bar{G}_{d}(j \omega) \tilde{d}_{2}$. The upper-bound envelope condition (2.18) is satisfied if and only if $\sup _{\Delta \in \Delta}\left\{\sigma_{i}\left(\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta)\right)\right\}=1$ for each $i=1, \ldots, n_{\sigma}$, at every $\omega$, where $\bar{G}_{d o}$ is the outer of $\bar{G}_{d}$.

Proof. Start from the observation that if all the worst-case singular values $i= 1, \ldots, n_{\sigma}$ are equal to one for each $i=1, \ldots, n_{\sigma}$, then,

$$
\sup _{\Delta \in \Delta} \bar{\sigma}_{i}\left(\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta)\right)=1, \quad \forall \omega,
$$

is equivalent to

$$
\begin{aligned}
\sup _{\Delta \in \boldsymbol{\Delta}}\left\|\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta) d_{1}(j \omega)\right\|_{2} & =\left\|\bar{G}_{d i}(j \omega) \tilde{d}_{2}(j \omega)\right\|_{2} \\
& \forall\left\|\tilde{d}_{2}(j \omega)\right\|_{2}=1, \forall \omega .
\end{aligned}
$$

where $\tilde{d}_{1}$ a unitary vector such that the output of the particular realization of $\tilde{G}_{d}(j \omega, \Delta) \tilde{d}_{1}$ matches the direction of $\bar{G}_{d}(j \omega) \tilde{d}_{2}$. Hence, clearly $\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta) d_{1}(j \omega)$ points in the same direction as $\bar{G}_{d o}^{-1}(j \omega) \bar{G}_{d}(j \omega) d_{2}(j \omega)= \bar{G}_{d i}(j \omega) d_{2}(j \omega)$. Since these point in the same direction, clearly

$$
\sup _{\Delta \in \Delta}\left(\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta) d_{1}(j \omega)\right)=\bar{G}_{d i}(j \omega) d_{2}(j \omega), \quad \forall\left\|\tilde{d}_{2}(j \omega)\right\|_{2}=1, \forall \omega
$$

Then, premultiplication with $\bar{G}_{d o}(j \omega)$ gives

$$
\sup _{\Delta \in \Delta}\left(\tilde{G}_{d}(j \omega, \Delta) d_{1}(j \omega)\right)=\bar{G}(j \omega) d_{2}(j \omega), \quad \forall\left\|\tilde{d}_{2}(j \omega)\right\|_{2}=1, \forall \omega
$$

and therefore,

$$
\sup _{\Delta \in \Delta}\left\|\tilde{G}_{d}(j \omega, \Delta) d_{1}(j \omega)\right\|_{2}=\left\|\bar{G}(j \omega) d_{2}(j \omega)\right\|_{2}, \quad \forall\left\|\tilde{d}_{2}(j \omega)\right\|_{2}=1, \forall \omega,
$$

Lemma 2.14 is proven by executing the steps in opposite direction, which concludes the proof.

Hence, a tight upper bound implies that the worst-case amplification of $\tilde{G}_{d}(j \omega, \Delta)$ is equal to the amplification of $\bar{G}_{d}(j \omega)$ at every $\omega$. Next, the complete solution to the robust fault detection filter design problem is presented.

Theorem 2.15. Let the optimal fault detection filter which maximizes (2.13) be parameterized as

\[
\epsilon=R_{\mathrm{opt}}\left[\begin{array}{ll}
\tilde{M}_{u} & -\tilde{N}_{u}
\end{array}\right]\left[\begin{array}{l}
y  \tag{2.19}\\
u
\end{array}\right]
\]

and suppose the following assumptions are satisfied.

\begin{enumerate}
  \item Let $\bar{G}_{d}=\left(A, B_{d}, C, D_{d}\right)$ be written as a state-space system with $A$ Hurwitz and $(A, C)$ detectable.
  \item $D_{d}$ has full row rank.
  \item $\bar{G}_{d}(s)$ has no transmission zeros on the imaginary axis.
  \item Let $\bar{G}_{d}(s)$ be a tight upper-bound realization of $\tilde{G}_{d}(s, \Delta)$.
\end{enumerate}

Then, there exists a co-inner-outer factorization of $\bar{G}_{d}(s)=G_{d o}(s) G_{d i}(s)$ with the optimal post-filter $R_{\text {opt }}(s)=\gamma G_{\text {do }}^{-1}$ and $\tilde{M}_{u}, \tilde{N}_{u} \in \mathcal{R} \mathcal{H}_{\infty}$ any $L C F$ of the nominal system $G_{u}(s, 0)$, which achieves for all $\omega, i=1, \ldots, n_{\sigma}$, and $\bar{\Delta} \in \boldsymbol{\Delta}$

$$
\sup _{R(s) \in \mathcal{R} \mathcal{H}_{\infty}} J_{i, \omega, \bar{\Delta}}\left(R_{\mathrm{opt}}\right)=\sigma_{i}\left(R(j \omega) T_{\tilde{\epsilon} f}(j \omega, \bar{\Delta})\right)
$$

The corresponding state-space representation of $R_{\mathrm{opt}}$ is given by

\[
R_{\mathrm{opt}}(s)=\gamma\left[\begin{array}{c|c}
A+L_{0} C & L_{0}  \tag{2.20}\\
\hline R_{d}^{-1 / 2} C & R_{d}^{-1 / 2}
\end{array}\right] \in \mathcal{R} \mathcal{H}_{\infty}
\]

in which $R_{d}:=D_{d} D_{d}^{T}>0$ and $Y \geq 0$ is the stabilizing solution to the Riccati equation

$$
\begin{gathered}
\left(A-B_{d} D_{d}^{T} R_{d}^{-1} C\right) Y+Y\left(A-B_{d} D_{d}^{T} R_{d}^{-1} C\right)^{T}- \\
Y C^{T} R_{d}^{-1} C Y+B_{d}\left(I-D_{d}^{T} R_{d}^{-1} D_{d}\right) B_{d}^{T}=0
\end{gathered}
$$

such that $A-B_{d} D_{d}^{T} R_{d}^{-1} C-Y C^{T} R_{d}^{-1} C$ is stable and


\begin{equation*}
L_{0}:=-\left(B_{d} D_{d}^{T}+Y C^{T}\right)^{T} \tag{2.21}
\end{equation*}


Proof. Since (2.17) holds where $\tilde{d}_{1}$ a unitary vector such that the output of the particular realization of $\tilde{G}_{d}(j \omega, \Delta) \tilde{d}_{1}$ matches the direction of $\bar{G}_{d}(j \omega) \tilde{d}_{2}$,


\begin{equation*}
\left\|R(j \omega) \tilde{G}_{d}(j \omega, \Delta) \tilde{d}_{1}\right\|_{2} \leq\left\|R(j \omega) \bar{G}_{d}(j \omega) \tilde{d}_{2}\right\|_{2} \quad \forall \omega, \forall \Delta \in \boldsymbol{\Delta} \tag{2.22}
\end{equation*}


Then, if $R(j \omega), \tilde{G}_{d}(j \omega, \Delta)$ and $\bar{G}_{d}(j \omega)$ are stable proper transfer function matrices,

$$
\left\|R(s) \tilde{G}_{d}(s, \Delta)\right\|_{\infty} \leq\left\|R(s) \bar{G}_{d}(s)\right\|_{\infty} \quad \forall \Delta \in \Delta
$$

It follows from substitution into (2.11) that


\begin{equation*}
J_{i, \omega, \bar{\Delta}}(R)=\frac{\sigma_{i}\left(R(j \omega) T_{\tilde{\epsilon} f}(j \omega, \bar{\Delta})\right)}{\left\|R(s) \tilde{G}_{d}(s, \Delta)\right\|_{\infty}} \geq \frac{\sigma_{i}\left(R(j \omega) T_{\tilde{\epsilon} f}(j \omega, \bar{\Delta})\right)}{\left\|R(s) \bar{G}_{d}(s)\right\|_{\infty}} \tag{2.23}
\end{equation*}


Assuming a tight upper bound as in Assumption 4 in the theorem gives that

$$
\left\|R(s) \tilde{G}_{d}(s, \Delta)\right\|_{\infty}=\left\|R(s) \bar{G}_{d}(s)\right\|_{\infty}
$$

which results in equality in (2.23) as


\begin{equation*}
J_{i, \omega, \bar{\Delta}}(R)=\frac{\sigma_{i}\left(R(j \omega) T_{\tilde{\epsilon} f}(j \omega, \bar{\Delta})\right)}{\left\|R(s) \tilde{G}_{d}(s, \Delta)\right\|_{\infty}}=\frac{\sigma_{i}\left(R(j \omega) T_{\tilde{\epsilon} f}(j \omega, \bar{\Delta})\right)}{\left\|R(s) \bar{G}_{d}(s)\right\|_{\infty}} \tag{2.24}
\end{equation*}


From Assumptions 1 to 3 follows that a CIOF of $\bar{G}_{d}(s)$ exists. I.e., $\bar{G}_{d}(s)=G_{d o}(s) G_{d i}(s)$ exists, where $G_{d i}(s)$ is the co-inner matrix satisfying $G_{d i}(j \omega) G_{d i}^{T}(-j \omega)=I$ and having $\sigma\left(G_{d i}(j \omega)\right)=I$ for all $\omega$, and $G_{d o}(s)$ is coouter satisfying $G_{d o}^{-1} \in \mathcal{R} \mathcal{H}_{\infty}$ and thus $G_{d o}(s)^{-1} G_{d o}(s)=I$. Now parameterize $R(s)$ as

$$
R(s)=\gamma Q(s) G_{d o}^{-1}(s)
$$

where $Q(s) \in \mathcal{R} \mathcal{H}_{\infty}$ an arbitrary stable TFM. Substitution into (2.24) yields


\begin{align*}
J_{i, \omega, \bar{\Delta}}(R) & =\frac{\sigma_{i}\left(R(j \omega) T_{\tilde{\epsilon} f}(j \omega, \bar{\Delta})\right)}{\left\|R(s) \bar{G}_{d}(s)\right\|_{\infty}}=\frac{\sigma_{i}\left(\gamma Q(j \omega) G_{d o}^{-1}(j \omega) T_{\tilde{\epsilon} f}(j \omega, \bar{\Delta})\right)}{\left\|\gamma Q(s) G_{d i}(s)\right\|_{\infty}} \\
& =\frac{\sigma_{i}\left(Q(j \omega) G_{d o}^{-1}(j \omega) T_{\tilde{\epsilon} f}(j \omega, \bar{\Delta})\right)}{\|Q(s)\|_{\infty}}  \tag{2.25}\\
& \leq \sigma_{i}\left(G_{d o}^{-1}(j \omega) T_{\tilde{\epsilon} f}(j \omega, \bar{\Delta})\right)
\end{align*}


where it is used that $G_{d i}(j \omega) G_{d i}^{\top}(-j \omega)=I$ so that $\left\|Q(s) G_{d i}(s)\right\|_{\infty}=\|Q(s)\|_{\infty}$.\\
From the inequality in (2.25) follows that setting $R_{\text {opt }}(s)=\gamma G_{d o}^{-1}(s)$, i.e., $Q(s)=I$ gives the optimal performance. Hence, it holds that for all $\omega, \Delta \in \boldsymbol{\Delta}$, and $i=1, \ldots, n_{\sigma}$


\begin{equation*}
J_{i, \omega, \bar{\Delta}}\left(R_{\mathrm{opt}}\right)=\sigma_{i}\left(G_{d o}^{-1}(j \omega) T_{\tilde{\epsilon} f}(j \omega, \bar{\Delta})\right) \tag{2.26}
\end{equation*}


resulting in the optimal solution of Problem 2.8 for uncertain closed-loop systems. Evidently, this $R_{\text {opt }}$ also maximizes the performance index with worst-case fault sensitivity (2.11). The state-space realization of the outer term $G_{d o}^{-1}(s)$ follows directly from Lemma 2.6.

Next, a series of observations will be presented concerning the assumptions 1 to 3 . Following this, an interpretable explanation of the optimal filter is given and an analysis will be provided regarding the implications of the derived outcome with respect to assumption 4 .

Remark 2.16. While it might appear that requiring $A$ to be Hurwitz in $A s$ sumption 1 is restrictive, this is not the case. In fact, the system described by the $T F M \bar{G}_{d}(s)$ is always stable, as it characterizes the closed-loop TFM $\tilde{d} \rightarrow \tilde{\epsilon}$.

Remark 2.17. Assumption 2 implies that $n_{y} \leq n_{d}$ and every output is subject to some form of disturbance or measurement noise. It can be argued that this assumption can be made without loss of generality, as it is practically impossible to obtain perfect measurements in any system. Additionally, it is reasonable to assume that the measurement noise is independent of each other. Hence, it is reasonable to assume that the disturbance matrix $D_{d}$ has full row rank. Readers are referred to [157] for an adaptation of the state-space matrices of $G_{d}$ in case its $D$ matrix is not full row rank.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-054}
\captionsetup{labelformat=empty}
\caption{Figure 2.5: Generic fault detection configuration for uncertain closed-loop controlled systems. The control input $u$ and output $y$ form the inputs for the fault detection (FD) system which generates the residual signal $\epsilon$.}
\end{center}
\end{figure}

Remark 2.18. In numerous applications, Assumption 3 is not considered restrictive, as many systems inherently include a certain level of damping, resulting in zeros positioned away from the imaginary axis. However, in cases where Assumption 3 is not satisfied, alternative approaches are available if $\bar{G}_{d}(s)$ features zeros on the imaginary axis. Readers are directed to [101] for further details on these non-standard approaches.

The following two remarks relate to the solution itself.\\
Remark 2.19. Note that the achieved performance $J_{i, \omega, \bar{\Delta}}(R)$ is independent on the choice of observer gain matrices $L$ in the $L C F$ of the nominal plant $G(s, 0)$ used in the filter design, see Lemma 2.5.

Remark 2.20. Note that the solution is completely determined by the uncertainty and disturbance models encaptured in $\tilde{G}_{d}(s, \Delta)$ and is therefore independent of the fault model $\tilde{G}_{f}(s, \Delta)$.

The presented strategy can be interpreted as follows. First, the closed-loop configuration in Figure 2.1 is reconfigured to the form in Figure 2.5. Here, the uncertainty inherently present in the closed loop is located in the blocks $\left[T_{y r}^{\Delta}(s, \Delta) \quad T_{y d}^{\Delta}(s, \Delta)\right]$ and $T_{y f}^{\Delta}(s, \Delta)$. The LCF $\tilde{N}_{u}, \tilde{M}_{u}$ cancels the effect of the\\
nominal closed loop and the remaining robust fault-to-disturbance optimization is performed by $R$. To be precise, this filter is based on the overbound $\bar{G}_{d}(s)$ of $\tilde{M}_{u}\left[T_{y r}^{\Delta}(s, \Delta) \quad T_{y d}^{\Delta}(s, \Delta)\right]$.

In order to interpret the optimal post-filter, $R(s)=\gamma G_{d o}^{-1}(s)$, recall that the co-outer of a TFM can be interpreted as a frequency domain magnitude profile. Hence, by multiplication with the inverse of the magnitude profile, the worstcase influence of the exogenous disturbance $\tilde{d}$ on the residuals $\epsilon$ is homogenized across the entire subspace. As a consequence, the optimal solution affects the TFM from faults $f$ to residuals $\epsilon$ by an inverse weighting of this magnitude profile.

To attain the supremum as defined in (2.13) and thereby achieve optimal fault detection performance, it is imperative that the upper-bound realization satisfies Assumption 4. This condition is inherently met if $\bar{G}_{d}(s) \in \tilde{G}_{d}(s, \Delta)$ and (2.17) holds for all frequencies $\omega$ and uncertainties $\Delta \in \boldsymbol{\Delta}$. In such instances, the worst-case realization consistently bounds the other realizations across all frequencies from above. However, if the upper bound complies with (2.17) but is not tight, conservatism is introduced resulting in a suboptimal, yet still robust, filter in view of (2.13). In this case, the result is merely optimal given the conservative upper bound.

Next, the design of an optimal fault detection filter and a strategy to minimize conservatism in the upper-bound envelope is examined. Additionally, remarks are given regarding the uncertainty modeling. Subsequently, the optimal filter is computed and applied to a next-generation motion stage, and the time-domain responses for fault detection are shown.

\subsection*{2.5 Design}
This section first examines how the upper-bound envelope $\bar{G}_{d}(s)$ can be interpreted. Subsequently, it is shown how to verify whether a candidate $\bar{G}_{d}(s)$ satisfies the Lemmas 2.13 and 2.14 by means of the structured singular value. Finally, it is shown how $\bar{G}_{d}(s)$ is obtained. The proposed method based on [195] uses nonlinear optimization to find the worst-case values for uncertain parameters and matrix uncertainty samples. It then applies the boundary Nevanlinna-Pick (BNP) method [12] to create a stable, norm-bounded LTI uncertainty interpolating these samples.
\researchnote[JC][2025-10-14]{2.5节\\ 1.几何直观解释上界\\2.利用结构奇异值验证候选的上界是否满足lemma：上界和紧上界给出的数学条件\\3.如何获取上界：参考文献195、12}


\subsection*{2.5.1 Interpreting the upper-bound envelope $\overline{\boldsymbol{G}}_{\boldsymbol{d}}(s)$}
The condition in (2.17) is intuitively interpreted via the following simplified example based on a real-valued system having 2 outputs. Consider an uncertain system described by a real-valued matrix $A^{m \times n}\left(\omega_{i}, \Delta\right)$, with $n \geq m$, at a particular frequency $\omega_{i}$ with real-valued uncertainty $\Delta$. Then, the singular values

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-056}
\captionsetup{labelformat=empty}
\caption{Figure 2.6: Illustrative example to interpret the upper bound \$\textbackslash bar\{G}\_\{d\}(s)\$, let $A\left(\omega_{i}, \Delta\right)$ be a real valued uncertain matrix with 2 singular values at a specific frequency $\omega_{i}$. The ellipses spanned by $A\left(\omega_{i}, \Delta\right)$ are depicted for four different realizations of $\Delta$ in (-). The ellipse of the upper bound $\bar{A}\left(\omega_{i}\right)$, plotted in (-).\}\end{center}
\end{figure}

obtained from the singular value decomposition (SVD) $A=U \Sigma V^{\top}$ of the $m \times n$ matrix can be interpreted as the magnitude of the semi-axes of an $m$-dimensional ellipsoid in an $m$-dimensional space. Figure 2.6 illustrates this for $m=2$, where four different realizations of $A\left(\omega_{i}, \Delta\right)$ are depicted with their corresponding ellipses. The upper-bound ellipse $\bar{A}\left(\omega_{i}\right)$ encapsulates all realizations of $A\left(\omega_{i}, \Delta\right)$ in order to satisfy (2.17).

\researchnote[JC][2025-10-14]{对于奇异值分解的理解\\ 1.奇异值分解的应用范围很广泛，因此我们首先要明确是对谁进行奇异值分解，这样才有具体的物理意义：这里是对系统的传递函数矩阵$A\left(\omega_{i}, \Delta\right)$进行奇异值分解。\\2.部分前置知识：在多输入多输出（MIMO）系统中，传递函数不再是一个标量，而是一个矩阵 G(s)。这个矩阵描述了所有输入通道到所有输出通道的动态关系。\\3.疑问：为什么是在特定频率点 s = jω 上对 G(s) 进行奇异值分解，而不是对一个“所有频率”的单一对象进行分解？这涉及到奇异值分解的对象应该是什么样的\\***a.奇异值分解是一种线性代数运算，运算对象是固定的矩阵，不能直接对一个函数G(s)进行SVD，但是可以对G(s)在任意一个给定点$s0=jw$进行SVD。因此这个过程是逐频率点进行的，即选择一个频率点，计算该频率下的传递函数矩阵值（复常数矩阵），对这个复矩阵进行奇异值分解，得到该频率点下的输入方向、增益、输出方向。再选择下一个频率点进行上述操作。\\***b.因为我们在G(s)的每个频率点上都进行了一个SVD，所以SVD得到的结果（一组奇异值和奇异向量）都是频率的函数。每个频率点都有一组奇异值，每组奇异值中都有一个最大值，那这样的话，每组的最大奇异值是随着频率点不同而不同的，由此可以得到最大奇异值（最小奇异值同理）随着频率$w$变化的曲线，这就是奇异值bode图。}

\researchnote[JC][2025-10-14]{4.最大奇异值和最小奇异值代表的含义：σ̄(ω) 的曲线告诉我们，在所有可能的输入方向中，系统在哪个频率下增益最高（最敏感），相当于在最大的（ 每个频率对应一个传函，每个传函都能进行SVD并产生一组奇异值，每组奇异值都有相应的本组最大奇异值）里面选最大的（所有频率上的最大），大中之大。σ̲(ω) 的曲线则告诉我们，系统在哪个频率下增益最低（最不敏感/最“迟钝”）。方向和增益的频变特性：系统的“自然输入/输出方向”也可能随着频率而改变。在某个频率下最容易激励的模态，在另一个频率下可能并不显著。}


\methodnote[JC][2025-10-14]{结合现在的具体场景，奇异值分解的作用：扩展扰动$r$和$d$到残差$\epsilon$的扩展扰动传递矩阵$\tilde{G}_{d}(s, \Delta)$的上界称之为$\bar{G}_{d}(s)$，对其逐频率点求SVD，得到的就是输入（扩展扰动）对输出（残差）的影响随频率变化的曲线。}
\methodnote[JC][2025-10-14]{为了更好的描述输入和输出之间的映射关系，想要了解这个映射有什么特性，长什么样，那最好就先把输入归一化，这样才能排除由于输入的大小导致的输出的不同，而可以通过输出的大小直接得到映射的信息，也就是说由于输入都是一样的，产生不同的原因只能是由于映射本身。具体做法如下：\\想象所有可能的扩展扰动信号（$r$ 和 $d$ 组成的向量）的集合是一个单位球（所有范数≤1的输入）。通过矩阵 $\tilde{G}_{d}(j\omega, \Delta)$ 映射后，这个单位球会变成一个椭球（在输出空间，即残差空间中）。这个椭球的半轴长度就是该矩阵的奇异值，半轴的方向就是奇异向量的方向。}
\researchnote[JC][2025-10-14]{关于椭球的理解，不同频率、不同不确定性和椭球的对应关系：\\1.固定频率，改变不确定性 $\Delta$ 的影响，不同$\Delta$影响的是系统本身，改变的是这个系统，不同的$\Delta$其实就是不同的系统在进行对比，固定频率的话就是不同系统都在这个频率下进行对比：当 $\Delta$ 变化时，矩阵 $\tilde{G}_{d}(j\omega, \Delta)$ 也会变化，从而导致输出椭球的形状（奇异值）和方向（奇异向量） 发生改变。如图2.6所示，不同的 $\Delta$ 会产生不同形状和方向的椭圆。\\2.固定$\Delta$，改变频率的影响，就类似于bode图，由于$\Delta$相同，那么其实是同一个系统在不同频率上的对比。\\3.上界 $\bar{G}_{d}(s)$ ：上述的是系统的扩展扰动传递函数矩阵$\tilde{G}_{d}(s)$，而我想找到一个传递函数矩阵$\bar{G}_{d}(s)$，能够包络所有的扩展扰动的放大。我们的目标是找到一个确定的，不含不确定性的“包络椭球”，这个椭球在每一个频率点 $s=j\omega$ 上，都能完全包含所有 $\Delta$ 所产生的各种椭球。这个“包络椭球”就对应于 $\bar{G}_{d}(j\omega)$ 的奇异值分解，注意是每个频率点$\omega_{i}$上的所有系统不确定性$\Delta_{k}$情况下，产生的$\tilde{G}_{d}(j\omega_{i}, \Delta_{k})$都要被我的$\bar{G}_{d}(j\omega_{i})$包络住}
The co-outer of a TFM can be interpreted as a frequency domain magnitude profile. Hence, premultiplication with the inverse of the co-outer term, i.e., $\bar{A}_{o}^{-1}\left(\omega_{i}\right) \bar{A}\left(\omega_{i}\right)=\bar{A}_{i}\left(\omega_{i}\right)$, leaves an inner matrix and thus homogenizes the magnitude profile. As a consequence, applying the same transformation to $A\left(\omega_{i}, \Delta\right)$ maps all realizations within the unit ball, see Figure 2.7.
\researchnote[JC][2025-10-14]{传递函数矩阵的co-outer因子含义解释：\\$\bar{A_{o}}(\omega_{i})$是上界系统$\bar{A}(\omega_{i})$的的co-outer因子，可以将其理解为上界系统“幅度-方向”特性中的“纯幅度”部分。因此在目前的场景中，就是$\bar{G_{do}}(\omega_{i})$是上界系统$\bar{G_{d}}(s)$在频率点$s=j\omega$上的实现$\bar{G_{d}}(\omega_{i})$的纯幅度部分。}

Remark 2.21. Note that conservatism is introduced if $\bar{G}_{d}(s)$ is chosen such that the ellipsoids formed by $\bar{G}_{d}(s)$ at each frequency $\omega$ do not tightly capture the realizations of $\tilde{G}_{d}(s, \Delta)$. In such case, the magnitude profile of the realizations are shrunk further than strictly necessary by premultiplication with the co-outer of this particular $\bar{G}_{d}(s)$.
\researchnote[JC][2025-10-14]{紧上界的理解：\\Lemma 2.14 定义了什么是``紧上界''。它说，一个上界是``紧''的，如果对于每一个输出方向，不确定系统在这个方向上能达到的最大增益，正好等于上界系统在这个方向上的增益。在图2.7和co-outer变换的语境下，这个条件等价于：\[\sup_{\Delta \in \boldsymbol{\Delta}} \left\{ \sigma_i \left( \tilde{G}_{d0}^{-1}(j\omega) \tilde{G}_d(j\omega, \Delta) \right) \right\} = 1 \quad \text{for each } i = 1, \ldots, n_\sigma\]这是什么意思？\\- \(\sigma_i(\cdot)\) 代表第 \(i\) 个奇异值。\\- 这个条件要求，对于每一个主方向（每一个奇异值对应的轴），至少有一个不确定性 \(\Delta\) 的实现，使得变换后的系统 \(\tilde{G}_d(\omega)^{-1} \tilde{G}_d\) 在这个方向上的增益恰好等于1。\\在图2.7中的体现：\\一个``紧''的上界设计，会使得至少有一个变换后的实线椭圆（比如 \(\Delta\) 对应的那个）恰好与单位圆相切。如果所有实线椭圆都远离单位圆边界，躲在很靠里的位置，那么就说明我们的上界 \(\tilde{G}_d\) 设计得过于保守（即Remark 2.21所说的保守性），因为它把不确定系统的能力估计得过高了。}


\subsection*{2.5.2 Validating candidate $\overline{\boldsymbol{G}}_{\boldsymbol{d}}$ : Singular values and $\boldsymbol{\mu}$-test}
Validating whether a candidate $\bar{G}_{d}$ satisfies Lemmas 2.13 and 2.14, and thus is suitable to solve the robust fault detection problem, can be done as follows. First, the special case where $n_{y}=1$ is examined and subsequently the approach is given for arbitrary output dimension.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-057}
\captionsetup{labelformat=empty}
\caption{Figure 2.7: Illustrative example to interpret the homogenization of the magnitude profile of \$\textbackslash bar\{G}\_\{d\}(s)\$, let $A\left(\omega_{i}, \Delta\right)$ be a real valued uncertain matrix with 2 singular values at a specific frequency $\omega_{i}$. The ellipses spanned by $\bar{A}_{o}^{-1}\left(\omega_{i}\right) A\left(\omega_{i}, \Delta\right)$ are mapped within the unit ball and are graphed for four different realizations of $\Delta$ in (-). The ellipse corresponding to the homogenized upper-bound system $\bar{A}_{o}^{-1}\left(\omega_{i}\right) \bar{A}\left(\omega_{i}\right)$, plotted in (-).\}\end{center}
\end{figure}

\researchnote[JC][2025-10-14]{为什么要施加co-outer逆变换：\\通过对比变换前后来说明\\通过对上界系统和不确定系统进行相同的坐标变换（即乘以$\bar{G}_{do}^{-1}$），将复杂的椭球包络问题，转化为一个简单的单位圆包容问题。\\变换前（对应于图2.6）：我们需要判断一个大的椭球（上界）是否能够包住一堆方向各异的小椭球（不确定系统的各种实现）。这很困难，因为既要考虑大小也要考虑方向。\\变换后（图2.7所示）：那个大的椭球被“拉直”成了一个单位圆。此时，我们只需要检查所有那些小椭球是否都落在这个单位圆内部。判断标准变得极其简单：其最大奇异值是否小于等于1。\\总的来说，我们通过一个巧妙的坐标变换（乘以co-outer的逆），将鲁棒性分析中复杂的“几何包络”问题，简化成了一个标准的“范数有界”检验问题。之后，我们只需要验证 $\mu_{\Delta}(\bar{G}_{do}^{-1}(j\omega) \tilde{G}_{d}(j\omega,\Delta)) \\leq 1 $是否成立即可，这大大简化了分析和验证的流程。}

For systems with a single output $n_{y}=1$, e.g., SISO or multiple-input singleoutput (MISO) system, a singular value check over the entire frequency axis suffices, that is, condition (2.17) is satisfied if $\bar{\sigma}\left(\tilde{G}_{d}(j \omega, \Delta)\right) \leq \bar{\sigma}\left(\bar{G}_{d}(j \omega)\right)$ for all $\omega$ and $\Delta \in \Delta$. A thorough check can be performed by means of checking the structured singular value, i.e., if $\mu_{\Delta}\left(\tilde{G}_{d}(j \omega, \Delta)\right) \leq \bar{\sigma}\left(\bar{G}_{d}(j \omega)\right)$ for all $\omega$, where $\mu$ is computed with respect to the structure of $\boldsymbol{\Delta}$. In case the inequality holds, the candidate $\bar{G}_{d}$ satisfies Lemma 2.13. In case the latter holds with equality, Lemma 2.14 is satisfied.
\researchnote[JC][2025-10-14]

For systems with arbitrary number of outputs, condition (2.17) cannot be verified directly by comparing the singular values of $\bar{G}_{d}(s)$ to $\tilde{G}_{d}(s, \Delta)$. This can be understood from Figure 2.6 and is attributed to uncertainty-induced rotation of the related ellipses. To overcome this, it is required to first compute part of the solution, i.e., the co-outer factor of $\bar{G}_{d}(s)$. Then, a singular value check can be performed on the homogenized system, that is, condition (2.17) is satisfied if $\bar{\sigma}\left(\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta)\right) \leq 1$ for all $\omega$ and all $\Delta \in \boldsymbol{\Delta}$. A thorough check for all $\Delta \in \Delta$ is performed by testing the structured singular value. If the structured singular value $\mu_{\Delta}\left(\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta)\right) \leq 1$ for all $\omega$, where $\mu$ is computed with respect to the structure $\boldsymbol{\Delta}$, the candidate $\bar{G}_{d}$ satisfies Lemma 2.13.
\researchnote[JC][2025-10-14]{多输出系统和单输出系统的区别：}

To satisfy the condition in Lemma 2.14 all singular values are relevant. To this end, $\sup _{\Delta \in \Delta}\left\{\sigma_{i}\left(\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta)\right)\right\}=1$ has to hold for each $i=1, \ldots, n_{\sigma}$, at every $\omega$. The closer $\sup _{\Delta \in \Delta}\left\{\sigma_{i}\left(\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta)\right)\right\}, i=1, \ldots, n_{\sigma}$, ap-\\
proaches 1 from below, the tighter the upper-bound envelope, which implies a less conservative result. This principle is further elaborated upon in the numerical example in Section 2.6.2.

\subsection*{2.5.3 Computing candidate $\bar{G}_{d}$ : Worst-case gain and Nevanlinna-Pick interpolation}
To solve the fault detection filter optimization problem, described by Problem 2.8, a well-designed upper bound $\bar{G}_{d}(s)$ is required to achieve satisfactory performance. Two approaches are described which give an initial design that is subsequently scaled in order to satisfy the condition in Lemma 2.13. Consider the scaled $\bar{G}_{d, \text { init }}(s)$


\begin{equation*}
\bar{G}_{d}(s)=W_{o}(s) \bar{G}_{d, \text { init }}(s) W_{i}(s), \tag{2.27}
\end{equation*}


where $W_{o}(s) \in \mathcal{R} \mathcal{H}_{\infty}^{n_{y} \times n_{y}}$ and $W_{i}(s) \in \mathcal{R} \mathcal{H}_{\infty}^{\left(n_{y}+n_{d}\right) \times\left(n_{y}+n_{d}\right)}$ are transfer function matrices for scaling. Next, two methods are described to obtain $\bar{G}_{d, \text { init }}(s)$. First, based on a single frequency that corresponds to the worst-case gain, and second, through interpolation over multiple frequencies using nonlinear optimization and boundary Nevanlinna-Pick interpolation.\\
2.5.3.1 Upper bound based on worst-case frequency: An initial $\bar{G}_{d, \text { init }}(s)$ is derived from a worst-case gain analysis [187] as follows. Assume that $\bar{G}_{d, \text { init }}(s, \Delta)$ is parameterized by the LFT

$$
\bar{G}_{d, \text { init }}(s, \Delta)=\mathcal{F}_{u}(D(s), \tilde{\Delta}(s))
$$

with structured mixed uncertainty $\tilde{\Delta} \in \tilde{\Delta}$ and $D(s)$ of compatible dimensions, where

$$
\tilde{\boldsymbol{\Delta}}:=\left\{\operatorname{diag}\left(\tilde{\Delta}_{p}, \tilde{\Delta}_{d}(s)\right), \tilde{\Delta}_{p} \in \tilde{\boldsymbol{\Delta}}_{p}, \tilde{\Delta}_{d} \in \tilde{\boldsymbol{\Delta}}_{d}\right\},
$$

in which the subscripts $p$ and $d$ refer to parametric and dynamic respectively, and

$$
\begin{aligned}
& \tilde{\Delta}_{p}:=\left\{\operatorname{diag}\left(\tilde{p}_{1} I_{n_{1}}, \ldots, \tilde{p}_{n_{r}} I_{n_{r}}\right), \tilde{p}_{j} \in \mathbb{R},\left|\tilde{p}_{i}\right| \leq 1, j=1, \ldots, n_{r}\right\}, \\
& \tilde{\Delta}_{d}:=\left\{\operatorname{diag}\left(\tilde{\Delta}_{1}(s), \ldots, \tilde{\Delta}_{n_{z}}(s)\right),\left\|\tilde{\Delta}_{j}(s)\right\|_{\infty} \leq 1, j=1, \ldots, n_{z}\right\} .
\end{aligned}
$$

Definition 2.22. The worst-case gain of the uncertain system is

$$
\bar{\Gamma}:=\sup _{\tilde{\Delta}(s) \in \Delta}\left\|\mathcal{F}_{u}(D(s), \tilde{\Delta}(s))\right\|_{\infty}
$$

if $\mathcal{F}_{u}(D(s), \tilde{\Delta}(s))$ ) is robustly stable and $\Gamma=\infty$ otherwise.

This worst-case gain is equivalently calculated by computing the peak gain frequency by frequency and subsequently taking the maximum over all frequencies. To this end, define\\
$\mathbf{Q}_{d}:=\left\{\operatorname{diag}\left(Q_{1}, \ldots, Q_{n_{Q}}\right), Q_{j} \in \mathbb{C}^{r_{j} \times c_{i}}, \bar{\sigma}\left(Q_{j}\right)=1, \operatorname{rank}\left(Q_{j}\right)=1, j=1, \ldots, n_{Q}\right\}$.\\[0pt]
It is sufficient to restrict the complex matrices to be rank one [188]. The mixed set is then defined as

$$
\mathbf{Q}:=\left\{\operatorname{diag}\left(\tilde{\Delta}_{p}, Q_{d}\right), \tilde{\Delta}_{p} \in \tilde{\Delta}_{p}, Q_{d} \in \mathbf{Q}_{d}\right\}
$$

The worst-case gain (2.22) is then equivalent to

$$
\bar{\Gamma}:=\max _{\omega \in \mathbb{R} \cup \infty} \Gamma(\omega),
$$

where the peak gain at a specific $\omega$ is defined as

$$
\Gamma(\omega):=\sup _{Q \in \mathbf{Q}} \bar{\sigma}\left(\mathcal{F}_{u}(D(j \omega), Q)\right) .
$$

The worst-case frequency is

$$
\omega_{w c}=\arg \max _{\omega \in \mathbb{R} \cup \infty} \Gamma(\omega) .
$$

Lower bounds $L_{k}$ and upper bounds $U_{k}$ of $\Gamma(\omega)$ are computed for each frequency $\omega_{k}$ such that $L_{k} \leq \Gamma\left(\omega_{k}\right) \leq U_{k}$. Skewed- $\mu$ power iterations at $\omega_{k}$ then yields $Q_{k} \in \mathbf{Q}$ such that

$$
\bar{\sigma}\left(\mathcal{F}_{u}\left(D\left(j \omega_{k}\right), Q_{k}\right)\right)=L_{k} .
$$

By means of the Skewed- $\mu$ power iterations, the $Q_{k, w c}$ that corresponds to $\omega_{w c}$ is calculated. The found parametric part $\tilde{\Delta}_{p, w c}$ can directly be substituted into $\tilde{\Delta}_{w c} \in \tilde{\boldsymbol{\Delta}}$. The $\tilde{\Delta}_{d, w c}$ is found by construction of an LTI uncertainty that interpolates $Q_{d, w c}$ at $\omega_{w c}$ [286, proof of Theorem 9.1]. With this result, the worst-case gain realization at $\omega_{w c}$ is then constructed as $\bar{G}_{d, \text { init }}(s, \Delta)= \mathcal{F}_{u}\left(D(s), \tilde{\Delta}_{w c}(s)\right)$, with $\tilde{\Delta}_{w c}(s)=\operatorname{diag}\left(\tilde{\Delta}_{p, w c}, \tilde{\Delta}_{d, w c}(s)\right)$.\\
Remark 2.23. The skewed- $\mu$ power iteration approach is utilized because it allows for the determination of $Q_{k}$ at each frequency $\omega_{k}$, whereas algorithms for the upper bound do not directly provide $Q_{k}$. Given that the goal is to obtain an upper bound of $\tilde{G}_{d}(s, \Delta)$, it is crucial to check for any significant discrepancies between $L_{k}$ and $U_{k}$. If such discrepancies exist, the obtained $\bar{G}_{d, \text { init }}(s, \Delta)$ can be adjusted by inflating it with weighting filters.

Example 2.24. Consider the uncertain system

$$
\begin{aligned}
\tilde{G}_{d}(s, \Delta)= & \left(7 G_{1}(s)\left(0.25+\delta_{1}\right)+3 G_{2}(s)\left(1-\delta_{1}\right)\right. \\
& \left.+3 G_{3}(s)\left(0.75-\delta_{1}\right)\right)\left(1+W(s) \Delta_{1}(s)\right)
\end{aligned}
$$

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-060}
\captionsetup{labelformat=empty}
\caption{Figure 2.8: Illustrative example to show the gain of \$\textbackslash bar\{G}\_\{d, \text{init}\}(s)\$ based on the worst-case gain $\tilde{G}_{d}\left(s, \Delta_{w c}\right)$ (一) and based on Nevanlinna-Pick interpolation $\tilde{G}_{d}\left(s, \Delta_{n p}\right)$ (一), where the interpolated frequencies are marked ( $\mathbf{x}$ ). The nominal system is shown $\tilde{G}_{d}(s, 0)$ (-) as well as the lowerbound $L(\square)$ computed with skewed- $\mu$ power iterations. The candidate $\tilde{G}_{d}\left(s, \Delta_{w c}\right)$ is largest at $\omega_{w c}$, but not necessarily large at all frequencies. The candidate $\tilde{G}_{d}\left(s, \Delta_{n p}\right)$ based on Nevanlinna-Pick interpolation clearly has a high gain across all frequencies, but does not match the lower-bound $L$ at all frequencies.\}\end{center}
\end{figure}

with $G_{1}=\frac{\frac{1}{\pi} s}{(s+2 \pi)^{2}}, G_{2}=\frac{\frac{1}{10 \pi} s}{(s+20 \pi)^{2}}$, and $G_{3}=\frac{\frac{1}{100 \pi} s}{(s+200 \pi)^{2}}$, dynamic weight $W(s)=0.2 \frac{s+10 \pi}{s+20 \pi}$, and uncertain parameters $\delta_{1} \in \mathbb{R}$ with $\left|\delta_{1}\right| \leq 1$, and $\Delta_{1}(s)$ with $\left\|\Delta_{1}\right\|_{\infty} \leq 1$. The nominal system $\tilde{G}_{d}(s, 0)$ is depicted in Figure 2.8, as well as the worst-case gain lower bound $L(\omega)$. The maximum of $L(\omega)$ is at $\omega_{w c}=1 \mathrm{~Hz}$. The related realization, obtained through skewed$\mu$ power iterations and interpolation [286, proof of Theorem 9.1], is $\delta_{1}=1$ and $\Delta_{1}(s)=\frac{s^{2}-2.388 \cdot 10^{4}+3.069 \cdot 10^{6}}{s^{2}+2.388 \cdot 10^{4}+3.069 \cdot 10^{6}}$ and is depicted in Figure 2.8. Clearly, this $\bar{G}_{d, \text { init }}(s)=\mathcal{F}_{u}\left(D(s), \operatorname{blkdiag}\left(\delta_{1}, \Delta_{1}(s)\right)\right)$ has the largest gain over all frequencies, however, is not necessarily high at other frequencies.

The approach in this subsection yields a sample of the uncertain system that maximizes the gain at $\omega_{w c}$, the frequency where the peak gain occurs. This $\bar{G}_{d, \text { init }}(s)$ may have the largest gain over all frequencies, but is not necessarily high at other frequencies, see Figure 2.8. In this case, $\bar{G}_{d, \text { init }}(s)$ can be tuned using $W_{o}$ and $W_{i}$ such that $\mu_{\Delta}\left(\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta)\right) \leq 1$. Next, a different approach is presented that allows to obtain $\tilde{\Delta}_{d}(s)$ ) by interpolation over multiple frequencies.\\
2.5.3.2 Upper bound based on multiple frequency interpolation: The second approach involves multiple frequency interpolation. To this end, consider a collection of frequencies $\left\{\omega_{k}\right\}_{k=1}^{n_{\omega}}$ with worst-case gain lower bounds $\left\{L_{k}\right\}_{k=1}^{n_{\omega}}$,\\
and the cost $\tilde{J}: \boldsymbol{\Delta} \rightarrow \mathbb{R}$,


\begin{equation*}
\tilde{J}(\tilde{\Delta}(s)):=\sum_{k=1}^{n_{\omega}} \bar{\sigma}\left(\mathcal{F}_{u}\left(D\left(j \omega_{k}\right), \tilde{\Delta}\left(j \omega_{k}\right)\right)\right) \tag{2.28}
\end{equation*}


The construction problem over multiple frequencies is to find the structured $\tilde{\Delta}_{m}(s) \in \boldsymbol{\Delta}$, where $m$ refers to multiple, which maximizes $\tilde{J}(\tilde{\Delta}(s))$.

The theoretical upper bound to $\tilde{J}(\tilde{\Delta}(s))$ is $\tilde{J}_{u}:=\sum_{k=1}^{n_{\omega}} L_{k}$. If a system only has dynamic uncertainties, it is possible to find $\tilde{\Delta}_{m}(s)$ such that $\tilde{J}_{( }^{)}\left(\tilde{\Delta}_{m}(s)\right)=\tilde{J}_{u}$. When mixed uncertainties are present, the theoretical upper bound is generally unattainable because parametric uncertainties couple frequencies. Specifically, different values of the same parametric uncertainty might be needed to achieve the lower bound $L_{k}$ at each frequency.

To find the realization $\tilde{\Delta}_{m}(s) \in \tilde{\boldsymbol{\Delta}}$, nonlinear optimization is used to find $\tilde{\Delta}_{p, m} \in \tilde{\Delta}_{p}$ and the complex matrices $\left\{Q_{d, k}\right\}_{k=1}^{n_{\omega}}$. Subsequently, the interpolant $\tilde{\Delta}_{d, m}(s) \in \tilde{\Delta}_{d}$ is computed using the boundary Nevanlinna Pick method [14, Example 21.3.1 and Corollary 21.4.2]. The reader is referred to [195, Algorithm 2] for details regarding the nonlinear optimization using the interior-point algorithm and application of the boundary Nevanlinna Pick method.

Example 2.25. Consider the uncertain system from Example 2.24, but where the worst-case gain is maximized in view of (2.28) at $\omega_{1}=\omega_{w c}=1 \mathrm{~Hz}, \omega_{2}=10$ Hz , and $\omega_{3}=100 \mathrm{~Hz}$, simultaneously, see Figure 2.8. By means of nonlinear optimization and boundary Nevanlinna-Pick interpolation it is found that the worst-case $\delta_{1}=-1$ and $\Delta_{1}=\frac{s^{6}-1.696 \cdot 10^{5} s^{5}+\ldots+9.929 \cdot 10^{15}}{s^{6}+1.696 \cdot 10^{5} s^{5}+\ldots+9.929 \cdot 10^{15}}$. This $\bar{G}_{d, \text { init }}(s)= \mathcal{F}_{u}\left(D(s), \operatorname{blkdiag}\left(\delta_{1}, \Delta_{1}(s)\right)\right)$ is depicted in Figure 2.8 and has a large gain over all frequencies, achieving $L_{k}$ at $\omega_{2}$ and $\omega_{3}$ at the cost of a small gain reduction at $\omega_{1}$.

The principle of the second method using boundary Nevanlinna-Pick interpolation and nonlinear optimization is further elaborated upon in the numerical example in Section 2.6.2.

Remark 2.26. Be aware that the construction of the perturbed models $\bar{G}_{d, \text { init }}(s)$ focuses on the largest singular value. Still, $\bar{G}_{d}(s)$ must be weighted using $W_{o}$ and $W_{i}$ such that $\mu_{\Delta}\left(\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta)\right) \leq 1$. This amounts to checking and possibly weighting the other principal directions as well.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-062}
\captionsetup{labelformat=empty}
\caption{Figure 2.9: Overview of the prototype reticle stage, where (a) indicates the moving stage (the reticle) and (b) indicates the force frame. The vibration isolation system is denoted by (c).}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-062(1)}
\captionsetup{labelformat=empty}
\caption{Figure 2.10: Close up of the prototype reticle stage, where (a) is a horizontal position sensor, (b) is the metrology frame, and (c) is a capacitive vertical position sensor.}
\end{center}
\end{figure}

Remark 2.27. A conservative upper-bound realization $\bar{G}_{d}(s)$ can always be constructed based solely on the first singular value $\bar{\sigma}\left(\tilde{G}_{d}(s, \Delta)\right)$. Here, a weight $W(s)$ is created such that $\bar{\sigma}\left(\tilde{G}_{d}(s, \Delta)\right) \leq W(s)$. Subsequently $\bar{G}_{d}(s)$ is constructed as $\bar{G}_{d}(s)=W(s) I$ which results in a diagonal post-filter $R(s)$ with equal singular values. This concept can be related to Figure 2.6, where the upper bound in (一) is a circle instead of an ellipsoid.

\subsection*{2.6 Numerical example}
In this section, the proposed approach is validated using an uncertain model of a next-generation motion system. This system is a prototype reticle stage, see Figure 2.9 and 2.10 . The reticle stage is actuated by 8 force actuators in the vertical\\
direction and the position is measured with 8 capacitive sensors with nanometer accuracy, of which a two-input two-output equivalent plant is considered. First, the system setup and considered models are introduced. Subsequently, a robust fault detection filter is designed. Here, the upper bound $\bar{G}_{d}(s)$ is examined in detail as well as its influence of the closed-loop transfer functions to the residuals. Finally, the time-domain response is computed and discussed.

\subsection*{2.6.1 Uncertain system model and closed-loop configuration}
First, a non-parametric system identification experiment is performed, see Figure 2.11. Subsequently, a parametric modal nominal model of order 6 is fitted based on the non-parametric frequency response function. To represent the mismatch between the frequency response data and nominal model $G$, structured uncertainty is considered. To this end, the nominal model $G$ is expanded with an uncertain channel which together form the generalized plant $P$, which is partitioned as

\[
\left[\begin{array}{c}
z_{\Delta}  \tag{2.29}\\
\hline \tilde{y}_{1} \\
\tilde{y}_{2}
\end{array}\right]=\underbrace{\left[\begin{array}{c|cc}
G_{z_{\Delta} w_{\Delta}} & G_{z_{\Delta} u_{1}} & G_{z_{\Delta} u_{2}} \\
\hline G_{y_{1} w_{\Delta}} & G_{11} & G_{12} \\
G_{y_{2} w_{\Delta}} & G_{21} & G_{22}
\end{array}\right]}_{P}\left[\begin{array}{c}
w_{\Delta} \\
\hline u_{1} \\
u_{2}
\end{array}\right] .
\]

Here, the uncertainty channel is described by $w_{\Delta}=\Delta z_{\Delta}$ with dynamic normbounded uncertainty $\Delta \in \boldsymbol{\Delta}$. The uncertain plant is then constructed as $G_{u}= \mathcal{F}_{u}(P, \Delta)$ of which multiple realizations are depicted in Figure 2.11.

The system operates in closed loop, see Figure 2.1, and is robustly stabilized by controller with a target bandwidth of 140 Hz that is synthesized with a onestep robust control approach [247].

Alongside the uncertain plant model, the disturbance model $G_{d}$ and the fault model $G_{f}$ are considered certain. The disturbance is modeled as a high-pass filter resembling the effect of high frequency independent measurement noise on both sensors with a frequency above 500 Hz . The bode magnitude plot is depicted in Figure 2.12. The frequency spectrum of the fault model is considered to be flat. The fault model is shown in Figure 2.12.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-064(1)}
\captionsetup{labelformat=empty}
\caption{Figure 2.11: Bode magnitude plot of the FFR. The measured frequency response function is depicted in (-) together with the nominal 6th order plant model $G$ in (-) and the uncertain state space model \$G\_\{u}\$ in (-) visualized for 50 different uncertainties $\Delta \in \boldsymbol{\Delta}$.\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-064}
\captionsetup{labelformat=empty}
\caption{Figure 2.12: Disturbance model \$G\_\{d}\$ (—) and fault model $G_{f}$ (—).\}\end{center}
\end{figure}

\subsection*{2.6.2 Robust fault detection filter synthesis}
Next, the introduced models are used to synthesize the robust fault detection filter. Using the solution proposed in Section 2.4, a filter is designed that rejects model uncertainties and disturbances, while amplifying faults. First, a stable LCF, $\tilde{M}_{u}, \tilde{N}_{u}$, of the nominal system $G_{u}(s, 0)$ is computed, see Lemma 2.5. Given these factors, the uncertain plant $G_{u}(s, \Delta)$, disturbance and fault models $G_{d}(s)$ and $G_{f}(s)$, respectively, and the robustly stabilizing feedback controller $C(s)$, the closed-loop transfers from exogenous inputs to the pre-residual, $\left[\begin{array}{lll}r & d & f\end{array}\right] \rightarrow \tilde{\epsilon}$, are computed. This allows to determine $\tilde{G}_{d}^{2 \times 4}(s, \Delta)$, see (2.9).

Now, the upper-bound envelope $\bar{G}_{d}(s)$ is designed. First, a collection of frequencies $\left\{\omega_{k}\right\}_{k=1}^{n_{\omega}}$ is selected. In this case, $\omega_{1}=1 \mathrm{~Hz}, \omega_{2}=\omega_{w c}=219 \mathrm{~Hz}$, and $\omega_{3}=1000 \mathrm{~Hz}$ are chosen. Subsequently, the cost $\tilde{J}$, see (2.28), is maximized which yields $\Delta\left(j \omega_{1}\right), \Delta\left(j \omega_{2}\right), \Delta\left(j \omega_{3}\right)$. Based on these, an interpolant, $\Delta_{n p}(s)$, is obtained by application of the BNP approach. Hence, the perturbed model of the uncertain system that maximizes $\tilde{J}$, is given by $\bar{G}_{d, \text { init }}(s)=\mathcal{F}_{u}\left(D(s), \Delta_{n p}(s)\right)$. The singular values of $\tilde{G}_{d}(s, \Delta)$ are depicted in Figure 2.13 as well as the worstcase realization $\mathcal{F}_{u}\left(D(s), \Delta_{w c}(s)\right)$ and the multiple frequency interpolated realization $\bar{G}_{d, \text { init }}(s)$.

Subsequently, $\bar{G}_{d}(s)$ is updated via the filters $W_{o}$ and $W_{i}$ in an iterative fashion such that Lemma 2.13 is satisfied. The filters $W_{o}$ and $W_{i}$ are depicted in Figure 2.14, and the singular values of the resulting $\bar{G}_{d}(s)$ are shown in Figure 2.13. The co-outer factor of $\bar{G}_{d}(s)$ is computed with Lemma 2.6 and the resulting optimal post-filter $R(s)$ is computed by means of Theorem 2.15 where $\gamma=1$. In this iterative procedure, it is verified whether the structured singular value satisfies $\mu_{\Delta}\left(\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta)\right) \leq 1$, as shown in Figure 2.15. In addition, the singular values $\sigma_{i}\left(T_{\varepsilon \tilde{d}}\right) \leq 1$ are depicted, as well as the singular values of the resulting transfer from faults to residual $\sigma_{i}\left(T_{\varepsilon f}\right)$ in Figure 2.16. The structured singular value check and $\sigma_{i}\left(T_{\varepsilon \tilde{d}}\right)$ show where to locally inflate the upper-bound envelope through $W_{o}$ and $W_{i}$ if the condition is not met, or at which frequencies to deflate the envelope to mitigate any conservatism. Finally, the obtained postfilter $R(s)$ is shown in Figure 2.17.

The singular values in Figure 2.13 show that interpolant obtained using the BNP approach gives a good first model which is slightly inflated in order to satisfy Lemma 2.13. Next to $\bar{\sigma}\left(T_{\varepsilon f}\right)$, Figure 2.16 shows the optimal fault sensitivity in case $\Delta=0$. It is concluded that the fault sensitivity degradation due to model uncertainty is limited. The optimal post filter in case $\Delta=0$ is depicted in 2.17, which equals the inverse of the disturbance model. The effect of considering the modeling uncertainty is clearly visible by comparison to the singular values of $\bar{G}_{d}(s, \Delta)$ in Figure 2.13. As a result, the singular values of transfer $T_{\epsilon \tilde{d}}:[r d] \rightarrow \epsilon$ are mapped below the specified bound $\gamma=1$ satisfying the constraint opted in Theorem 2.15 in Section 2.4, and in addition, amplification of transfer $T_{\epsilon f}: f \rightarrow \epsilon$ is achieved.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-066(1)}
\captionsetup{labelformat=empty}
\caption{Figure 2.13: Singular value plots of uncertain transfer function matrix \$\textbackslash tilde\{G}\_\{d\}(s, \textbackslash Delta)\$ in (-) together with the singular values of worst-case gain upper bound $\mathcal{F}_{u}\left(D(s), \Delta_{w c}(s)\right)$ in (-), as well as the multiple frequency interpolant via the BNP approach $\bar{G}_{d, \text { init }}(s)=\mathcal{F}_{u}\left(D(s), \Delta_{n p}(s)\right)$ (一). The frequencies over which is interpolated are indicated ( $\mathbf{x}$ ). The singular values of the weighted upper bound are indicated in (---).\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-066}
\captionsetup{labelformat=empty}
\caption{Figure 2.14: Bode magnitude plots of the iteratively tuned filters \$W\_\{o}(s)\$ and $W_{i}(s)$. The first and second entry of the block diagonal $W_{o}$ are depicted as (一) and (---). The first, second, third, and fourth entry of these $W_{i}$ are depicted as (—), (---), (-), and (---), respectively.\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-067}
\captionsetup{labelformat=empty}
\caption{Figure 2.15: Structured singular value \$\textbackslash mu\_\{\textbackslash Delta}\textbackslash left(\textbackslash bar\{G\}\textit{\{d o\}\^{}\{-1\}(j \textbackslash omega) \textbackslash tilde\{G\}}\{d\}(j \textbackslash omega, \textbackslash Delta)\textbackslash right)\$. Since the lower bound (-) and the upper bound (-) of the structured singular value are below $\gamma=1$ (---) the user-defined maximum disturbance sensitivity is satisfied.\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-067(1)}
\captionsetup{labelformat=empty}
\caption{Figure 2.16: Singular values of transfer \$T\_\{\textbackslash epsilon \textbackslash tilde\{d}\}:\textbackslash left[\[
\begin{array}{ll}r & d\end{array}
\]\textbackslash right] \textbackslash rightarrow \textbackslash epsilon\$ in (一) together with bound $\gamma=1$ in (---) (left) and the singular values of transfer $T_{\epsilon f}: f \rightarrow \epsilon$ (right) for 50 different uncertainties $\Delta \in \boldsymbol{\Delta}$. On top of that, the singular values of the maximum achievable transfer $T_{\epsilon f}$ is plotted in (-) when the optimal post-filter is used in the case of no model uncertainty. Hence, performance degradation due to model uncertainty is limited.\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-068(1)}
\captionsetup{labelformat=empty}
\caption{Figure 2.17: Bode magnitude plot of the post-filter $R$ in (-) together with the optimal post-filter if no model uncertainty is present in (-). Observe the shape of the post-filter with respect to the singular values of \$\textbackslash bar\{G}\_\{d\}\$ in Figure 2.13.\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-068}
\captionsetup{labelformat=empty}
\caption{Figure 2.18: Time response of both residual signals \$\textbackslash epsilon\_\{1}\$ and $\epsilon_{2}$ in (-) for 50 different uncertainties $\Delta \in \boldsymbol{\Delta}$ together with the reference input $r$ in (-) and fault input $f$ in ( $\cdots$ ). A detection threshold $|\beta|=1$ is plotted in (一). Indeed, the residuals exceed the user-defined $\beta$-bounds when a fault is present in the system.\}\end{center}
\end{figure}

\subsection*{2.6.3 Time-domain response}
A time domain simulation is performed to demonstrate the fault detection process using the fault detection filter $R(s)$ from Figure 2.17. For this purpose, the disturbance is modeled as two independent normally distributed white noise signals with standard deviation $\sigma=0.1$. The setpoint $r$ is a block-signal with\\
amplitude equal to 1 and a frequency of $f_{r}=50 \mathrm{~Hz}$. The setpoint is activated and applied to the motion DOF at $T=0.05 \mathrm{~s}$. A fault is added to the system at $T=0.1 \mathrm{~s}$ with an amplitude of 1 and frequency $f_{f}=12.5 \mathrm{~Hz}$. The resulting residual signals $\epsilon_{1}$ and $\epsilon_{2}$ for different values of uncertainty $\Delta \in \boldsymbol{\Delta}$ are shown in Figure 2.18 together with the exogenous inputs $r$ and $f$.

The effect of the disturbance and setpoint are clearly suppressed and the fault is easily distinguished in the the residual signals. Deriving time-domain bounds for residual evaluation is beyond the scope of this chapter, however, based on the signal between $t=0.05 \mathrm{~s}$ and $t=0.1 \mathrm{~s}$, a conservative threshold for fault detection is selected as $|\beta|=1$. In case a fault is present, the residuals exceed this threshold, allowing for successful fault detection despite exogenous disturbances and modeling uncertainty.

\subsection*{2.7 Conclusion}
In this chapter, an optimal solution is derived which solves the robust fault detection filter design problem for continuous-time LTI uncertain systems operating in an open or closed-loop setting. The solution is obtained by solving a single Riccati equation, which maximizes performance across all frequencies, and is completely determined by an upper-bound envelope which bounds the uncertainty and disturbance models. By achieving an optimal compromise between sensitivity to faults and the rejection of disturbances and modeling uncertainty inherent in any practical systems, the proposed method effectively addresses a key challenge in fault-diagnosis system design.

\section*{Appendices}
\section*{2.A Alternative representation residual dynamics for uncertain closed-loop systems}
Consider the residual dynamics in (2.9). This appendix shows how to rewrite the residual dynamics to a different insightful form by factoring out a common uncertainty. Factoring out $G_{d}$ and $G_{f}$ gives

$$
\begin{aligned}
\epsilon= & R \tilde{M}_{u}\left\{\tilde{G}_{u}(s, \Delta) K S_{\Delta} r+\left(I-\tilde{G}_{u}(s, \Delta) K S_{\Delta}\right) G_{d}(s, \Delta) d\right. \\
& \left.+\left(I-\tilde{G}_{u}(s, \Delta) K S_{\Delta}\right) G_{f}(s, \Delta) f\right\}
\end{aligned}
$$

Taking out $\left(I-\tilde{G}_{u}(s, \Delta) K S_{\Delta}\right)$ gives

$$
\begin{aligned}
\epsilon= & R \tilde{M}_{u}\left(I-\tilde{G}_{u}(s, \Delta) K S_{\Delta}\right) \\
& \times\left\{\left(I-\tilde{G}_{u}(s, \Delta) K S_{\Delta}\right)^{-1} \tilde{G}_{u}(s, \Delta) K S_{\Delta} r+G_{d}(s, \Delta) d+G_{f}(s, \Delta) f\right\}
\end{aligned}
$$

Consider the following related to the factor $I-\tilde{G}_{u}(s, \Delta) K S_{\Delta}$. Using that $S_{\Delta}+ T_{\Delta}=I$ in (2.30), with $T_{\Delta}=\left(I+G_{u}(s, \Delta) K\right)^{-1} G_{u}(s, \Delta) K=G_{u}(s, \Delta) K(I+ \left.G_{u}(s, \Delta) K\right)^{-1}$, this factor can be simplified as follows.

$$
\begin{aligned}
I-\tilde{G}_{u}(s, \Delta) K S_{\Delta} & =I-\left(G_{u}(s, \Delta)-G_{u}(s, 0)\right) K S_{\Delta} \\
& =I-T_{\Delta}+G_{u}(s, 0) K S_{\Delta} \\
& =S_{\Delta}+G_{u}(s, 0) K S_{\Delta} \\
& =\left(I+G_{u}(s, 0) K\right) S_{\Delta} \\
& =S^{-1} S_{\Delta}
\end{aligned}
$$

where $S=(I+G(s, 0) K)^{-1}$ is the nominal sensitivity function. Next, consider the following related to the factor $\left(I-\tilde{G}_{u}(s, \Delta) K S_{\Delta}\right)^{-1} \tilde{G}_{u}(s, \Delta) K S_{\Delta}$ in (2.30).

$$
\begin{aligned}
& \left(I-\tilde{G}_{u}(s, \Delta) K S_{\Delta}\right)^{-1} \tilde{G}_{u}(s, \Delta) K S_{\Delta} \\
& =\left(\tilde{G}_{u}(s, \Delta) K S_{\Delta}\left(\left(\tilde{G}_{u}(s, \Delta) K S_{\Delta}\right)^{-1}-I\right)\right)^{-1} \tilde{G}_{u}(s, \Delta) K S_{\Delta} \\
& =\left(\left(\tilde{G}_{u}(s, \Delta) K S_{\Delta}\right)^{-1}-I\right)^{-1}\left(\tilde{G}_{u}(s, \Delta) K S_{\Delta}\right)^{-1} \tilde{G}_{u}(s, \Delta) K S_{\Delta} \\
& =\left(S_{\Delta}^{-1}\left(\tilde{G}_{u}(s, \Delta) K\right)^{-1}-I\right)^{-1} \\
& =\left(\left(I+G_{u}(s, \Delta) K\right)\left(\tilde{G}_{u}(s, \Delta) K\right)^{-1}-I\right)^{-1} \\
& =\left(\left(I+G_{u}(s, \Delta) K-\tilde{G}_{u}(s, \Delta) K\right)\left(\tilde{G}_{u}(s, \Delta) K\right)^{-1}\right)^{-1} \\
& =\left(\left(I+G_{u}(s, 0) K\right)\left(\tilde{G}_{u}(s, \Delta) K\right)^{-1}\right)^{-1} \\
& =\tilde{G}_{u}(s, \Delta) K S
\end{aligned}
$$

Hence, the residual dynamics (2.30), and thus (2.9), can be written as

$$
\epsilon=R \tilde{M}_{u} S^{-1} S_{\Delta}\left(\tilde{G}_{u}(s, \Delta) C S r+G_{d}(s, \Delta) d+G_{f}(s, \Delta) f\right)
$$

which reveals that a large part of the uncertainty can be factored out and applies to all exogenous inputs $r, d$, and $f$ equally.

\section*{Chapter}
\section*{Robust Fault Detection with Application to a Prototype Wafer Stage ${ }^{1}$ }
\remarknote[JC][2025-10-13][high]{测试}
\begin{abstract}
Ensuring reliability and safety in high-precision production systems requires effective fault detection, yet distinguishing faults from disturbances and modeling uncertainties poses a significant challenge. This study presents the design of a robust fault detection filter for a wafer stage in closed-loop operation, used for lithography in the semiconductor industry. The proposed framework starts with the identification of the closed-loop system using multisine excitation, allowing the development of a quality certificate for the model in the form of uncertainty bounds. The framework employs an uncertainty-independent upper bound on disturbances and modeling errors to synthesize an optimal fault detection filter. The application to a next-generation prototype wafer stage demonstrates the effectiveness of this method, achieving a balance between fault sensitivity and the rejection of disturbances and modeling uncertainties. This marks the first implementation of such a robust fault detection technique in high-precision production equipment, significantly enhancing fault detection reliability and contributing to improved operational safety and performance.
\end{abstract}

\footnotetext{${ }^{1}$ The results in this chapter constitute Contribution II in Section 1.6. The chapter is based on: [45] K. Classens, T. Ickenroth, K. Tiels, P. Tacx, J. van de Wijdeven, W. P. M. H. Heemels, and T. Oomen, "Robust Fault Detection with Application to a Prototype Wafer Stage," submitted for journal publication.
}\subsection*{3.1 Introduction}
The economic value of high-tech equipment, such as wafer stages for lithography, is highly dependent on uptime and throughput. Over the past decades, significant progress has been made toward achieving faster and more accurate machines [184]. However, advanced maintenance techniques have only recently gained traction, despite the necessity of maintenance to prevent inevitable failures that lead to significant downtime [40]. Predictive maintenance (PdM), proven effective in other fields, identifies faults early through fault detection and isolation (FDI), offering a superior approach compared to traditional preventive or reactive strategies [287].

Fault detection and isolation has proven to be a successful strategy in various fields including aerospace [170, 288] and automotive [102, 284]. Mechatronic devices in these safety-critical fields increasingly integrate fault detection, driven by regulatory requirements, such as emissions control in vehicles, and the shift from mechanical to electro-mechanical components. FDI is also found in the chemical and process industry [19, 55, 194]. Components in these fields such as valves and hydraulic pistons are frequently equipped with basic diagnostics capabilities like limit and plausibility checks, which means that often only binary decisions are provided, detecting complete breakdowns rather than nuanced faults. While fault diagnosis concepts are less frequent, they are vital for early hazard detection.

Due to the high financial stakes involved in high-precision mechatronic production equipment, exceptional reliability is required. To this end, advanced FDI techniques are necessary that address the inherent variability and uncertainty of these systems, ensuring early detection of faults and performance degradation. A model-based approach is key here because it allows for a deep understanding of the underlying physical dynamics, which is crucial for precise and reliable maintenance. In contrast to black-box solutions, which are generally unacceptable in industrial environments due to their lack of transparency and predictability, model-based methods offer the clarity and insight needed to detect subtle anomalies, pinpoint their origin, and intervene before they lead to major failures. This proactive approach promises to enhance the equipment its operational efficiency but also extends its lifespan, providing substantial economic benefits.

Driven by the increasing demand for safer and more reliable systems, modelbased methods have made significant progress [35, 63, 94, 137, 262]. Among these, observer-based methods, as considered in this chapter, have gained substantial attention for their effectiveness in detecting various types of faults [35, 79]. Building on these methods, comprehensive strategies for fault detection and isolation (FDI) have been developed [35, 63, 262]. A key challenge in fault detection (FD) is distinguishing faults from unknown disturbances. To address this, several optimal fault diagnosis methods have been developed for linear time-invariant (LTI) systems, including factorization-based techniques [62, 142,

157], and $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ techniques, which are typically solved using Linear Matrix Inequality (LMI) synthesis [134, 156, 248, 270].

Robust methods have been developed to explicitly address modeling uncertainty. Among these methods, many utilize the $\mathcal{H}_{\infty}$ criterion, which is optimized through $\mu$-synthesis [216, 242]. This approach provides a structured way to handle uncertainties. Additionally, $\mathcal{H}_{\infty}$ model-matching techniques have been employed, utilizing LMI optimization to obtain effective fault detection filters [285]. The combined $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ criterion further refines these methods, providing comprehensive solutions through iterative LMI approaches [125, 269].

Contrary to methods designed for LTI systems without uncertainty [62, 157, 248], robust methods in the existing literature tend to be either overly conservative, lack guarenteed optimality, or are difficult to apply. For instance, techniques that rely solely on the $\mathcal{H}_{\infty}$ criterion do not directly account for $\mathcal{H}_{-}$fault sensitivity, which must be analyzed retrospectively. Model-matching techniques often fail to guarantee optimality in terms of $\mathcal{H}_{-}$fault sensitivity, as their effectiveness heavily depends on the reference model. Additionally, literature lacks proof of the practical applicabilty of these robust methods on industrial machines and it is unclear how the uncertain model is obtained, particularly in a closed-loop complex system with considerable number of inputs and outputs.

Although several important steps have been taken towards fault diagnosis for complex systems, at present a user-friendly procedure for application of robust fault diagnosis in an industrial setting is lacking. This chapter applies an optimal $\mathcal{H}_{i} / \mathcal{H}_{\infty}$ solution to the fault detection filter design problem for LTI multi-input multi-output (MIMO) uncertain closed-loop systems on a prototype industrial wafer stage. The solution uses a single Riccati equation based on an upper bound of the uncertainty and disturbance model and achieves an optimal compromise between the rejection of disturbances and modeling uncertainty with respect to fault sensitivity. The entire process is detailed from obtaining a model with uncertainty bounds to synthesis and application of the fault detection filter. The scope is confined to the fault detection task, however, yet can be readily expanded for isolation purposes. The approach presented in this chapter is directly applicable to a wide range of applications, particularly mechatronic systems with multiple actuators and sensors.

To summarize, the main contributions are be outlined as follows.\\
C1: This chapter details the entire process from system identification to the synthesis and design of robust fault detection filters.

C2: The proposed method is validated experimentally on a next-generation prototype wafer stage.

The chapter is organized as follows. Initially, the chapter presents the necessary notation and preliminaries in Section 3.2. This is followed by the introduction of the prototype wafer stage and the formulation of the primary problem\\
in Section 3.3. In Section 3.4, the proposed solution is provided. Subsequently, Section 3.5 discusses the application of this approach to the wafer stage, with a specific focus on developing the system model and defining its uncertainty bounds. Subsequently the time-domain experimental results are shown. The chapter concludes by summarizing the key findings and implications of the research.

\subsection*{3.2 Notation and preliminaries}
The sets of real numbers and nonnegative real numbers are indicated by $\mathbb{R}$ and $\mathbb{R}_{\geq 0}$. By $\|\cdot\|_{2}$ the Euclidean norm is defined. The maximum and minimum singular values of the matrix $A$ are denoted by $\bar{\sigma}(A)$ and $\underline{\sigma}(A)$, respectively. The real-rational subspace of $\mathcal{H}_{\infty}$ is denoted by $\mathcal{R} \mathcal{H}_{\infty}$. The signal $y \in \mathcal{L}_{2}$ if $\|y\|_{2}^{2}=\int_{0}^{\infty} y^{\top}(t) y(t) \mathrm{d} t<\infty$. The signal $y \in \mathcal{L}_{2 e}$ if $\|y\|_{2 T}^{2}=\int_{0}^{T} y^{\top}(t) y(t) \mathrm{d} t< \infty, T \in \mathbb{R}_{\geq 0}$. A transfer function $N$ is called inner if $N \in \mathcal{R} \mathcal{H}_{\infty}$ and $N^{H} N=I$ and co-inner if $N \in \mathcal{R} \mathcal{H}_{\infty}$ and $N N^{H}=I$. A transfer function $M$ is called outer if $M \in \mathcal{R} \mathcal{H}_{\infty}$ and has full row normal rank and has no open right half plane zeros.

Definition 3.1. (Linear fractional transformation) For matrices $N$ and $M= \left[\begin{array}{ll}M_{11} & M_{12} \\ M_{21} & M_{22}\end{array}\right]$ of appropriate partitioning, the lower linear fractional transformation (LFT) is defined as $\mathcal{F}_{l}(M, N)=M_{11}+M_{12} N\left(I-M_{22} N\right)^{-1} M_{21}$ and the upper LFT as $\mathcal{F}_{u}(M, N)=M_{22}+M_{21} N\left(I-M_{11} N\right)^{-1} M_{12}$, under the assumption that the involved matrix inverses exists.

Consider uncertainties whose values on the imaginary axis admit the structure $\boldsymbol{\Delta}_{s}=\left\{\operatorname{diag}\left(p_{1} I, \ldots, p_{n_{r}} I, \delta_{1} I, \ldots, \delta_{n_{c}} I, \Delta_{1}, \ldots, \Delta_{n_{z}}\right) \in \mathbb{C}^{p \times q}\right.$ whose blocks satisfy $p_{j} \in \mathbb{R}$ with $\left|p_{j}\right| \leq 1$ for $j=1, \ldots, n_{r}, \delta_{j} \in \mathbb{C}$ with $\left|\delta_{j}\right| \leq 1$ for $j=1, \ldots, n_{c}$, and $\Delta_{j} \in \mathbb{C}^{p_{j} \times q_{j}}$ with $\left\|\Delta_{j}\right\|_{2} \leq 1$ for $j=1, \ldots, n_{z}$. The actual set of uncertainties $\boldsymbol{\Delta}:=\left\{\Delta(s) \in \mathcal{R} \mathcal{H}_{\infty} \mid \Delta(i \omega) \in \boldsymbol{\Delta}_{s}\right.$ for all $\left.\omega \in \mathbb{R} \cup\{\infty\}\right\}$.

The structured singular value of a matrix $P$ with respect to the set $\boldsymbol{\Delta}$ is defined as


\begin{equation*}
\mu_{\boldsymbol{\Delta}}(P)=\frac{1}{\sup \{r \mid \operatorname{det}(I-P \Delta) \neq 0 \text { for all } \Delta \in r \boldsymbol{\Delta}\}} \tag{3.1}
\end{equation*}


The following definitions are used in this chapter [63, 157, 233, 286].\\
Definition 3.2. (Minimum gain) The smallest gain of the continuous-time LTI system $G: \mathcal{L}_{2 e} \rightarrow \mathcal{L}_{2 e}$, that is the $\mathcal{H}_{-}$index, is defined as


\begin{equation*}
\|G\|_{-}=\inf _{\omega \in \mathbb{R}_{\geq 0}} \underline{\sigma}(G(j \omega)) \tag{3.2}
\end{equation*}


The minimum gain is not a norm and therefore named the $\mathcal{H}_{-}$index.

Definition 3.3. (Maximum gain) The $\mathcal{H}_{\infty}$ norm of the continuous-time LTI system $G: \mathcal{L}_{2 e} \rightarrow \mathcal{L}_{2 e}$, denoted as $\|G\|_{\infty}$, is given by


\begin{equation*}
\|G\|_{\infty}=\sup _{\omega \in \mathbb{R}_{\geq 0}} \bar{\sigma}(G(j \omega)) \tag{3.3}
\end{equation*}


Lemma 3.4. (Robust Stability) Consider the system $N=\left[\begin{array}{l|l}N_{11} & N_{12} \\ \hline N_{21} & N_{22}\end{array}\right]$ in the $N \Delta$-structure, i.e., such that $\left[\begin{array}{c}u_{\Delta} \\ z\end{array}\right]=N\left[\begin{array}{c}y_{\Delta} \\ w\end{array}\right]$ and $y_{\Delta}=\Delta u_{\Delta}$ with $\Delta \in \Delta$. Assume that the nominal system $N_{22}$ and the perturbations $\Delta$ are stable. Then, the uncertain system is stable for all plants in the model set, i.e., $\mathcal{F}_{u}(N, \Delta)$ is stable $\forall \Delta \in \boldsymbol{\Delta}$, if and only if $\Leftrightarrow \mu_{\Delta}\left(N_{11}\right)<1, \forall \omega$.

Proof. The proof is provided in [233, Chapter 8].\\
Lemma 3.5. (Left Coprime Factorization) The transfer matrices $M, N \in \mathcal{R} \mathcal{H}_{\infty}$ are called left coprime over $\mathcal{R} \mathcal{H}_{\infty}$ if there exist two transfer matrices $X, Y \in \mathcal{R} \mathcal{H}_{\infty}$ such that the Bezout Identity holds, i.e.,

$$
M X+N Y=I
$$

Let $G(s)$ be a proper real rational transfer matrix. A left coprime factorization (LCF) of system $G$ is a factorization $G=M^{-1} N$, where $M$ and $N$ are leftcoprime over $\mathcal{R} \mathcal{H}_{\infty}$. Let $G=\left[\begin{array}{c|c}A & B \\ \hline C & D\end{array}\right]$ be a detectable state-space realization of $G$ and $L$ be a matrix with appropriate dimensions such that $A+L C$ is stable, then a left coprime factorization of $G$ is given by state space representation

$$
\left[\begin{array}{cc}
M & N
\end{array}\right]:=\left[\begin{array}{c|cc}
A+L C & L & B+L D \\
\hline C & I & D
\end{array}\right] \in \mathcal{R} \mathcal{H}_{\infty} .
$$

Proof. The proof is provided in [286, Chapter 13].\\
Lemma 3.6. (Co-inner-outer factorization) Let $G \in \mathcal{R} \mathcal{H}_{\infty}^{p \times m}$ and assume $p \leq m$. Then, there exists a $L C F G=\tilde{M}^{-1} \tilde{N}$ such that $\tilde{N}$ is a co-inner and $\tilde{M}$ is coouter if and only if $G G^{H}>0$ on the $j \omega$-axis, including at $\infty$. This factorization is unique up to a constant unitary multiple. Assume that $G=\left[\begin{array}{c|c}A-j \omega I & B \\ \hline C & D\end{array}\right]$ is detectable and that $G=\left[\begin{array}{c|c}A & B \\ \hline C & D\end{array}\right]$ has full row rank for all $\omega \in \mathbb{R}_{\geq 0}$. Then, a particular realization of the desired co-inner-outer factorization (CIOF) is

\[
\left[\begin{array}{cc}
\tilde{M} & \tilde{N}
\end{array}\right]:=\left[\begin{array}{c|cc}
A+L C & L & B+L D  \tag{3.4}\\
\hline R^{-1 / 2} C & R^{-1 / 2} & R^{-1 / 2} D
\end{array}\right] \in \mathcal{R} \mathcal{H}_{\infty},
\]

where $R=D D^{\top}>0$, and $L=-\left(B D^{\top}+Y C^{\top}\right)^{\top} R^{-1}$ and $Y \geq 0$ be the stabilizing solution to the Riccati equation

$$
\begin{gathered}
\left(A-B D^{\top} R^{-1} C\right) Y+Y\left(A-B D^{\top} R^{-1} C\right)^{\top}- \\
Y C^{\top} R^{-1} C Y+B\left(I-D^{\top} R^{-1} D\right) B^{\top}=0,
\end{gathered}
$$

such that $A-B D^{\top} R^{-1} C-Y C^{T} R^{-1} C$ is stable.\\[0pt]
Proof. The proof is provided in [286, Chapter 13]. \(\square\)

\subsection*{3.3 System description and problem formulation}
This section starts with a description of the prototype wafer stage system. Following this, the optimal fault detection problem is formulated for uncertain closed-loop systems.

\subsection*{3.3.1 Prototype wafer stage setup}
This chapter examines the prototype wafer stage depicted in Figure 3.1 and 3.2. This wafer stage exemplifies a future next-generation high-precision positioning system. Designed as a lightweight mechanical structure, the stage achieves high accelerations and performs fast motion tasks [184]. Consequently, it exhibits complex dynamic behavior due to mechanical compliance. The system is equipped with a large number of spatially distributed actuators and sensors, making it over-actuated and over-sensed, meaning the number of actuators and sensors exceeds six. This configuration allows for compensation of structural deformations in addition to the conventional six rigid body motion degrees of freedom (translations and rotations) [254]. Specifically, the stage is actuated by thirteen Lorentz actuators in the out-of-plane direction and four Lorentz actuators in the horizontal plane. The stage is measured by four encoders in the out-of-plane direction and three capacitive sensors in the horizontal plane.

The stage uses passive gravity compensators to reduce the required actuator power. These compensators magnetically levitate the stage, counterbalancing the weight of the chuck. For nanometer positioning, the stage is closed-loop controlled at a sampling frequency of 10 kHz . The control system is decentralized, with each of the six degrees of freedom controlled by a dedicated PID controller achieving an 80 Hz bandwidth. In this setup, five rigid body positions are maintained at zero, except for the vertical out-of-plane direction, which follows a smooth fourth-order setpoint.

Identifying a parametric model of such a large, complex MIMO system with a quality certificate necessitates advanced identification algorithms, detailed in

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-078}
\captionsetup{labelformat=empty}
\caption{Figure 3.1: Prototype experimental wafer stage setup. The moving part, the chuck, is indicated by (a) and is suspended by gravitaty compensators on the force frame (b) which is on top of the base frame (c). Currently, the chuck and force frame are slid out of the machine, whereas during operation, it is underneath the metroframe (d), see Figure 3.2. The metroframe is isolated from the fixed world through air mounts (e). There are four Lorentz actuators in the horizontal plane of which one is indicated by (f). These actuators apply a tangential force to the chuck. The actuators in the vertical plane are positioned between the chuck and the force frame. The position of the chuck is measured in the horizontal plane by means of capacitive sensors and measured in the vertical plane by means of linear encoders. The chuck has four scales (g) measured by the encoders (h) on the metroframe.}
\end{center}
\end{figure}

Section 3.5. The obtained uncertain model is used to synthesize a fault detection filter for two types of faults: an actuator fault at a corner of the chuck and an anomalous force exerted on one side of the chuck. The latter fault resembles an anomaly from a different subsystem interacting with the chuck.

\subsection*{3.3.2 $\mathcal{H}_{i} / \mathcal{H}_{\infty}$-problem formulation}
Consider the input-output representation of the uncertain LTI processes described by


\begin{equation*}
y=G_{u}(s, \Delta) u+G_{d}(s) d+G_{f}(s) f \tag{3.5}
\end{equation*}


where $G_{u}(s, \Delta)$ is an uncertain transfer function matrix (TFM) from the Laplace-transformed time-dependent control input $u$ to the output $y$, whereas $G_{d}(s)$ and $G_{f}(s)$ are transfer function matrices of the Laplace-transformed timedependent disturbance $d$, and fault $f$, to the output $y$. The time-dependent vectors are $y(t) \in \mathbb{R}^{n_{y}}, u(t) \in \mathbb{R}^{n_{u}}, d(t) \in \mathbb{R}^{n_{d}}$, and $f(t) \in \mathbb{R}^{n_{f}}$, respectively. The modeling uncertainty is denoted by $\Delta \in \boldsymbol{\Delta}$ and can be parametric or dynamic with suitable dimensions. Throughout, the Laplace operator $s$ is omitted when

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-079}
\captionsetup{labelformat=empty}
\caption{Figure 3.2: Wafer stage positioned underneath the metroframe. The vertical position is measured using encoders (a) attached to the metroframe. The wafer stage is on top of the force frame (b) which houses the multiple actuators in the vertical plane and four actuators in the horizontal plane (c). A scanning sensor slide (d) is hovering inbetween the chuck and the mirror (e) which is attached to the metroframe. The scanning sensory slide can be used to measure the out-of-plane vertical deflection across the chuck.}
\end{center}
\end{figure}

clear from context. The system (3.5) is controlled to follow a reference $r(t) \in \mathbb{R}^{n_{y}}$ by means of a robustly stabilizing feedback controller $C(s)$, i.e., $u=C(r-y)$, see Figure 3.3. Substitution of the feedback relation into (3.5) results in the closed-loop input-output relation for linear uncertain systems, given by


\begin{equation*}
y=S_{\Delta}\left(G_{u} C r+G_{d} d+G_{f} f\right), \tag{3.6}
\end{equation*}


where $S_{\Delta}=\left(I+G_{u} C\right)^{-1}$ is the uncertain sensitivity function.\\
The closed-loop system is augmented with a fault detection system which takes as inputs the control input $u$ and output $y$. The fault detection system generates residual signals $\epsilon(t) \in \mathbb{R}^{n_{y}}$ that allow to detect faults $f$, despite the influence of the external disturbances $r$ and $d$. All residual generators can be parameterized as [64]


\begin{equation*}
\epsilon=R(s)\left(\tilde{M}_{u}(s) y-\tilde{N}_{u}(s) u\right), \tag{3.7}
\end{equation*}


where $R(s) \in \mathcal{R} \mathcal{H}_{\infty}^{n_{y} \times n_{y}}$ is a post-filter of the pre-residual $\tilde{\epsilon}=\tilde{M}_{u}(s) y- \tilde{N}_{u}(s) u$, which in time-domain is $\tilde{\epsilon}(t) \in \mathbb{R}^{n_{y}}$. The transfer function matrices $\tilde{M}_{u}(s), \tilde{N}_{u}(s) \in \mathcal{R} \mathcal{H}_{\infty}$ form a left coprime factorization of the nominal plant $G_{u}(s, 0)$, i.e., $G_{u}(s, 0)=\tilde{M}_{u}^{-1} \tilde{N}_{u}$. The uncertain feedback system (3.6) augmented with the fault detection system (3.7) is graphically depicted in Figure 3.3.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-080}
\captionsetup{labelformat=empty}
\caption{Figure 3.3: Generic fault detection configuration for uncertain closed-loop controlled systems. The control input $u$ and output $y$ form the inputs for the fault detection (FD) system which generates the residual signal $\epsilon$.}
\end{center}
\end{figure}

Let the uncertain part of the plant $G_{u}(s, \Delta)$ be defined as


\begin{equation*}
\tilde{G}_{u}(s, \Delta):=G_{u}(s, \Delta)-G_{u}(s, 0) . \tag{3.8}
\end{equation*}


Substitution of (3.6), the control law $u=C(r-y)$, and (3.8) into (3.7), gives the residual dynamics for uncertain closed-loop systems as


\begin{align*}
\epsilon= & R \tilde{M}_{u}\{\underbrace{\tilde{G}_{u}(s, \Delta) C S_{\Delta}}_{T_{\tilde{\epsilon} r}^{\Delta}} r+\underbrace{\left(G_{d}(s)-\tilde{G}_{u}(s, \Delta) C S_{\Delta} G_{d}(s)\right.}_{T_{\tilde{\epsilon} d}^{\Delta}}) d \\
& +\underbrace{\left(G_{f}(s)-\tilde{G}_{u}(s, \Delta) C S_{\Delta} G_{f}(s)\right.}_{T_{\tilde{\epsilon} f}^{\Delta}}) f\} \tag{3.9}
\end{align*}


To enhance readability, let $T_{\tilde{\epsilon} r}(s, \Delta)=\tilde{M}_{u} T_{\tilde{\epsilon} r}^{\Delta}, T_{\tilde{\epsilon} d}(s, \Delta)=\tilde{M}_{u} T_{\tilde{\epsilon} d}^{\Delta}$ and $T_{\tilde{\epsilon} f}(s, \Delta)=\tilde{M}_{u} T_{\tilde{\epsilon} f}^{\Delta}$ be defined as the uncertain transfers from the reference $r$, the disturbances $d$, and the faults $f$, to the pre-residual $\tilde{\epsilon}$. Then, the residual dynamics for uncertain closed-loop systems can be written as

\[
\epsilon=R \underbrace{\left[\begin{array}{cc}
T_{\tilde{\epsilon} r}(s, \Delta) & T_{\tilde{\epsilon} d}(s, \Delta)
\end{array}\right]}_{\tilde{G}_{d}(s, \Delta)} \underbrace{\left[\begin{array}{c}
r  \tag{3.10}\\
d
\end{array}\right]}_{\tilde{d}}+R T_{\tilde{\epsilon} f}(s, \Delta) f .
\]

where $\tilde{G}_{d}(s, \Delta)$ describes the transfer from the extended disturbance input $\tilde{d}= \begin{array}{ll}{[r} & d]^{\top}\end{array}$ to the pre-residual $\tilde{\epsilon}$.

The residual dynamics clearly show the inherent trade-off between sensitivity to faults and robustness against external disturbances and the effect of modeling uncertainties. It is desirable to minimize $R \tilde{G}_{d}(s, \Delta)$ for all $\Delta \in \Delta$ in order to reduce the impact of $r$ and $d$ on the residual. Conversely, it is also desirable to maximize $R T_{\tilde{\epsilon} f}(s, \Delta)$ for all $\Delta \in \boldsymbol{\Delta}$ to enhance fault sensitivity.

A natural way to evaluate the robustness against disturbances $\tilde{d}$ is through the $\mathcal{H}_{\infty}$ norm, whereas characterizing the sensitivity of faults necessitates a more intricate approach. The singular values of a matrix give a measure for the amplification in the direction of maximum action among all directions orthogonal to the singular vectors of any larger singular value. In essence, these give a measure for amplification in the principal directions of a system. In this context, all singular values $\sigma_{i}\left(R(j \omega) T_{\tilde{\epsilon} f}(j \omega, \Delta)\right), \omega \in[0, \infty), \Delta \in \boldsymbol{\Delta}$, where $i=1, \ldots, n_{\sigma}$ and $n_{\sigma}=\min \left(n_{y}, n_{f}\right)$, together form a measure of the fault sensitivity and the corresponding singular values cover all directions of the subspace spanned by $R(j \omega) T_{\tilde{\epsilon} f}(j \omega, \Delta)$.

Given the measure for worst-case disturbance amplification and the measure for fault sensitivity, the following robust $\mathcal{H}_{i} / \mathcal{H}_{\infty}$ performance index is defined which considers all elements $\bar{\Delta} \in \boldsymbol{\Delta}$ as


\begin{equation*}
J_{i, \omega, \bar{\Delta}}(R)=\frac{\sigma_{i}\left(R(j \omega) T_{\tilde{\epsilon} f}(j \omega, \bar{\Delta})\right)}{\left\|R(s) \tilde{G}_{d}(s, \Delta)\right\|_{\infty}} \tag{3.11}
\end{equation*}


Here, $\left\|R(s) \tilde{G}_{d}(s, \Delta)\right\|_{\infty}$ is based on all $\Delta \in \boldsymbol{\Delta}$, whereas $J_{i, \omega, \bar{\Delta}}(R)$ relates to the fault sensitivity based on an element $\bar{\Delta} \in \Delta$. Both stem from the same set.

The objective is to find the fault detection filter that maximizes the ratio $J_{i, \omega, \bar{\Delta}}$ in (3.11) for all singular values $i$, at every frequency $\omega$, and for every element $\bar{\Delta} \in \Delta$.

Problem 3.7. Consider the residual dynamics (3.9) of a closed-loop uncertain system described by (3.6) and let $\gamma>0$ be a user-defined combined disturbance and uncertainty rejection level. Find a single stable TFM Ropt $(s) \in \mathcal{R} \mathcal{H}_{\infty}^{n_{y} \times n_{y}}$ such that


\begin{equation*}
R_{\mathrm{opt}}(s)=\underset{R(s) \in \mathcal{R} \mathcal{H}_{\infty}}{\arg \sup }\left\{\sigma_{i}\left(R(j \omega) T_{\tilde{\epsilon} f}(j \omega, \bar{\Delta})\right) \mid\left\|R(s) \tilde{G}_{d}(s, \Delta)\right\|_{\infty} \leq \gamma\right\} \tag{3.12}
\end{equation*}


for all $i=1, \ldots, n_{\sigma}$, for all $\bar{\Delta} \in \boldsymbol{\Delta}$, and for all $\omega \in[0, \infty)$, where $\Delta \in \boldsymbol{\Delta}$.\\
Remark 3.8. Note that the introduction of $\gamma>0$ has no effect on the performance index (3.11). Since the filter $R(j \omega)$ can be scaled arbitrarily, the bound $\gamma$ merely serves as a scaling parameter that has no influence on the optimal ratio, but ensures that the optimal solution for optimization problem (3.12) is unique.

\subsection*{3.4 Solution}
In this section, the solution to the robust fault detection filter optimization problem (3.12) is presented. First, an upper-bound envelope is established to encapsulate the worst-case scenario of the uncertain exogenous disturbances. Subsequently, this envelope is used to solve the optimal robust fault detection filter design problem, followed by several remarks.

Lemma 3.9. Let $\tilde{d}_{1}$ and $\tilde{d}_{2}$ be unitary input vectors, i.e., $\left\|\tilde{d}_{1}\right\|_{2}=\left\|\tilde{d}_{2}\right\|_{2}=1$, then $\bar{G}_{d}(s) \in \mathcal{R} \mathcal{H}_{\infty}^{n_{y} \times\left(n_{y}+n_{d}\right)}$ is defined as the upper-bound envelope of $\tilde{G}_{d}(s, \Delta)$ if for all vectors with $\left\|\tilde{d}_{2}\right\|_{2}=1$ the following condition is satisfied


\begin{equation*}
\left\|\tilde{G}_{d}(j \omega, \Delta) \tilde{d}_{1}\right\|_{2} \leq\left\|\bar{G}_{d}(j \omega) \tilde{d}_{2}\right\|_{2} \quad \forall \omega, \forall \Delta \in \boldsymbol{\Delta} \tag{3.13}
\end{equation*}


where $\tilde{d}_{1}$ a unitary vector such that the output of the particular realization of $\tilde{G}_{d}(j \omega, \Delta) \tilde{d}_{1}$ matches the direction of $\bar{G}_{d}(j \omega) \tilde{d}_{2}$. The upper-bound envelope condition (3.13) is satisfied if $\bar{\sigma}\left(\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta)\right) \leq 1$ for all $\omega$ and all $\Delta \in \boldsymbol{\Delta}$, where $\bar{G}_{\text {do }}$ is the outer of $\bar{G}_{d}$.

Proof. The proof is provided in [46, Lemma 4].\\
Lemma 3.10. Let $\tilde{d}_{1}$ and $\tilde{d}_{2}$ be unitary input vectors, then $\bar{G}_{d}(s) \in \mathcal{R} \mathcal{H}_{\infty}^{n_{y} \times\left(n_{y}+n_{d}\right)}$ is defined as a tight upper-bound envelope of $\tilde{G}_{d}(s, \Delta)$ if for all $\left\|\tilde{d}_{2}\right\|=1$ the following condition is satisfied


\begin{equation*}
\sup _{\Delta \in \boldsymbol{\Delta}}\left\|\tilde{G}_{d}(j \omega, \Delta) \tilde{d}_{1}\right\|_{2}=\left\|\bar{G}_{d}(j \omega) \tilde{d}_{2}\right\|_{2} \quad \forall \omega, \tag{3.14}
\end{equation*}


where $\tilde{d}_{1}$ a unitary vector such that the output of the particular realization of $\tilde{G}_{d}(j \omega, \Delta) \tilde{d}_{1}$ matches the direction of $\bar{G}_{d}(j \omega) \tilde{d}_{2}$. The upper-bound envelope condition (3.14) is satisfied if and only if $\sup _{\Delta \in \Delta}\left\{\sigma_{i}\left(\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta)\right)\right\}=1$ for each $i=1, \ldots, n_{\sigma}$, at every $\omega$, where $\bar{G}_{\text {do }}$ is the outer of $\bar{G}_{d}$.

Proof. The proof is provided in [46, Lemma 5].\\
Hence, a tight upper bound implies that the worst-case amplification of $\tilde{G}_{d}(j \omega, \Delta)$ is equal to the amplification of $\bar{G}_{d}(j \omega)$ at every $\omega$. Next, the complete solution to the robust fault detection filter design problem is presented.

Theorem 3.11. Let the optimal fault detection filter which maximizes (3.12) be parameterized as

\[
\epsilon=R_{\mathrm{opt}}\left[\begin{array}{ll}
\tilde{M}_{u} & -\tilde{N}_{u}
\end{array}\right]\left[\begin{array}{c}
y  \tag{3.15}\\
u
\end{array}\right]
\]

and suppose the following assumptions are satisfied.

\begin{enumerate}
  \item Let $\bar{G}_{d}=\left(A, B_{d}, C, D_{d}\right)$ be written as a state-space system with $A$ Hurwitz and $(A, C)$ detectable.
  \item $D_{d}$ has full row rank.
  \item $\bar{G}_{d}(s)$ has no transmission zeros on the imaginary axis.
  \item Let $\bar{G}_{d}(s)$ be a tight upper-bound realization of $\tilde{G}_{d}(s, \Delta)$.
\end{enumerate}

Then, there exists a co-inner-outer factorization of $\bar{G}_{d}(s)=G_{d o}(s) G_{d i}(s)$ with the optimal post-filter $R_{\text {opt }}(s)=\gamma G_{\text {do }}^{-1}$ and $\tilde{M}_{u}, \tilde{N}_{u} \in \mathcal{R} \mathcal{H}_{\infty}$ any $L C F$ of the nominal system $G_{u}(s, 0)$, which achieves for all $\omega, i=1, \ldots, n_{\sigma}, \bar{\Delta} \in \boldsymbol{\Delta}$

$$
\sup _{R(s) \in \mathcal{R} \mathcal{H}_{\infty}} J_{i, \omega, \bar{\Delta}}\left(R_{\mathrm{opt}}\right)=\sigma_{i}\left(R(j \omega) T_{\tilde{\epsilon} f}(j \omega, \bar{\Delta})\right) .
$$

The corresponding state-space representation of $R_{\mathrm{opt}}$ is given by

\[
R_{\mathrm{opt}}(s)=\gamma\left[\begin{array}{c|c}
A+L_{0} C & L_{0}  \tag{3.16}\\
\hline R_{d}^{-1 / 2} C & R_{d}^{-1 / 2}
\end{array}\right] \in \mathcal{R} \mathcal{H}_{\infty}
\]

in which $R_{d}:=D_{d} D_{d}^{T}>0$ and $Y \geq 0$ is the stabilizing solution to the Riccati equation

$$
\begin{gathered}
\left(A-B_{d} D_{d}^{T} R_{d}^{-1} C\right) Y+Y\left(A-B_{d} D_{d}^{T} R_{d}^{-1} C\right)^{T}- \\
Y C^{T} R_{d}^{-1} C Y+B_{d}\left(I-D_{d}^{T} R_{d}^{-1} D_{d}\right) B_{d}^{T}=0
\end{gathered}
$$

such that $A-B_{d} D_{d}^{T} R_{d}^{-1} C-Y C^{T} R_{d}^{-1} C$ is stable and


\begin{equation*}
L_{0}:=-\left(B_{d} D_{d}^{T}+Y C^{T}\right)^{T} \tag{3.17}
\end{equation*}


Proof. The proof is provided in [46, Theorem 1].\\[0pt]
A series of observations is presented concerning the assumptions 1 to 3 in [46]. The presented strategy can be interpreted as follows. The LCF $\tilde{N}_{u}, \tilde{M}_{u}$ cancels the effect of the nominal closed loop and the remaining fault-to-disturbance optimization is performed by $R$. To be precise, this filter is based on the overbound $\bar{G}_{d}(s)$ of $\tilde{M}_{u}\left[T_{y r}^{\Delta}(s, \Delta) \quad T_{y d}^{\Delta}(s, \Delta)\right]$.

In order to interpret the optimal post-filter, $R(s)=\gamma G_{d o}^{-1}(s)$, recall that the co-outer of a TFM can be interpreted as a frequency domain magnitude profile. Hence, by multiplication with the inverse of the magnitude profile, the worstcase influence of the exogenous disturbance $\tilde{d}$ on the residuals $\epsilon$ is homogenized across the entire subspace. As a consequence, the optimal solution affects the TFM from faults $f$ to residuals $\epsilon$ by an inverse weighting of this magnitude profile.

To attain the supremum as defined in (3.12) and thereby achieve optimal fault detection performance, it is imperative that the upper-bound realization satisfies Assumption 4. This condition is inherently met if $\bar{G}_{d}(s) \in \tilde{G}_{d}(s, \Delta)$ and (3.13) holds for all frequencies $\omega$ and uncertainties $\Delta \in \boldsymbol{\Delta}$. In this case, the worst-case realization consistently bounds the other realizations across all frequencies from above. However, if the upper bound complies with (3.13), but is not tight, conservatism is introduced resulting in a suboptimal, yet still robust, filter in view of (3.12). In this case, the result is merely optimal given the conservative upper bound.

Next, the proposed method is applied to a prototype wafer stage. In the next section, the entire process is addressed from system identification, fault detection filter synthesis, to residual generation for the time-domain responses.

\subsection*{3.5 Experimental results}
In this section, a robust fault detection filter is designed and applied to the prototype wafer stage introduced in Section 3.3.1. First, the setup is revisited to examine the models required for fault diagnosis filter synthesis and the related closed-loop block diagram for filter synthesis. Following this, the procedure for system identification is described, with particular attention to the uncertainty of the obtained model. Subsequently, the robust fault diagnosis filter is synthesized and applied to the wafer stage. Finally, the time-domain response of the residual signals is examined.

\subsection*{3.5.1 Setup}
Consider the schematic overview of the wafer stage in Figure 3.4. For the purpose of validating the proposed fault detection approach, solely the out-of-plane $z$ direction is considered. The other DOFs, i.e., the three rotations and the position in horizontal plane, are regulated to zero by decentralized controllers. Hence, it is assumed that the interaction between the controlled DOFs is negligible. In the $z$ direction, four actuators, denoted as $u_{i}, i=1, \ldots, 4$, are positioned at the corners of the chuck. The position of the chuck is measured by four sensors, denoted as $y_{i}, i=1, \ldots, 4$, located near the actuators. Additionally, the positions of the two considered faults are highlighted: one fault is located at actuator 1, and the other fault occurs between actuators 1 and 2 . The first actuator is manipulated such that an additional force is superimposed to resemble an actuator fault. An additional actuator is positioned underneath the chuck between the first and second input to apply the second fault. Both of these are highlighted in Figure 3.4.

Consider the schematic overview of the wafer stage in Figure 3.4. For the purpose of validating the proposed fault detection approach, solely the out-ofplane $z$-direction is considered. The other DOFs, i.e., the three rotations and

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-085}
\captionsetup{labelformat=empty}
\caption{Figure 3.4: Schematic drawing of the chuck of the prototype wafer stage. The four inputs \$u\_\{i}\$ and outputs $y_{i}$, located at the corners of the stage $i=1, \ldots, 4$, are depicted. Additionally, the location of the two faults $f_{i}, i=1,2$, are indicated.\}\end{center}
\end{figure}

the positions in horizontal plane, are regulated to zero by decentralized controllers. Hence, it is assumed that the interaction between the controlled DOFs is negligible. In the $z$-direction, four actuators, denoted by $u_{i}, i=1, \ldots, 4$, are positioned at the corners of the chuck. The position of the chuck is measured by four sensors located near the actuators, denoted by $y_{i}, i=1, \ldots, 4$. Additionally, the positions of the two considered faults are highlighted: actuator 3 is assumed to be susceptible to faults, and the other fault is an external anomalous force between actuators 1 and 2 . The third actuator is manipulated such that an additional force is superimposed to resemble an actuator fault. An additional actuator is positioned underneath the chuck between the first and second input to apply the second fault. Both of these are highlighted in Figure 3.4.

The rigid body in $z$-direction is controlled by a robustly stabilizing feedback controller $C_{\mathrm{fb}}: e_{\mathrm{rb}} \mapsto u_{\mathrm{rb}}$, and follows a setpoint $r$, where $e_{\mathrm{rb}}=r-z_{\mathrm{rb}}$. The rigid body displacement $z_{\mathrm{rb}}=T_{y} z$, where $z=\left[\begin{array}{llll}y_{1} & y_{2} & y_{3} & y_{4}\end{array}\right]^{\top}$, and $T_{y}= \frac{1}{4}\left[\begin{array}{llll}1 & 1 & 1 & 1\end{array}\right]$. The controller distributes the input over the four actuators via $u=T_{u} u_{\mathrm{rb}}$, where $T_{u}=T_{y}^{\top}$ and $u=\left[\begin{array}{llll}u_{1} & u_{2} & u_{3} & u_{4}\end{array}\right]^{\top}$. The corresponding block diagram is depicted in Figure 3.5.

The first step is to reconstruct the first part of the residual dynamics $\tilde{G}_{d}(\Delta)$, see (3.10). To this end, parametric models of the plant, the controller, and the disturbance are required. Additionally, the fault model is required in order to evaluate the performance of the eventually obtained fault detection filter. The next subsection describes in detail how to identify a model of the system $G_{u}$ with a measure for the uncertainty $\Delta$. The controller $C_{\mathrm{fb}}$ and the input and output transformation matrices $T_{u}$ and $T_{y}$ are known. Since the stage is magnetically levitated and the sensors are of high quality, solely measurement

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-086}
\captionsetup{labelformat=empty}
\caption{Figure 3.5: Block diagram for the synthesis of a fault detection filter for the prototype wafer stage. The uncertain plant \$G\_\{u}(\textbackslash Delta)\$, disturbance model $G_{d}$, and fault model $G_{f}$ are depicted. The four sensor outputs are transformed by $T_{y}$ to a the rigid body displacement $z_{\mathrm{rb}}$. The error is processed by the feedback controller $C_{\mathrm{fb}}$ and the input is distributed over the four actuators by $T_{u}$. The fault detection filter consists of a LCF of the nominal plant $G_{u}(0)$, denoted by $\tilde{N}_{u}, \tilde{M}_{u}$, and the optimal post-filter is described by $R$, resulting in the residual signals $\varepsilon$.\}\end{center}
\end{figure}

noise is included in the considered disturbance model $G_{d}$. It is assumed that each sensor is perturbed with Gaussian white noise with a standard deviation of $\sigma_{\text {std }}=10 \mathrm{~nm}$. This results in a diagonal transfer function matrix where the gain is equal to $\sigma_{\text {std }}$, i.e., $G_{d}=\sigma_{\text {std }} I_{4}$. Since the faults are artificially applied by two actuators, the transfer functions from the fault to the sensors can be identified and used as fault model $G_{f}(s)$. This model is depicted in Figure 3.6. This model is identified with the same procedure as the plant itself and is described in the next subsection.

\subsection*{3.5.2 System identification}
To identify the system $G_{u}$, first multisine experiments are performed. Subsequently, a nominal system is estimated in a model structure. Finally, an additive uncertain uncertainty model is embedded.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-087}
\captionsetup{labelformat=empty}
\caption{Figure 3.6: Parameteric fault model \$G\_\{f}(s)\$ obtained through mutltisine injection and frequency domain simplified refined instrumental variable (SRIVC) method with integrated prediction error minimization (IPEM).\}\end{center}
\end{figure}

3.5.2.1 Frequency domain identification framework: Consider the system identification scheme in Figure 3.7, where $G_{u}$ is a $n_{y} \times n_{u}$ LTI system. The signal $w \in \mathbb{R}^{n_{u}}$ is a periodic random phase multisine excitation signal and the output $y$ is perturbed with measurement noise $v_{y}$. Such multisine is defined as

$$
w(t)=\frac{1}{\sqrt{N}} \sum_{k=1}^{N / 2-1} U_{k} \cos \left(2 \pi k f_{0} t+\phi_{k}\right),
$$

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-088}
\captionsetup{labelformat=empty}
\caption{Figure 3.7: Block diagram for closed-loop system identification. The signal $w$ is a periodic multisine excitation signal. The output $y$ is perturbed with measurement noise \$v\_\{y}\$ characterized by a filtered independent and identically distributed random sequence. The signals $w, u$, and $y$ are used for closed-loop identification.\}\end{center}
\end{figure}

where the phase $\phi_{k}$ is random and uniformly distributed, $N$ is the total number of points per period, and $f_{0}$ is the frequency resolution. The amplitudes $U_{k}$ can be chosen to follow a desired amplitude spectrum. In this case, a uniform spectrum is chosen. The signals $w, u$, and $y$ are measured for system identification.

The discrete Fourier transformed (DFT) signals $u(t)$ and $y(t)$ are measured and are given by

\[
\left[\begin{array}{c}
Y(k)  \tag{3.18}\\
U(k)
\end{array}\right]=\left[\begin{array}{c}
G(k) \\
I
\end{array}\right] S(k) W(k)+\left[\begin{array}{c}
V_{y}(k) \\
V_{u}(k)
\end{array}\right],
\]

where $W(k)$ is the DFT of $w(t)$, and $V_{y}(k)$ and $V_{u}(k)$ are the DFT of the noise contribution of $v_{y}$ onto $y$ and $u$, respectively. The system $S(k)=(I+ \left.T_{u} C_{\mathrm{fb}}(k) T_{y} G_{u}(k)\right)^{-1}$ denotes the input sensitivity function.

A total of $M$ experiments are performed with $P$ steady-state periods, i.e., the first few periods that contain transient data are discarded. Subsequently, the best linear approximation (BLA) is computed using the robust method [202]. This is realized by computing


\begin{equation*}
\hat{G}_{\mathrm{bla}}(k)=\frac{1}{M P} \sum_{m=1}^{M} \sum_{p=1}^{P} Y^{[m, p]}(k) U^{[m, p]^{-1}}(k), \tag{3.19}
\end{equation*}


where [ $m, p$ ] refers to the DFTs of the $m^{\text {th }}$ experiment and $p^{\text {th }}$ period. A closedloop identification experiment is performed with $M=10$ realizations of $P=$ 10 steady-state periods. A multisine is injected in the frequency band $\omega= [0.22000] \mathrm{Hz}$ with a resolution of $f_{0}=0.2 \mathrm{~Hz}$. The BLA is computed and shown in Figure 3.8. This nonparametric frequency domain estimate serves as a basis to obtain a parametric uncertain model.\\
3.5.2.2 Nominal parametric model estimation: A parametric model $G_{u}$ is obtained using a novel modal identification algorithm based on the simplified\\
refined instrumental variable method (SRIVC) and integrated prediction error minimization (IPEM). To this end, the system is parameterized as

$$
G(s, \rho)=\sum_{i=1}^{i=n_{\mathrm{rb}}} \frac{\phi_{i} \psi_{i}^{\top}}{s^{2}}+\sum_{i=1}^{i=n_{\text {flex }}} \frac{\phi_{i} \psi_{i}^{\top}}{s^{2}+2 \zeta_{i} \omega_{i} s+\omega_{i}^{2}},
$$

where the parameter vector $\rho=\left[\phi_{1}, \ldots, \phi_{n_{\mathrm{rb}}+n_{\mathrm{flex}}}, \psi_{1}, \ldots, \psi_{n_{\mathrm{rb}}+n_{\mathrm{flex}}}, \zeta_{1}, \ldots\right.$, $\left.\zeta_{n_{\text {flex }}}, \omega_{1}, \ldots, \omega_{n_{\text {flex }}}\right]$. The algorithm, including a detailed analysis, will be published elsewhere. In essence, this is a frequency domain equivalent and the MIMO extension of the approach proposed in [112], where additive SISO systems are estimated with time-domain data using the simplified refined instrumental variable method (SRIVC) for additive systems. Using this approach a $20^{\text {th }}$-order modal model is estimated which has $n_{\mathrm{rb}}=1$ and $n_{\text {flex }}=9$. A $12^{\text {th }}$-order approximate model is obtained by truncating the last modes such that $n_{\mathrm{rb}}=1$ and $n_{\text {flex }}=4$. This model is assumed to be the nominal model $G_{0}(s)$, and is depicted in Figure 3.8.\\
3.5.2.3 Uncertain model estimation: To obtain the uncertain plant model $G_{u}(\Delta)$, it is important that the model set includes the true system under consideration. This inclusion precisely allows to account for the impact of uncertainty in the residuals. The uncertain plant $G_{u}$ is constructed using an additive uncertainty, expressed as


\begin{equation*}
G_{u}=G_{0}+\Delta, \tag{3.20}
\end{equation*}


with $\Delta \in \hat{\boldsymbol{\Delta}}$, where $\hat{\boldsymbol{\Delta}}=\left\{\Delta \in \mathcal{R} \mathcal{H}_{\infty} \mid\|\Delta\|_{\infty} \leq \alpha\right\}$, or equivalently as

$$
G_{u}=\mathcal{F}_{u}(H, \Delta),
$$

with

$$
H=\left[\begin{array}{cc}
0 & I \\
\alpha & G_{0}
\end{array}\right],
$$

with $\Delta \in \hat{\boldsymbol{\Delta}}$, where $\hat{\boldsymbol{\Delta}}=\left\{\Delta \in \mathcal{R} \mathcal{H}_{\infty} \mid\|\Delta\|_{\infty} \leq 1\right\}$. The parameter $\alpha$ is computed by $\alpha=\left\|\hat{G}_{\text {bla }}-G_{0}\right\|_{\infty}$. The resulting uncertain plant $G_{u}(\Delta)$ together with the frequency response data and nominal model $G_{u}(0)=G_{0}$ are shown in Figure 3.8.

Remark 3.12. Optionally, the covariance of the BLA is used to create a confidence bound related to the FRF measurement. This confidence bound can be used as a basis to shape the uncertain system such that the model set encompasses the entire confidence region. To this end, it might be beneficial to shape $\Delta$ with a dynamic filter $W(s)$ giving the plant $G_{u}=G+W \Delta$. Instead of the additive structure, other structures such as multiplicative or dual-Youla-Kučera uncertainty structures may be more suitable [246].

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-090}
\captionsetup{labelformat=empty}
\caption{Figure 3.8: Bode magnitude plots of the BLA in (-), measured using multisine excitation and the robust method, the \$20\^{}\{\textbackslash text \{th }\}\$-order fit in (-), obtained using the frequency domain simplified refined instrumental variable (SRIVC) method with integrated prediction error minimization (IPEM), the $12^{\text {th }}$-order nominal plant $G_{0}$ in (-), and the uncertain plant $G_{u}(\Delta)$ in ( $\square$ ).\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-091(1)}
\captionsetup{labelformat=empty}
\caption{Figure 3.9: Singular values \$\textbackslash sigma\_\{i}\textbackslash left(\textbackslash tilde\{G\}\_\{d\}(s, \textbackslash Delta)\textbackslash right)\$ for 100 different uncertainties $\Delta \in \boldsymbol{\Delta}$ in (一), together with singular values of the upper-bound envelope $\bar{G}_{d}(s)$ in (---).\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-091}
\captionsetup{labelformat=empty}
\caption{Figure 3.10: Structured singular value \$\textbackslash mu\_\{\textbackslash Delta}\textbackslash left(\textbackslash bar\{G\}\textit{\{d o\}\^{}\{-1\}(j \textbackslash omega) \textbackslash tilde\{G\}}\{d\}(j \textbackslash omega, \textbackslash Delta)\textbackslash right)\$. Since the lower bound (-) and the upper bound (-) of the structured singular value are below $\gamma=1$ (---) the user-defined maximum disturbance sensitivity is satisfied.\}\end{center}
\end{figure}

\subsection*{3.5.3 Robust fault detection filter synthesis}
Now that $G_{u}(\Delta)$ is identified, the fault detection filter is designed. The first step in obtaining the fault detection filter is to construct the left coprime factors $\tilde{M}_{u}, \tilde{N}_{u}$ of the nominal system $G_{u}(0)$. This is achieved using Lemma 3.5. Since the system has two integrators, these poles are shifted to the left half plane in the factorization such that two stable factors are obtained.

The second step is the design of the post-filter $R$. First, an upper-bound envelope $\bar{G}_{d}(s)$ is designed. Several approaches can be pursued for this purpose [46]. The singular values of $\tilde{G}_{d}(s, \Delta)$ are depicted in Figure 3.9 as well as\\
the designed upper-bound envelope $\bar{G}_{d}(s)$. The filter is designed in an iterative fashion such that Lemma 3.9 is satisfied. The co-outer factor of $\bar{G}_{d}(s)$ is computed with Lemma 3.6 and the resulting optimal post-filter $R(s)$ is computed by means of Theorem 3.11 where $\gamma=1$. In this iterative procedure, it is verified whether the structured singular value satisfies $\mu_{\Delta}\left(\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta)\right) \leq 1$ such that $\bar{\sigma}\left(\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta)\right) \leq 1$ for all $\omega$ and all $\Delta \in \boldsymbol{\Delta}$, as shown in Figure 3.10. In addition, the singular values $\sigma_{i}\left(T_{\varepsilon \tilde{d}}\right) \leq 1$ are depicted in Figure 3.11, as well as the singular values of the resulting transfer from faults to residual $\sigma_{i}\left(T_{\varepsilon f}\right)$. In this case, the upper-bound envelope $\bar{G}_{d}(s)$ is solely designed using the maximum singular value $\bar{\sigma}\left(\bar{G}_{d o}^{-1}(j \omega) \tilde{G}_{d}(j \omega, \Delta)\right)$, yielding a diagonal post-filter $R$. To obtain a less conservative post-filter, an alternative iterative procedure is followed in [46]. The structured singular value check and $\sigma_{i}\left(T_{\varepsilon \tilde{d}}\right)$ show where to locally inflate the upper-bound envelope if the condition is not met, or at which frequencies to deflate the envelope to mitigate any conservatism. Finally, the obtained diagonal post-filter $R(s)$ is shown in Figure 3.12.

Hence, the singular values of transfer $T_{\epsilon \tilde{d}}:[r d] \rightarrow \epsilon$ are mapped below the specified bound $\gamma=1$ satisfying the constraint in Theorem 3.11 in Section 3.4, and in addition, amplification of transfer $T_{\epsilon f}: f \rightarrow \epsilon$ is achieved in the lower frequency region.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-092}
\captionsetup{labelformat=empty}
\caption{Figure 3.11: Singular values of transfer \$T\_\{\textbackslash epsilon \textbackslash tilde\{d}\}:\textbackslash left[\[
\begin{array}{ll}r & d\end{array}
\]\textbackslash right] \textbackslash rightarrow \textbackslash epsilon\$ in (-) together with bound $\gamma=1$ in (---) (left) and the singular values of transfer $T_{\epsilon f}: f \rightarrow \epsilon$ (right) for 100 different uncertainties $\Delta \in \boldsymbol{\Delta}$. In the lower frequency region, the faults are significantly amplified with respect to the exogenous disturbances. At higher frequencies, the exogenous disturbances are amplified more.\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-093}
\captionsetup{labelformat=empty}
\caption{Figure 3.12: Diagonal post-filter $R$ that is used in the experiment.}
\end{center}
\end{figure}

\subsection*{3.5.4 Experimental time-domain results}
An experiment has been performed on the prototype wafer stage to verify the obtained fault detection filter presented in Section 3.5.3. The reference and fault signals that are applied to the system are depicted at the bottom of Figure 3.13. The reference is a smooth $4^{\text {th }}$ order setpoint with a base frequency of 1 Hz and an amplitude of $1 \mathrm{e}-4 \mathrm{~m}$, starting from $T=5 \mathrm{~s}$. After $T=10 \mathrm{~s}$ the first fault $f_{1}$ is injected to the system with a duration of 10 s , and consists out of a square wave with an amplitude of 0.5 N and a frequency of 0.5 Hz . The second fault $f_{2}$ is injected at $T=30 \mathrm{~s}$ which remains for 5 s which is a constant force of 0.1 N . There are no additional disturbances injected to the system, so the only disturbance acting on the system is assumed to be the measurement noise with $\sigma_{\text {std }}=10 \mathrm{~nm}$.

The four top plots in Figure 3.13 show the residuals generated by the fault detection filter. The effect of the disturbance and setpoint are clearly suppressed and the fault is easily distinguished in the the residual signals. Deriving time-domain bounds for residual evaluation is beyond the scope of this chapter,

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-094}
\captionsetup{labelformat=empty}
\caption{Figure 3.13: Time response of the four normalized residual signals \$\textbackslash epsilon\_\{1}\$ to $\epsilon_{4}$ in (一). In the bottom plot, the applied reference $r$ is shown in (-) together with the applied faults $f_{1}$ in (-) and $f_{2}$ in ( $\cdots$ ). A detection threshold $|\beta|=0.05$ is plotted in (一). Indeed, the residuals exceed the user-defined $\beta$-bounds when a fault is present in the system.\}\end{center}
\end{figure}

however, based on the signal between $t=5 \mathrm{~s}$ and $t=10 \mathrm{~s}$, a conservative threshold for fault detection is selected as $|\beta|=0.05$. This enables clear distinction between a faulty situation compared to a fault-free situation, regardless of the effect of model uncertainty and disturbances acting on the system.

\subsection*{3.6 Conclusion}
In this chapter, a robust fault detection filter is designed for a closed-loop controlled prototype wafer stage. The procedure encompasses everything from identifying an uncertain plant model to analyzing the time-domain residual responses. The fault detection filter is derived by solving a single Riccati equation, which maximizes performance across all frequencies and is governed by an upper-\\
bound envelope that constrains the uncertainty and disturbance models. This approach provides an optimal balance between fault sensitivity and the rejection of disturbances and modeling uncertainties, representing the first application of such a robust fault detection method to high-precision production equipment which marks a significant advancement in the field of fault diagnosis for mechatronics.

\section*{Chapter 4}
\section*{Robust $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ Fault Detection for Closed-Loop Systems with Application to a Prototype Wafer Stage ${ }^{1}$ }
\begin{abstract}
Downtime in advanced production equipment is increasingly costly, emphasizing the need to minimize interruptions and maintain operational efficiency. Current diagnostics and maintenance strategies, often reactive or preventive, are proving insufficient in addressing the complexities and demands of modern production systems. Continuous monitoring through a fault diagnosis system offers a promising solution, yet the main challenge lies in designing a system capable of distinguishing faults from disturbances and modeling uncertainties, present in any practical application. To address this challenge, the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ paradigm for uncertain linear time invariant systems is adapted to fit $\mu$-synthesis and $\mu_{g}$-analysis tools, offering a framework for robust fault detection and isolation by effectively managing disturbances and modeling uncertainties. The $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ design and analysis framework is successfully applied to an experimental next-generation wafer stage prototype, marking a notable advancement in practical complex industrial settings.
\end{abstract}

\footnotetext{${ }^{1}$ The results in this chapter constitute Contribution III in Section 1.6. The chapter is based on: [41] K. Classens, S. de Rijk, J. van de Wijdeven, W. P. M. H. Heemels, and T. Oomen, "Robust $H_{-} / H_{\infty}$ Fault Detection for Closed-Loop systems with Application to a Prototype Wafer Stage," submitted for journal publication.
}\subsection*{4.1 Introduction}
Advanced fault diagnosis systems are essential for minimizing downtime and maintaining operational efficiency in high-stakes environments where traditional reactive or preventive maintenance strategies often fall short. In high-stakes environments such as high-precision production equipment and safety-critical applications, a thorough understanding of the underlying physics is crucial. This fundamental knowledge enables more accurate identification and interpretation of faults, which is important to ensure both the reliability and safety of these systems. Consequently, model-based approaches for Fault Detection and Isolation (FDI) have been gaining increasing interest due to their ability to leverage this detailed physical understanding.

Several methods have been proposed to design model-based fault diagnosis systems, as detailed in various textbooks $[17,21,35,63,94,115,137,197,232$, 262 ] and surveys [ $18,80,84,85,136,138,140,149,249,265,266,267$ ]. A distinction can be made between fault estimation and residual generation. In contrast to optimally estimating a fault indicating signal, residual generation does not only require attenuation of unknown inputs, but also requires a specified fault sensitivity. Hence, in contrast, a residual generation problem is inherently a mixed minimization and maximization problem, which motivated the introduction of the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ paradigm [35, 62, 64]. Fault estimation approaches thus mainly optimize for robustness and analyze fault sensitivity a postiori, whereas residual generation methods explicitly take fault sensitivity into account in its design. Roughly, these model-based methods are subdivided into parity spacebased methods, observer-based methods, and direct filtering approaches [125, 140, 170].

Achieving satisfactory performance in model-based fault diagnosis systems necessitates a delicate balance between fault sensitivity and disturbance rejection [62]. Various robust residual generation methods have been developed to address this need. Factorization-based techniques [62], often implemented through the solution of a Riccati equation [157], is one such approach. Alternatively, filters are synthesized by solving Linear Matrix Inequalities [134, 270]. These methods are considered optimal in that they make the residual as sensitive to faults as possible, assuming the disturbance and plant model are perfectly known.

However, models of real-world systems are inherently uncertain. To address this modeling uncertainty, robust methods based on minimizing an $\mathcal{H}_{\infty}$ norm or $\mu$-condition [ $70,125,154,165,166,167,207,216,241,242,285$ ] have been applied, sometimes in combination with $\mu_{g}$-analysis $[124,125,126,127,128$, 178] to analyze the performance in view of $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ requirements.

Many of the mentioned works consider fault diagnosis systems in an openloop setting, despite often being implemented in closed-loop controlled systems. Typical controllers actively compensate for faults, reducing the achievable fault sensitivity [125, 126]. This may yield unexpected residual responses, particularly\\[0pt]
when the presumed model deviates from the true system or when an uncertain model is considered and the control loop is not accounted for. This led to the development of integrated designs for diagnostics and control through a joint optimization criterion [147, 243]. However, in many industrial applications, the existing controller may not be altered, making this solution less suitable.

This work focuses the design and application of a direct filtering approach for residual generation for uncertain systems in closed loop. Hence, the $\mathcal{H}_{-}$fault sensitivity requirement, the modeling uncertainty, and the closed loop are explicitly addressed. To this end, the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ problem is recasted to a $\mathcal{H}_{\infty}$ condition [125, 126, 207] which allows to find a residual generator via $\mu$-synthesis and nonsmooth fixed structure synthesis [5, 53, 125]. Subsequently, a generalization of the structured singular value, $\mu_{g}$, is used for robust performance analysis in view of the original $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ requirements [178]. Where previously mentioned work focuses on the development of optimization algorithms to solve the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ problem, e.g., via tailored LMIs, this work aims to recast the problem such that widely accessible tools, such as $\mu$-tools [13], can directly be applied. Novelty lies in a new formulation for robust synthesis using a structured complex perturbation, which relaxes the requirements on the closed-loop performance channels. Additionally, a notable advancement is made towards complex industrial settings by application of the proposed approach to a mechatronic positioning system which is part of a lithography system for chip manufacturing.

The primary novelty of this work is summarized in the following contributions.

C1 The robust $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ problem is reformulated to enable a solution within the $\mathcal{H}_{\infty}$ framework with structured complex perturbation, significantly decreasing conservatism of the obtained residual generator.

C2 The proposed approach is experimentally validated on an industrial nextgeneration prototype wafer stage.

The structure of the chapter is as follows. After an introduction to the notation and preliminaries used, the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ problem is formulated. Subsequently, in Section 4.4, a solution is proposed to solve the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ problem in an $\mathcal{H}_{\infty}$ framework. In Section 4.5, $\mu$-analysis and synthesis is introduced with a structured perturbation. Subsequently, Section 4.6 shows how to recast the system such that standard fixed-structure synthesis tools can be used. In Section 4.7, $\mu_{g}$ is introduced to analyze robust performance of the synthesized filter. Eventually, the proposed approach is validated on a next-generation prototype wafer stage in Section 4.8 and the chapter concludes by summarizing the key findings.

\subsection*{4.2 Notation and preliminaries}
The sets of real numbers and nonnegative real numbers are indicated by $\mathbb{R}$ and $\mathbb{R}_{\geq 0}$. By $\|\cdot\|$ the Euclidean norm is defined. The maximum and minimum singular values of the matrix $A$ are denoted by $\bar{\sigma}(A)$ and $\underline{\sigma}(A)$, respectively. The real rational subspace of $\mathcal{H}_{\infty}$ is denoted by $\mathcal{R} \mathcal{H}_{\infty} . \quad y \in \mathcal{L}_{2}$ if $\|y\|_{2}^{2}= \int_{0}^{\infty} y^{\top}(t) y(t) \mathrm{d} t<\infty$, where $\|y\|_{2}$ denotes the $L_{2}$-norm. $y \in \mathcal{L}_{2 e}$ if $\|y\|_{2 T}^{2}= \int_{0}^{T} y^{\top}(t) y(t) \mathrm{d} t<\infty, T \in \mathbb{R}_{\geq 0}$. The Laplace operator is denoted by $s$.\\
Definition 4.1. (Linear fractional transformation) For matrices $N$ and $M= \left[\begin{array}{ll}M_{11} & M_{12} \\ M_{21} & M_{22}\end{array}\right]$ of appropriate partitioning, the lower linear fractional transformation (LFT) is defined as $\mathcal{F}_{l}(M, N)=M_{11}+M_{12} N\left(I-M_{22} N\right)^{-1} M_{21}$ and the upper LFT as $\mathcal{F}_{u}(M, N)=M_{22}+M_{21} N\left(I-M_{11} N\right)^{-1} M_{12}$, under the assumption that the involved matrix inverses exists.

The following definitions [157, 233, 270] are used in this chapter.\\
Definition 4.2. (Structured singular value) Consider a feedback interconnection defined by the equations $z=M x$, and $x=\Delta z$, where $M$ is a complex valued matrix and $\Delta$ and unknown complex valued matrix with known structure $\boldsymbol{\Delta}=\left\{\Delta=\operatorname{blkdiag}\left(\ldots, \Delta_{i}, \ldots\right) \mid i \in I\right\}$, where $I$ is the index set $(1, \ldots, n)$, with $n$ being the number of perturbation blocks. Note that the $\Delta_{i}$ blocks are not necessarily square. The structured singular value of a matrix $M$ with respect to the structured set $\boldsymbol{\Delta}$ is defined as


\begin{equation*}
\mu_{\boldsymbol{\Delta}}(M)=\frac{1}{\sup \{r \mid \operatorname{det}(I+M \Delta) \neq 0 \text { for all } \Delta \in r \boldsymbol{\Delta}\}} \tag{4.1}
\end{equation*}


An equivalent definition is introduced in [76],


\begin{align*}
\mu(M) & =\max _{\|x\|=1}\left\{\gamma \mid\left\|x_{i}\right\| \gamma \leq\left\|z_{i}\right\|, \forall i \in I\right\}, \\
& =\max _{\|x\|=1}\left\{\gamma \mid\left\|x_{i}\right\| \gamma=\left\|z_{i}\right\|, \forall i \in I\right\}, \tag{4.2}
\end{align*}


where the inequality may be replaced by equality as shown in [68].\\
Definition 4.3. (Minimum gain) The smallest gain of the continuous-time LTI system $G: \mathcal{L}_{2 e} \rightarrow \mathcal{L}_{2 e}$, that is the $\mathcal{H}_{-}$index, is defined as


\begin{equation*}
\|G\|_{-}=\inf _{\omega \in \mathbb{R} \cup\{\infty\}} \underline{\sigma}(G(j \omega)) . \tag{4.3}
\end{equation*}


The minimum gain is not a norm and therefore named the $\mathcal{H}_{-}$index.\\
Definition 4.4. (Maximum gain) The $\mathcal{H}_{\infty}$ norm of the continuous-time LTI system $G: \mathcal{L}_{2 e} \rightarrow \mathcal{L}_{2 e}$, denoted as $\|G\|_{\infty}$, is given by


\begin{equation*}
\|G\|_{\infty}=\sup _{\omega \in \mathbb{R} \cup\{\infty\}} \bar{\sigma}(G(j \omega)) . \tag{4.4}
\end{equation*}


\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-100}
\captionsetup{labelformat=empty}
\caption{Figure 4.1: Generic $N \Delta$-structure}
\end{center}
\end{figure}

Definition 4.5. ( $\mathcal{H}_{\infty}$ problem) Given a generalized plant $P(s)$, the $\mathcal{H}_{\infty}$ problem is to find the filter $K(s)$

$$
\min _{K(s)}\left\|\mathcal{F}_{l}(P(s), K(s))\right\|_{\infty} .
$$

This problem can be formulated as a convex optimization problem by turning it into a semidefinite program via linear matrix inequality constraints or by solving a pair of Riccati equations [286].

The following definitions and its relation to $\mu$ are used in the remainder of this chapter. To this end, consider the generic interconnection shown in Figure 4.1, where $N$ is partitioned as $N=\left[\begin{array}{ll}N_{11} & N_{12} \\ N_{21} & N_{22}\end{array}\right]$, corresponding to the dimensions of $w, z, \Delta_{z}$, and $\Delta_{w}$.

Definition 4.6. (Nominal Stability (NS)) The system is stable with no model uncertainty, i.e., $N S \Leftrightarrow N$ internally stable.

Definition 4.7. (Nominal Performance (NP)) The system satisfies the performance specifications with no model uncertainty, i.e., NP $\Leftrightarrow \bar{\sigma}\left(N_{22}\right)=\mu_{\Delta_{P}}< 1, \forall \omega$, and NS. Here, $\Delta_{P}=\mathbb{C}^{n_{w} \times n_{z}}$ with $n_{w}=\operatorname{dim}(w)$ and $n_{z}=\operatorname{dim}(z)$.

Definition 4.8. (Robust Stability (RS)) The system is stable for all plants in the model set, i.e., $R S \Leftrightarrow \mu_{\Delta}\left(N_{11}\right)<1, \forall \omega$, and $N S$.

Definition 4.9. (Robust Performance (RP)) The system satisfies the performance specifications for all plants in the model set, i.e., $R P \Leftrightarrow \mu_{\bar{\Delta}}(N)< 1, \forall \omega, \bar{\Delta}=\left[\begin{array}{cc}\Delta & 0 \\ 0 & \Delta_{P}\end{array}\right]$ and $N S$.\\
$\mu$-synthesis. At present, there are no direct methods to synthesize a $\mu$ optimal solution for robust performance. However, integrating $\mu$-analysis with $\mathcal{H}_{\infty}$-synthesis has proven to give satisfactory results. This process is known as $D K$-iterations. To this end, consider that

$$
\mu(N) \leq \min _{D \in \mathcal{D}} \bar{\sigma}\left(D N D^{-1}\right),
$$

where $D$ is a scaling matrix in line with the dimensions of $N$. The set $\mathcal{D}$ is a set of block-diagonal matrices whose structure is compatible with $\Delta$, i.e., $\Delta D=D \Delta$. The main idea is to minimize the peak value over frequency of this upper bound, i.e.,

$$
\min _{K}\left(\min _{D \in \mathcal{D}}\left\|D N(K) D^{-1}\right\|_{\infty}\right) .
$$

The $D K$-process refers to alternating this minimization with respect to $D$ and $K$ while holding the other fixed. To this end, initialize $D(s)$ as a stable rational transfer matrix with appropriate structure. Subsequently, the $D K$-iterations are performed as follows.

\begin{enumerate}
  \item $K$-step. Synthesize an $\mathcal{H}_{\infty}$ controller for the scaled problem. That is, $\min _{K}\left\|D N(K) D^{-1}\right\|_{\infty}$ with fixed $D(s)$.
  \item $D$-step. Find $D(j \omega)$ to minimize at each frequency $\bar{\sigma}\left(D N D^{-1}(j \omega)\right)$ with fixed $K$ (and thus fixed $N$ ).
  \item Fit the magintude of each element of $D(j \omega)$ to a stable and minimum phase transfer function $D(s)$ and go to Step 1.
\end{enumerate}

The algorithm terminates if the performance criterion is met, i.e., $\left\|D N D^{-1}\right\|_{\infty}<$ 1, or until the $\mathcal{H}_{\infty}$-norm no longer decreases. Although the $K$-step and $D$-step are convex, joint convexity is not guaranteed. Nonetheless, practical experience indicates that the method generally performs well in most scenarios.

\subsection*{4.3 Problem formulation}
Consider the uncertain system described by $P$ and $\Delta$ which is closed-loop controlled by a controller $C$, and is augmented with fault detection filter $Q$, see Figure 4.2. The system $P$ and controller $C$ are linear time-invariant (LTI) systems and consider a norm-bounded uncertainty $\Delta \in \boldsymbol{\Delta}:\|\Delta\|_{\infty} \leq 1$ in the set $\boldsymbol{\Delta}:= \left\{\Delta(s) \in \mathcal{R} \mathcal{H}_{\infty} \mid \Delta(j \omega) \in \Delta_{c}\right.$ for all $\left.\omega \in \mathbb{R} \cup\{\infty\}\right\}$, which admits the blockdiagonal structure $\boldsymbol{\Delta}_{c}=\left\{\Delta_{c}=\operatorname{diag}\left(p_{1} I, \ldots, p_{n_{r}} I, \delta_{1} I, \ldots, \delta_{n_{c}} I, \Delta_{1}, \ldots, \Delta_{n_{z}}\right) \mid\right. \left.p_{j} \in \mathbb{R}, \delta_{j} \in \mathbb{C}, \Delta_{j} \in \mathbb{C}^{p_{j} \times q_{j}},\left\|\Delta_{c}\right\|<1\right\}$. Note that $\left\|\Delta_{c}\right\|<1$ simply implies that $\left|p_{j}\right|<1$ for $j=1, \ldots, n_{r},\left|\delta_{j}\right|<1$ for $j=1, \ldots, n_{c}$, and $\left\|\Delta_{j}\right\|<1$ for $j=1, \ldots, n_{z}$.

The closed-loop system is driven by a disturbance $d \in \mathbb{R}^{n_{d}}$ and is susceptible to faults $f \in \mathbb{R}^{n_{f}}$. The control input $u \in \mathbb{R}^{n_{u}}$ and output $y \in \mathbb{R}^{n_{y}}$ serve as input of the residual generator $Q:=\left[\begin{array}{ll}Q_{y} & Q_{u}\end{array}\right]$ which produces the residual signals $\varepsilon \in \mathbb{R}^{n_{\varepsilon}}$. The internal signals associated with the uncertainty are $\Delta_{w} \in \mathbb{R}^{n_{\Delta}}$ and $\Delta_{z} \in \mathbb{R}^{n_{\Delta}}$. Consider the following assumptions and problem.

Assumption 4.10. It is assumed that the controller robustly stabilizes the closed-loop interconnection $\mathcal{F}_{l}\left(\mathcal{F}_{u}(P, \Delta), C\right)$.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-102}
\captionsetup{labelformat=empty}
\caption{Figure 4.2: Uncertain system \$\textbackslash mathcal\{F}\_\{u\}(P, \textbackslash Delta)\$, controlled by controller $C$, and augmented by the residual generator $Q$. The system is subject to disturbances $d$ and faults $f$. The input $u$ and output $y$ serve as inputs of the residual generation system which generates residual signals $\varepsilon$.\}\end{center}
\end{figure}

Assumption 4.11. It is assumed that the faults are almost detectable [215].\\
Problem 4.12. The aim is to find a residual generation filter $Q \in \mathcal{R} \mathcal{H}_{\infty}^{n_{\varepsilon} \times\left(n_{y}+n_{u}\right)}$ such that the residual signal $\varepsilon$ meets the following requirements.\\
$R 1\left\|T_{\varepsilon d}\right\|_{\infty}<\gamma_{1}$ for all $\Delta \in \boldsymbol{\Delta}:\|\Delta\|_{\infty} \leq 1$, where $T_{\varepsilon d}: d \mapsto \varepsilon$.\\
$R 2\left\|T_{\varepsilon f}\right\|_{-}>\gamma_{2}$ for all $\Delta \in \Delta:\|\Delta\|_{\infty} \leq 1$ in the frequency range $\Omega= \left[\omega_{1}, \omega_{2}\right]$, where $T_{\varepsilon f}: f \mapsto \varepsilon$. Here, $\Omega$ denotes the interval where faults are likely concentrated.

The scalars $\gamma_{1}, \gamma_{2} \in \mathbb{R}_{+}$are measures for the required disturbance attenuation and fault sensitivity, respectively, and

\[
\left[\begin{array}{c}
T_{\varepsilon d}  \tag{4.5}\\
T_{\varepsilon f}
\end{array}\right]=Q \mathcal{F}_{l}\left(\mathcal{F}_{u}(P, \Delta), C\right) .
\]

The effectiveness of the fault detection filter hinges on the ratio $\mathcal{J}=\frac{\gamma_{2}}{\gamma_{1}}$. Clearly, superior performance is achieved with a larger value of $\mathcal{J}$. Given that faults and disturbances might occur within overlapping frequency regions and share the same subspace, achieving a balanced compromise between fault sensitivity and disturbance attenuation is inherent to addressing this problem.

Remark 4.13. The problem also holds for open-loop systems and is obtained by setting $C=0$ in (4.12). In this case, the open-loop system must be internally stable.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-103}
\captionsetup{labelformat=empty}
\caption{Figure 4.3: Uncertain system \$\textbackslash mathcal\{F}\_\{u\}(G, \textbackslash Delta)\$, controlled by feedback controller $C$, and augmented by the residual generator $Q$. The system is driven by a setpoint $r$ and is subject to disturbances $d$ and faults $f$. The input $u$ and output $y$ serve as inputs of the residual generation system which generates residual signals $\varepsilon$.\}\end{center}
\end{figure}

The framework depicted in Figure 4.2 is versatile and applicable to many systems, including an uncertain plant under feedback control, which is briefly examined in the following example and is revisited in the experimental validation.

Example 4.14. Consider the uncertain plant model, described by $G= \left[\begin{array}{ll}G_{11} & G_{12} \\ G_{21} & G_{22}\end{array}\right]$ and uncertainty $\Delta$. The system is controlled with a feedback controller $C_{\mathrm{fb}}$ and follows a setpoint $r \in \mathbb{R}^{n_{y}}$, see Figure 4.3. The system is subjected to faults $f \in \mathbb{R}^{n_{y}}$ on the output side, and to an additive output disturbance $d_{y} \in \mathbb{R}^{n_{y}}$. Consider the stacked disturbance vector $d=\left[\begin{array}{c}r \\ d_{y}\end{array}\right]$. Then, this simple feedback loop fits the generic framework where

$$
\left[\begin{array}{c}
\Delta_{w} \\
y \\
u \\
u_{C}
\end{array}\right]=\underbrace{\left[\begin{array}{cccc}
G_{11} & 0 & 0 & G_{12} \\
G_{21} & D_{1} & I & G_{22} \\
0 & 0 & 0 & I \\
-G_{21} & D_{2} & -I & -G_{22}
\end{array}\right]}_{=P}\left[\begin{array}{c}
\Delta_{z} \\
d \\
f \\
y_{C}
\end{array}\right],
$$

with $D_{1}=\left[\begin{array}{ll}0 & I\end{array}\right]$, and $D_{2}=\left[\begin{array}{ll}I & -I\end{array}\right]$.

\subsection*{4.4 Solution: An $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ to an $\mathcal{H}_{\infty}$ approach}
This section describes a solution to Problem 4.12. To this end, the requirements R1 and R2 are reformulated into an $\mathcal{H}_{\infty}$-framework using a classical loop-shaping methodology. Next, in Section 4.5, the obtained uniform bounds are used in a

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-104}
\captionsetup{labelformat=empty}
\caption{Figure 4.4: Uncertain system \$\textbackslash mathcal\{F}\_\{u\}(P, \textbackslash Delta)\$, weighted by $W_{d}^{-1}$. Additionally, a fictitious performance channel $\tilde{\varepsilon}$ is created via the filter $W_{F}$. The system is controlled by controller $C$, and augmented by the residual generator $Q$.\}\end{center}
\end{figure}

$\mu$-synthesis framework with a structured complex perturbation where part of the performance channels is excluded, allowing to mitigate conservatism. Subsequently, in Section 4.6, the system is manipulated such that standard $\mu$-tools can be used for synthesis. Finally, the system is recasted in a framework such that non-smooth fixed-structure synthesis tools can be applied, which allows to reduce conservatism further.

Consider two real-rational and invertible shaping filters, describing the desired response from disturbance and faults to the residual, described by $W_{d}$ : $d \mapsto \tilde{d}$ and faults $W_{f}: f \mapsto \tilde{f}$, respectively, which satisfy

$$
\begin{aligned}
& \left\|W_{d}\right\|_{\infty} \leq \gamma_{1}, \\
& \left\|W_{f}\right\|_{-} \geq \gamma_{2} .
\end{aligned}
$$

In the following theorem, the weighted system is converted into uniform bounds.\\[0pt]
Theorem 4.15. (based on [126, 207]) Consider the robust disturbance suppression requirement $R 1$ and the robust fault sensitivity requirement $R 2$.

Consider a new full rank filter, $W_{F}$, such that $\left\|W_{f}\right\|_{-}=\frac{\gamma_{2}}{1+\gamma_{2}}\left\|W_{F}\right\|_{-}$and $\left\|W_{F}\right\|_{-}>1+\gamma_{2}$. Define the fictitious signal $\tilde{\varepsilon}$ such that $\tilde{\varepsilon}=\varepsilon-W_{F} f$, see Figure 4.4.

Then sufficient conditions for the robust disturbance suppression requirement and the robust fault sensitivity requirement to hold, are


\begin{equation*}
\left\|T_{\varepsilon d} W_{d}^{-1}\right\|_{\infty}<1, \Rightarrow\left\|T_{\varepsilon d}\right\|_{\infty}<\gamma_{1}, \tag{4.6}
\end{equation*}


and


\begin{equation*}
\left\|T_{\varepsilon f}-W_{F}\right\|_{\infty}<1 \Leftrightarrow\left\|T_{\tilde{\varepsilon} f}\right\|_{\infty}<1 \Rightarrow\left\|T_{\varepsilon f}\right\|_{-}>\gamma_{2} . \tag{4.7}
\end{equation*}


Proof. The proof is provided in Appendix 4.A.\\
Using the result presented in Theorem 4.15, the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ problem is recasted such that the $\mathcal{H}_{-}$condition is converted into a sufficient fictious $\mathcal{H}_{\infty}$ requirement. First, the system is represented in the generic $\Delta \bar{P} Q$-framework as illustrated in Figure 4.5. To this end, define the input of the filter $Q$ as $u_{Q}=\left[\begin{array}{ll}y & u\end{array}\right]^{\top}$, and the output as $y_{Q}=\varepsilon$. Now consider $\tilde{P}:\left[\begin{array}{lll}\Delta_{z} & d & f\end{array}\right]^{\top} \mapsto\left[\begin{array}{ll}\Delta_{w} & u_{Q}\end{array}\right]^{\top}$ which is partitioned according to its input and outputs as

\[
\tilde{P}=\mathcal{F}_{l}(P, C)=\left[\begin{array}{ccc}
\tilde{P}_{11} & \tilde{P}_{12} & \tilde{P}_{13}  \tag{4.8}\\
\tilde{P}_{21} & \tilde{P}_{22} & \tilde{P}_{23}
\end{array}\right],
\]

Then, $\bar{P}:\left[\begin{array}{llll}\Delta_{z} & \tilde{d} & f & y_{Q}\end{array}\right]^{\top} \mapsto\left[\begin{array}{llll}\Delta_{w} & \varepsilon & \tilde{\varepsilon} & u_{Q}\end{array}\right]^{\top}$, is given by

$$
\bar{P}=\left[\begin{array}{cccc}
\tilde{P}_{11} & \tilde{P}_{12} W_{d}^{-1} & \tilde{P}_{13} & 0 \\
0 & 0 & 0 & I \\
0 & 0 & -W_{F} & I \\
\tilde{P}_{21} & \tilde{P}_{22} W_{d}^{-1} & \tilde{P}_{23} & 0
\end{array}\right]
$$

Given the $\mathcal{H}_{\infty}$ performance requirements, described by (4.6) and (4.7), the main aim is to minimize the diagonal entries of $T:\left[\begin{array}{ll}\tilde{d} & f\end{array}\right]^{\top} \mapsto\left[\begin{array}{ll}\varepsilon & \tilde{\varepsilon}\end{array}\right]^{\top}$, given by

\[
\left.T=\mathcal{F}_{l}\left(\mathcal{F}_{u}(\bar{P}, \Delta), Q\right)\right]=\left[\begin{array}{ll}
T_{\varepsilon \tilde{d}} & T_{\varepsilon f}  \tag{4.9}\\
T_{\tilde{\varepsilon} \tilde{d}} & T_{\tilde{\varepsilon} f}
\end{array}\right]
\]

and achieve $\left\|T_{\varepsilon \tilde{d}}\right\|_{\infty}<1$ and $\left\|T_{\tilde{\varepsilon} f}\right\|_{\infty}<1$ such that the requirements R1 and R2 are satisfied in line with Theorem 4.15.\\
Remark 4.16. Since the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$-problem is now reformulated as an $\mathcal{H}_{\infty}$ problem, a sufficient condition to satisfy the requirements $R 1$ and $R 2$ follows from the Small Gain Theorem [60], i.e., $\left\|\mathcal{F}_{l}(\bar{P}, Q)\right\|_{\infty}<1$. Be aware that this approach is conservative since

\begin{enumerate}
  \item the uncertainty $\Delta$ may be structured. This structure is neglected due to the worst-case nature of the Small Gain Theorem.
  \item the off-diagonal entries are included in the Small Gain Theorem.
\end{enumerate}

Remark 4.17. The $\mathcal{H}_{\infty}$ synthesis problem which aims to find $Q$ such that $\min _{Q(s)}\| \| \mathcal{F}_{l}(\bar{P}(s), Q(s)) \|_{\infty}<1$, can be solved in various ways as implemented in hinfsyn.m. Finding a fault detection filter in this manner leads to a conservative outcome due to the implicit minimization of $T_{\varepsilon f}$. Namely, the sensitivity to faults should be as large as possible.

The main idea is to structure the complex matrix which stems from the $\mathcal{H}_{\infty}$ performance specification such that mainly $T_{\varepsilon \tilde{d}}$ and $T_{\tilde{\varepsilon} f}$ are minimized and the undesired off-diagonal channels are discarded. To this end, the next step is to adapt the complex perturbation block related to the joint generalized disturbance and performance channel.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-106}
\captionsetup{labelformat=empty}
\caption{Figure 4.5: System recasted in generic \$\textbackslash Delta \textbackslash bar\{P} Q\$-form. The closed-loop control system $\mathcal{F}_{u}(P, \Delta)$, the weights $W_{d}$, and $W_{F}$ are collapsed into $\bar{P}$, to create the performance channels $\varepsilon$, and $\tilde{\varepsilon}$.\}\end{center}
\end{figure}

\subsection*{4.5 Structured robust performance analysis and synthesis}
First robust performance analysis is considered with a structured performance block. Next, it is shown how to exploit this approach in standard $D K$-iteration tools to synthesize a fault detection filter which is guaranteed to achieve robust performance in view of the structured performance block.

\subsection*{4.5.1 $\mu$-analysis with structured performance}
In order to test for robust performance (RP), the system is rearranged into the $N \Delta$-form of Figure 4.1, where $N:\left[\begin{array}{lll}\Delta_{z} & \tilde{d} & f\end{array}\right]^{\top} \mapsto\left[\begin{array}{lll}\Delta_{w} & \varepsilon & \tilde{\varepsilon}\end{array}\right]$, which is equal to


\begin{equation*}
N=\mathcal{F}_{l}(\bar{P}, Q) . \tag{4.10}
\end{equation*}


Robust performance is tested by computing the structured singular value $\mu$ of $N$ as stated in the following theorem.

Theorem 4.18. Consider the uncertain system in Figure 4.1. Assume nominal stability such that $N$ is (internally) stable. Then,

$$
\begin{aligned}
R P & \Leftrightarrow\left\|\mathcal{F}_{u}(N, \Delta)\right\|_{\infty}<1, \quad \forall \Delta \in \Delta:\|\Delta\|_{\infty} \leq 1, \\
& \Leftrightarrow \quad \mu_{\hat{\Delta}}(N(j \omega))<1, \quad \forall \omega,
\end{aligned}
$$

where $\mu$ is computed with respect to the structure

$$
\hat{\Delta}=\left[\begin{array}{cc}
\Delta & 0 \\
0 & \Delta_{P}
\end{array}\right]
$$

and $\Delta_{P}$ is a full complex perturbation with the same dimension as $\mathcal{F}_{u}(N, \Delta)^{\top}$.\\[0pt]
Proof. The proof is provided in [233, Chapter 8].\\
The standard RP test includes all performance channels of $T$ in (4.9) due to the full complex block $\Delta_{p}$. The main idea is to relax this condition and to perform a test on a subset of the entries of $T$. To analyze the diagonal of $T$, consider the following theorem concerning what is referred to as robust partial performance (RPP), where partial indicates the partially filled complex performance block.

Theorem 4.19. Consider the uncertain system in Figure 4.1. Assume nominal stability such that $N$ is (internally) stable and consider $T=\mathcal{F}_{u}(N, \Delta)$, partitioned as $T=\left[\begin{array}{cc}T_{\varepsilon \tilde{d}} & T_{\varepsilon f} \\ T_{\tilde{\varepsilon} \tilde{d}} & T_{\tilde{\varepsilon} f}\end{array}\right]$. Then, a sufficient condition for RPP is

$$
\begin{aligned}
\mathrm{RPP} & \Leftrightarrow\left\|T_{\varepsilon \tilde{d}}\right\|_{\infty}<1,\left\|T_{\tilde{\varepsilon} f}\right\|_{\infty}<1, \quad \forall \Delta \in \Delta:\|\Delta\|_{\infty} \leq 1 \\
& \Leftarrow \mu_{\hat{\Delta}_{\mathrm{part}}}(N(j \omega))<1, \quad \forall \omega
\end{aligned}
$$

where $\mu$ is computed with respect to the structure

\[
\hat{\Delta}_{\mathrm{part}}=\left[\begin{array}{cc}
\Delta & 0  \tag{4.11}\\
0 & \Delta_{P_{\mathrm{part}}}
\end{array}\right]
\]

where the performance block $\Delta_{P_{\text {part }}}$ is structured as

$$
\Delta_{P_{\mathrm{part}}}=\left[\begin{array}{cc}
\Delta_{d} & 0 \\
0 & \Delta_{f}
\end{array}\right]
$$

in which $\Delta_{d}=\mathbb{C}^{n_{d} \times n_{\varepsilon}}$ and $\Delta_{f}=\mathbb{C}^{n_{f} \times n_{\varepsilon}}$.\\
Proof. The proof is provided in Appendix 4.B.\\
Remark 4.20. The condition in Theorem 4.19 is sufficient since next to $\left\|T_{\varepsilon \tilde{d}}\right\|_{\infty}<1$ and $\left\|T_{\tilde{\varepsilon} f}\right\|_{\infty}<1$, the condition implies that $\left\|\mathcal{F}_{u}\left(T, \Delta_{d}\right)\right\|_{\infty}<1$ for all $\Delta_{d}$, and $\left\|\mathcal{F}_{l}\left(T, \Delta_{f}\right)\right\|_{\infty}<1$ for all $\Delta_{f}$ due to the interaction between the $\Delta_{d}$ and $\Delta_{f}$ loops, see the proof for details. Yet, considering the block-diagonal $\Delta_{P_{\text {part }}}$ is significantly less stringent with respect to the full $\Delta_{p}$ as in Theorem 4.18.

Now that a robust performance test is described with a structured performance block, it is not yet clear how to integrate this for robust fault detection filter synthesis. To this end, the system is reformulated once again such that it is directly implementable in standard $\mu$-synthesis tools using $D K$-iterations.

\subsection*{4.5.2 $\mu$-synthesis with structured performance}
The structured singular value is not only powerful for analysis, but is also used to find a residual generator $Q$ that minimizes a $\mu$-condition via $D K$-iterations [233], see Section 4.2. Since standard $\mu$-tools for DK-iterations do not allow to structure the performance block $\Delta_{p}$, the main idea is include the partitioned performance block into the structured uncertainty and create a void channel related to the full complex perturbation. To this end, consider the following theorem.

Theorem 4.21. Consider the system $N=\mathcal{F}_{l}(\bar{P}, Q)$, where $N$ is internally stable. Then,


\begin{equation*}
\mu_{\hat{\Delta}_{\text {part }}}(N(j \omega))=\mu_{\hat{\Delta}_{\text {part-void }}}\left(N^{\star}(j \omega)\right), \tag{4.12}
\end{equation*}


where

\[
N^{\star}(j \omega)=\left[\begin{array}{cc}
N(j \omega) & 0  \tag{4.13}\\
0 & 0
\end{array}\right]
\]

the related block structure for $\mu_{\hat{\Delta}_{\text {part-void }}}$ is

\[
\hat{\Delta}_{\text {part-void }}=\left[\begin{array}{ccc}
\Delta & 0 & 0  \tag{4.14}\\
0 & \Delta_{P_{\text {part }}} & 0 \\
0 & 0 & \Delta_{P_{\text {void }}}
\end{array}\right],
\]

and $\Delta_{P_{\text {void }}}$ is a single complex perturbation block, i.e., $\Delta_{P_{\text {void }}}=\mathbb{C}$, see Figure 4.6.

Proof. The proof is provided in Appendix 4.C.

This theorem proves equivalence of between $\mu_{\hat{\Delta}_{\text {part-void }}}\left(N^{\star}(j \omega)\right)$ and $\mu_{\hat{\Delta}_{\text {part }}}(N(j \omega))$ as used in Theorem 4.19. Therefore, adding the void channels and $\Delta_{P_{\text {void }}}$ does not influence the result of the synthesis procedure.

Remark 4.22. The commonly used function musyn.m does not allow to structure the performance block as required for the approach in Section 4.5.1, but assumes a full complex perturbation $\Delta_{p}$. Manipulation of the generalized plant as described in Theorem 4.21 allows for direct implementation in musyn.m.

The approach in Theorem 4.19, considering a block diagonal performance block, mitigates the direct minimization of the off-diagonal entries of $T$. However, due to the remaining indirect crosstalk, it only provides a sufficient condition. To completely mitigate this crosstalk and achieve a necessary and sufficient condition, fixed structure synthesis approaches offer a solution, which will be examined next.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-109}
\captionsetup{labelformat=empty}
\caption{Figure 4.6: Schematic representation to implement a structured performance block in standard $\mu$-synthesis tooling.}
\end{center}
\end{figure}

\subsection*{4.6 Robust performance via fixed structure synthesis}
To focus solely on the diagonal entries of $T$ in the design of a fault diagnosis filter, fixed structure synthesis methods are suitable [5,53,125]. This approach utilizes specialized nonsmooth optimization techniques to minimize the $\mathcal{H}_{\infty}$ norm as a function of the tunable parameters.

The main idea is to isolate the diagonal entries of $T$, stack these in a block diagonal to avoid interaction, and synthesize a joint fault detection filter. To this end, let $\hat{P}=\mathcal{F}_{u}(\bar{P}, \Delta):\left[\begin{array}{lll}\tilde{d} & f & y_{Q}\end{array}\right]^{\top} \mapsto\left[\begin{array}{lll}\varepsilon & \tilde{\varepsilon} & u_{Q}\end{array}\right]^{\top}$, which is partitioned as

$$
\hat{P}=\left[\begin{array}{ccc}
\hat{P}_{11} & \hat{P}_{12} & \hat{P}_{13} \\
\hat{P}_{21} & \hat{P}_{22} & \hat{P}_{23} \\
\hat{P}_{31} & \hat{P}_{32} & \hat{P}_{33}
\end{array}\right]
$$

Isolating the channels $\hat{P}_{1}:\left[\begin{array}{cc}\tilde{d} & y_{Q}\end{array}\right]^{\top} \mapsto\left[\begin{array}{ll}\varepsilon & u_{Q}\end{array}\right]^{\top}$ and $\hat{P}_{2}:\left[\begin{array}{ll}f & y_{Q}\end{array}\right]^{\top} \mapsto \left[\begin{array}{ll}\tilde{\varepsilon} & u_{Q}\end{array}\right]^{\top}$ via

$$
\hat{P}_{1}=\left[\begin{array}{cc}
\hat{P}_{11} & \hat{P}_{13} \\
\hat{P}_{31} & \hat{P}_{33}
\end{array}\right], \quad \hat{P}_{2}=\left[\begin{array}{cc}
\hat{P}_{22} & \hat{P}_{23} \\
\hat{P}_{32} & \hat{P}_{33}
\end{array}\right],
$$

enables to write the diagonal entries of $T$ as

$$
T_{\varepsilon \tilde{d}}=\mathcal{F}_{l}\left(\hat{P}_{1}, Q\right), \quad T_{\tilde{\varepsilon} f}=\mathcal{F}_{l}\left(\hat{P}_{2}, Q\right) .
$$

These are combined in a block-diagonal plant $\check{P}=\hat{P}_{y}$ blkdiag $\left(\hat{P}_{1}, \hat{P}_{2}\right) \hat{P}_{u}$. The inputs and outputs of blkdiag $\left(\hat{P}_{1}, \hat{P}_{2}\right)$ are reordered using the following $\hat{P}_{u}$ and\\
$\hat{P}_{y}$ in order to allow for a joint linear fractional representation (LFR).

$$
\hat{P}_{u}=\left[\begin{array}{cccc}
I & 0 & 0 & 0 \\
0 & 0 & I & 0 \\
0 & I & 0 & 0 \\
0 & 0 & 0 & I
\end{array}\right], \quad \hat{P}_{y}=\left[\begin{array}{cccc}
I & 0 & 0 & 0 \\
0 & 0 & I & 0 \\
0 & I & 0 & 0 \\
0 & 0 & 0 & I
\end{array}\right]
$$

The combined uncertainty is obtained from $\operatorname{blkdiag}(\Delta, \Delta)$ and reordered such that $\check{\Delta}=\operatorname{diag}\left(p_{1} \operatorname{diag}(I, I), \ldots, p_{n_{r}} \operatorname{diag}(I, I), \delta_{1} \operatorname{diag}(I, I), \ldots, \delta_{n_{c}} \operatorname{diag}(I, I)\right.$, $\left.\Delta_{1} \operatorname{diag}(I, I), \ldots, \Delta_{n_{z}} \operatorname{diag}(I, I)\right)$. Finally, a block-diagonal joint residual generator $\check{Q}=\operatorname{blkdiag}(Q, Q)$ is considered to complete the components of the LFR. Hence, the joint LFR is given by $\check{T}=\mathcal{F}_{l}\left(\mathcal{F}_{u}(\check{P}, \check{\Delta}), \check{Q}\right)=\left[\begin{array}{cc}T_{\varepsilon \tilde{d}} & 0 \\ 0 & T_{\tilde{\varepsilon} f}\end{array}\right]$.

Remark 4.23. The fixed structure refers to the structure of the to be synthesized filter $\check{Q}$. The structure is fixed to be block-diagonal with two equal blocks $Q$. Additionally, the order of the filter $Q$ is user-defined.

Since the diagonal entries are now isolated, application of the robust performance Theorem 4.18 directly gives RPP as follows.

Theorem 4.24. Consider the uncertain system described by $\check{N}=\mathcal{F}_{l}(\check{P}, \check{Q})$ with uncertainty $\Delta$. Assume nominal stability such that $\check{N}$ is (internally) stable. Then,

$$
\begin{aligned}
\mathrm{RPP} & \Leftrightarrow\left\|T_{\varepsilon \tilde{d}}\right\|_{\infty}<1,\left\|T_{\tilde{\varepsilon} f}\right\|_{\infty}<1, \quad \forall \Delta \in \boldsymbol{\Delta}:\|\Delta\|_{\infty} \leq 1 \\
& \Leftrightarrow\left\|\mathcal{F}_{u}(\check{N}, \check{\Delta})\right\|_{\infty}<1, \quad \forall \Delta \in \boldsymbol{\Delta}:\|\Delta\|_{\infty} \leq 1 \\
& \Leftrightarrow \mu_{\hat{\Delta}}(\check{N}(j \omega))<1, \quad \forall \omega
\end{aligned}
$$

where $\mu$ is computed with respect to the structure

\[
\hat{\Delta}=\left[\begin{array}{cc}
\check{\Delta} & 0  \tag{4.15}\\
0 & \Delta_{P}
\end{array}\right]
\]

and $\Delta_{P}$ is a full complex pertiurbation with the same dimensions as $\mathcal{F}_{u}(\check{N}, \check{\Delta})^{\top}$.\\[0pt]
Proof. The proof is provided in [233, Chapter 8], where it additionally is used that due to the diagonal structure of $\check{T}$,

$$
\begin{aligned}
\|\check{T}\|_{\infty}<1, \forall \Delta \in \Delta:\|\Delta\|_{\infty} \leq 1 & \\
& \Leftrightarrow \\
\left\|T_{\varepsilon \tilde{d}}\right\|_{\infty}<1,\left\|T_{\tilde{\varepsilon} f}\right\|_{\infty} & <1, \forall \Delta \in \Delta:\|\Delta\|_{\infty} \leq 1 .
\end{aligned}
$$

Hence, a necessary and sufficient condition is obtained that solely impacts $T_{\varepsilon \tilde{d}}$ and $T_{\tilde{\varepsilon} f}$. This $\mu$-condition is used in combination with non-smooth $\mathcal{H}_{\infty}$ synthesis to achieve an optimal fault detection filter in view of the fictitious $\mathcal{H}_{\infty}$ formulation. The following remarks elaborate upon the initialization of the non-smooth optimization.

Remark 4.25. The fixed-structure synthesis algorithm implemented in musyn.m uses hinfstruct.m [5] and is guaranteed to converge to a critical point. Unfortunately, this is typically a local minimum due to the non-convexity of the underlying problem. Hence, there is no guarantee to reach the global optimum.

Remark 4.26. Since the fixed structure synthesis typically terminates in a local optimum, it is paramount to start the algorithm with an appropriate initial condition. To this end, the solution to the robust performance synthesis method from Section 4.5 is a suitable candidate. To use this $Q$, its order has to be matched with the desired order of the fixed-structure $\check{Q}$. A Gramian-based balanced statespace realization is advised to truncate the states corresponding to the smallest Hankel singular values. Alternatively, the optimization can be started from multiple initial conditions as the fixed-structure synthesis method lends itself for parallel computing.

Since the fault detection filter $Q$ is computed by means of $D K$-iterations, i.e., in an $\mathcal{H}_{\infty}$-framework, a $\gamma>1$ does not necessarily mean that the filter fails the original $\mathcal{H}_{-} / \mathcal{H}_{\infty}$-requirements R1 and R2. To verify the true performance in view of the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ specifications, $\mu_{g}$-analysis is required, which is introduced in the next section.

\subsection*{4.7 Robust performance in view of $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ conditions}
Since the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ framework is recasted to an $\mathcal{H}_{\infty}$ framework by means of the fictitious weighting filter $W_{F}$, see Section 4.4, conservatism is introduced. To analyze robust performance in view of the original $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ conditions with respect to the original weightings $W_{d}$ and $W_{f}$, a generalization of the structured singular value $\mu_{g}$ is introduced. This generalization takes the min-max nature of the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ framework into account and provides a frequency-based test for robust performance, i.e., joint robust fault sensitivity and robust disturbance attenuation.

Definition 4.27. (Generalized structured singular value [178]) Consider the structured $\Delta=\operatorname{blkdiag}\left(\Delta_{J}, \Delta_{K}\right)$, where $\Delta_{J}=\left\{\operatorname{blkdiag}\left(\ldots, \Delta_{j}, \ldots\right) \mid j \in J\right\}$ and $\Delta_{K}=\left\{\operatorname{blkdiag}\left(\ldots, \Delta_{k}, \ldots\right) \mid k \in K\right\}$. Note that the index set $I$, compared to Definition 4.2, has now been partitioned into $I=(J, K)$. Consider $z=M x$

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-112}
\captionsetup{labelformat=empty}
\caption{Figure 4.7: Weighted system used for the \$\textbackslash mu\_\{g}\$ robust performance test. The controller $C$ and residual generator $Q$ are collapsed into the generalized plant $P$. The performance channel $\varepsilon$ is duplicated since channel is used to analyze the maximum gain part and one channel is used to analyze minimum gain part in the $\mu_{g}$ test.\}\end{center}
\end{figure}

and $x=\Delta z$, where $z, x$, and the matrix $M$ are partitioned in accordance with $\Delta$, as

\[
M=\left[\begin{array}{ll}
M_{J J} & M_{J K}  \tag{4.16}\\
M_{K J} & M_{K K}
\end{array}\right] .
\]

The positive function $\mu_{g}(M)$ is defined on a domain $M \in \operatorname{dom}\left(\mu_{g}\right)$ by

\[
\mu_{g}(M):=\max _{\|x\|=1}\left\{\begin{array}{l|l}
\gamma & \begin{array}{l}
\left\|x_{j}\right\| \gamma \leq\left\|z_{j}\right\|, \forall j \in J \\
\left\|z_{k}\right\| \gamma \leq\left\|x_{k}\right\|, \forall k \in K
\end{array} \tag{4.17}
\end{array}\right\} .
\]

Unlike $\mu, \mu_{g}$ is not well defined for all complex valued matrices $M$. The domain of definition of $\mu_{g}$, denoted by $\operatorname{dom}\left(\mu_{g}\right)$, is given by

$$
M \in \operatorname{dom}\left(\mu_{g}\right) \quad \text { iff } M_{K K} x_{K}=0 \Rightarrow x_{K}=0 .
$$

Remark 4.28. Note that $M \in \operatorname{dom}\left(\mu_{g}\right)$ implies that $\operatorname{dim}\left(z_{k}\right) \geq \operatorname{dim}\left(x_{k}\right)$. In case $\operatorname{dim}\left(z_{k}\right)=\operatorname{dim}\left(x_{k}\right), M \in \operatorname{dom}\left(\mu_{g}\right)$ is equivalent to $M_{K K}$ being invertible.

In the general case, a precise upper bound of the generalized structured singular value can be computed with convex optimization as described in [178]. In the specific case where $M_{K K}$ is square and $M \in \operatorname{dom}\left(\mu_{g}\right)$, i.e., $M K_{K K}$ is full rank, the generalized structured singular value $\mu_{g}$ of a matrix $M$ can be reformulated as the structured singular value $\mu$ of a different matrix $\tilde{M}$.

Theorem 4.29. If $M \in \operatorname{dom}\left(\mu_{g}\right)$ and $M_{K K}$ is square, then


\begin{equation*}
\mu_{g}(M)=\mu(\tilde{M}), \tag{4.18}
\end{equation*}


where

\[
\tilde{M}=\left[\begin{array}{cc}
M_{J J}-M_{J K} M_{K K}^{-1} M_{K J} & M_{J K} M_{K K}^{-1}  \tag{4.19}\\
-M_{K K}^{-1} M_{K J} & M_{K K}^{-1}
\end{array}\right],
\]

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-113}
\captionsetup{labelformat=empty}
\caption{Figure 4.8: Closed-loop for robust performance analysis using \$\textbackslash mu\_\{g}\$. The uncertainty $\Delta_{J}$ relates to the $\mathcal{H}_{\infty}$ part of the check and the uncertainty $\Delta_{K}$ relates to the $\mathcal{H}_{-}$part.\}\end{center}
\end{figure}

and the block structure for the $\mu(\tilde{M})$ problem is


\begin{equation*}
\Delta=\operatorname{diag}\left(\Delta_{J}, \Delta_{K}^{\top}\right) . \tag{4.20}
\end{equation*}


Proof. The proof is provided in [178, Section V]. \(\square\)

Remark 4.30. In case $M_{K K}$ is not square, an alternative to the convex optimization approach in [178], a subset of the channels can be analyzed for which $M_{K K}$ is square. By analyzing different subsets robust performance can analyzed in view of the original $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ conditions.

Theorem 4.29 is applicable to analyze the robust performance of fault detection filters. To this end, the weighted plant in Figure 4.7 is considered. Here, the performance channel $\varepsilon$ is duplicated for the $\mu_{g}$ test.

For this analysis, consider Figure 4.8, where $M$ in (4.16), is structured using $\tilde{P}$ in (4.8) as

$$
\begin{gathered}
M_{J J}=\left[\begin{array}{cc}
\tilde{P}_{11} & \tilde{P}_{12} W_{d}^{-1} \\
Q_{y} \tilde{P}_{11}+Q_{u} \tilde{P}_{21} & \left(Q_{y} \tilde{P}_{12}+Q_{u} \tilde{P}_{22}\right) W_{d}^{-1}
\end{array}\right], \\
M_{J K}=\left[\begin{array}{c}
\tilde{P}_{13} W_{f}^{-1} \\
\left(Q_{y} \tilde{P}_{13}+Q_{u} \tilde{P}_{23}\right) W_{f}^{-1}
\end{array}\right], \\
M_{K J}=\left[\begin{array}{cc}
Q_{y} \tilde{P}_{11}+Q_{u} \tilde{P}_{21} & \left(Q_{y} \tilde{P}_{12}+Q_{u} \tilde{P}_{22}\right) W_{d}^{-1}
\end{array}\right],
\end{gathered}
$$

and

$$
M_{K K}=\left(Q_{y} \tilde{P}_{13}+Q_{u} \tilde{P}_{23}\right) W_{f}^{-1}
$$

The uncertainty blocks $\Delta_{J}$ and $\Delta_{K}$ are structured as

$$
\Delta_{J}=\left[\begin{array}{cc}
\Delta & 0 \\
0 & \Delta_{d}
\end{array}\right], \quad \Delta_{K}=\Delta_{f}
$$

First, R1 and R2 are revisited. In the proof of Theorem 4.19, it is shown that $\left\|T_{\varepsilon d} W_{d}^{-1}\right\|_{\infty}<1$ implies $\left\|T_{\varepsilon d}\right\|_{\infty}<\left\|W_{d}\right\|_{\infty}$ and thus implies requirement R1. In a similar fashion it can be shown that $\left\|T_{\varepsilon f} W_{f}^{-1}\right\|_{-}>1$ implies $\left\|T_{\varepsilon f}\right\|_{-}>\left\|W_{f}\right\|_{-}$ and thus implies requirement R2.

Consider the following theorem in view of these requirements.\\
Theorem 4.31. Consider $M$ as depicted in Figure 4.8 and partitioned according to (4.16). Let $\sup _{\omega} \mu_{\Delta_{J}}\left(M_{J J}(j \omega)\right)<1$ such that $\mathcal{F}_{u}\left(M_{J J}, \Delta_{J}\right)$ is well defined for all $\Delta \in \boldsymbol{\Delta}$, and let $M \in \operatorname{dom}\left(\mu_{g}\right)$. Then, a necessary and sufficient condition for

\begin{itemize}
  \item $\sup _{\omega} \bar{\sigma}\left(T_{\varepsilon d}(j \omega) W_{d}^{-1}(j \omega)\right)<1$ for all $\Delta \in \boldsymbol{\Delta}:\|\Delta\|_{\infty} \leq 1$
  \item $\inf _{\omega \in \Omega} \underline{\sigma}\left(T_{\varepsilon f}(j \omega) W_{f}^{-1}(j \omega)\right)>1$ for all $\Delta \in \boldsymbol{\Delta}:\|\Delta\|_{\infty} \leq 1$ in the frequency range $\Omega=\left[\omega_{1}, \omega_{2}\right]$,\\
to hold, is that
\end{itemize}

$$
\mu_{g, \tilde{\Delta}}(M(j \omega))<1 \quad \forall \omega \in \Omega
$$

where $\mu_{g}$ is computed with respect to the structure

$$
\tilde{\Delta}=\left[\begin{array}{cc}
\Delta_{J} & 0 \\
0 & \Delta_{K}
\end{array}\right]
$$

Proof. The proof is provided in Appendix 4.D.\\
This concludes the $\mu_{g}$-analysis for a frequency-based robust performance check. Next, a robust fault detection filter is designed for a next-generation prototype wafer stage and its performance is analyzed using the structured singular value $\mu_{g}$.

\subsection*{4.8 Experimental results on a prototype wafer stage}
In this section, the presented method is applied to detect faults in a prototype wafer stage, which is part of a lithography system used in the semiconductor

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-115}
\captionsetup{labelformat=empty}
\caption{Figure 4.9: Schematic drawing of the prototype wafer stage. The four inputs \$u\_\{i}\$ and outputs $y_{i}$, located at the corners of the stage $i=1, \ldots, 4$, are depicted. In addition, the location of two faults $f_{1}$ and $f_{2}$ is shown. In this image, the chuck (a) and force frame (b) are taken out of the machine. During operation, these are positioned in between the granite base frame (c) and the granite metro frame (d).\}\end{center}
\end{figure}

industry, see Figure 4.9. First, the setup is introduced, followed by a brief description of the system identification procedure used to obtain the uncertain plant model. Next, a robust fault detection filter is synthesized using three approaches: $\mu$ synthesis, $\mu$ synthesis with a partially filled performance block, and structured $\mu$ synthesis. The $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ robust performance is then verified using $\mu_{g}$ analysis. Finally, the time-domain response of the residual signal is presented.

\subsection*{4.8.1 System setup}
Consider the schematic overview of the wafer stage in Figure 4.9. The stage is actively controlled in six degrees of freedom (DOFs) by 17 actuators, with its position measured by seven sensors. For validating the proposed fault detection approach, only the out-of-plane $z$-direction is considered, while the other DOFs, three rotations and the horizontal plane position, are regulated to zero by decentralized controllers. Four actuators are positioned at the corners of the chuck, denoted by $u_{i}$, with $i=(1, \ldots, 4)$. The position of the chuck is measured by four sensors, $y_{i}$ with $i=(1, \ldots, 4)$. It is presumed that the interaction between the regulated DOFs and the $z$-direction is minimal.

The rigid body in the $z$-direction is controlled by a robustly stabilizing feedback controller $C_{\mathrm{fb}}: e_{\mathrm{rb}} \mapsto u_{\mathrm{rb}}$, and follows a setpoint $r$, where $e_{\mathrm{rb}}=r-z_{\mathrm{rb}}$. The rigid body displacement $z_{\mathrm{rb}}=T_{y} z$, with $z=\left[\begin{array}{llll}y_{1} & y_{2} & y_{3} & y_{4}\end{array}\right]^{\top}$ and $T_{y}=\frac{1}{4}\left[\begin{array}{llll}1 & 1 & 1 & 1\end{array}\right]$. The controller distributes the input over the four ac-

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-116}
\captionsetup{labelformat=empty}
\caption{Figure 4.10: Block diagram of the prototype wafer stage for fault detection filter synthesis. The uncertain system is described by \$\textbackslash mathcal\{F}\_\{u\}(G, \textbackslash Delta)\$ and controlled by $C_{\mathrm{fb}}$. To this end, the outputs and inputs related to the corners of the chuck are transformed by $T_{y}$ and $T_{u}$. The fault detection filter $Q$ is designed by weighting the generalized inputs by $W_{d}^{-1}$ and $W_{F}$, and minimizing the transfer functions $T_{\tilde{d} \varepsilon}$ and $T_{f \tilde{\varepsilon}}$. The filter $G_{f}$ maps the fault $f$ to the specific location where the faults are present in the control loop.\}\end{center}
\end{figure}

tuators via $u=T_{u} u_{\mathrm{rb}}$, where $T_{u}=T_{y}^{\top}$. The corresponding block diagram is depicted in Figure 4.10.

The system is susceptible to two types of faults, highlighted in Figure 4.9. The first fault is an actuator fault at $u_{3}$, with an additional force superimposed to resemble the fault. The second fault resembles an external anomalous force applied between actuators $u_{2}$ and $u_{3}$. An additional actuator positioned underneath the chuck between the second and third inputs is used to apply this fault.

The objective is to synthesize a fault detection filter $Q \in \mathcal{R} \mathcal{H}_{\infty}^{n_{\varepsilon} \times\left(n_{y}+n_{u}\right)}$ that meets the requirements in Problem 4.12. Here, the goal is to obtain a single detection signal, i.e., $n_{\varepsilon}=1$, with the inputs of the residual generator being of dimension $n_{y}+n_{u}=8$. The disturbance attenuation factor is set to $\gamma_{1}=1$, and the fault sensitivity measure is $\gamma_{2}=10$ in the lower frequency range $\Omega=[0.0025,1] \mathrm{Hz}$.

Remark 4.32. If the objective is to detect and isolate faults, the approach can be easily extended by setting $n_{\varepsilon}=2$. In this case, the first row of $Q$ is synthesized with the second fault treated as a disturbance to attenuate its effect, and vice versa for the second row of $Q$. The resulting filter yields two residuals where $\varepsilon_{1}$\\
is sensitive to $f_{1}$, and $\varepsilon_{2}$ is sensitive to $f_{2}$.

\subsection*{4.8.2 System identification}
A closed-loop multisine identification experiment has been performed to obtain a best linear approximation (BLA) of the system using the robust method [202]. This frequency response function (FRF), denoted by $\hat{G}_{\text {bla }}$, is used to fit a modal parametric modal model of order 20 . To this end, a novel modal identification algorithm is used, based on the simplified refined instrumental variable method (SRIVC) and integrated prediction error minimization (IPEM). The algorithm, including a detailed analysis, will be published elsewhere. This algorithm is a frequency domain and MIMO extension of [112]. The obtained nominal model is denoted by $G_{0}$, see [46] for details.

To obtain an uncertain representation of the wafer stage, consider the linear fractional representation

$$
G_{u}=\mathcal{F}_{u}(G, \Delta)
$$

with

$$
G=\left[\begin{array}{cc}
0 & I \\
\alpha & G_{0}
\end{array}\right]
$$

where $\Delta \in \hat{\boldsymbol{\Delta}}$. The set $\hat{\boldsymbol{\Delta}}=\left\{\Delta \in \mathcal{R} \mathcal{H}_{\infty} \mid\|\Delta\|_{\infty} \leq 1\right\}$, and the parameter $\alpha$ is computed by $\alpha=\left\|\hat{G}_{\text {bla }}-G_{0}\right\|_{\infty}$. The resulting uncertain plant $G_{u}$ together with the frequency response data $\hat{G}_{\text {bla }}$ and nominal model $G_{0}$ are shown in Figure 4.11.

Since the faults are artificially applied by two actuators, the transfer functions from the fault to the sensors can be identified. This model describes how the faults relate to the output and is denoted by $G_{f}(s)$. This model is identified with the same procedure as the plant itself, and is depicted in Figure 4.12.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-118}
\captionsetup{labelformat=empty}
\caption{Figure 4.11: Bode magnitude plots of the BLA in (-), measured using multisine excitation and the robust method, the \$20\^{}\{\textbackslash text \{th }\}\$-order nominal plant $G_{0}(-)$, obtained using the frequency domain simplified refined instrumental variable (SRIVC) method with integrated prediction error minimization (IPEM), and the uncertain plant $G_{u}(\Delta)$ in ( $\square$ ).\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-119}
\captionsetup{labelformat=empty}
\caption{Figure 4.12: Parameteric fault model \$G\_\{f}(s)\$ obtained through mutltisine injection and frequency domain simplified refined instrumental variable (SRIVC) method with integrated prediction error minimization (IPEM).\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-120}
\captionsetup{labelformat=empty}
\caption{Figure 4.13: Weighting functions used for fault detection filter synthesis. The weighting filter related to the output disturbance \$W\_\{d\_\{y}\}\^{}\{-1\}\$ is shown (-), as well as the weighting filter related to the setpoint $W_{r}^{-1}(-)$. The weighting filter $W_{f}$ is shown in (-) such that the fault sensitivty requirement is met in the frequency range $\Omega$. This range is highlighted by the vertical lines (---). The Weighting filter $W_{F}$ (一) is slightly above $W_{f}$.\}\end{center}
\end{figure}

\subsection*{4.8.3 Fault detection filter synthesis}
First, the remaining weighting filters in the block diagram of Figure 4.10 are introduced. Subsequently, a fault detection filter is synthesised with three different approaches. Thereafter, robust performance is verified using $\mu_{g}$-analysis. Consider an invertible weighting filter related to the generalized disturbances $W_{d}$ that is structured as

$$
W_{d}=\left[\begin{array}{cc}
W_{r} & 0 \\
0 & W_{d_{y}}
\end{array}\right],
$$

where $W_{r}$ relates to the setpoint and $W_{d_{y}}$ relates to output disturbances. Since the setpoint is typically known, $W_{r}$ is shaped to according to the frequency content of $r$. Based on the frequency content of $r$, the filter $W_{r}$ is shaped and its inverse is shown in Figure 4.13. It is assumed that the output disturbances are mainly present at the higher frequencies to which $W_{d}$ is shaped. Figure 4.13 shows the inverse of $W_{d_{y_{i}}}$ which relates to $W_{d}$ as $W_{d}=W_{d_{y_{i}}} I_{4 \times 4}$. The filter $W_{f}$ is shaped based on $\gamma_{2}=10$ in the lower frequency range $\Omega=[0.0025,1] \mathrm{Hz}$. The filter $W_{F}$ follows from Theorem 4.15.

Using the standard $\mu$-synthesis approach based on Theorem 4.18, including all entries of (4.9) resulting from the full complex $\Delta_{P}$, an optimal structured singular value $\mu_{\text {opt }}=11.06$ is achieved. The corresponding residual filter is referred to as $Q_{\mu}$. Based on this $\mu_{\text {opt }}$, no conclusions can be drawn regarding to

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-121}
\captionsetup{labelformat=empty}
\caption{Figure 4.14: Generalized structured singular value \$\textbackslash mu\_\{g, \textbackslash tilde\{\textbackslash Delta}\}\textbackslash left(M\_\{f\_\{i\}\}(j \textbackslash omega)\textbackslash right)\$ using the fault detection filter $Q_{\mu, \text { struc }}$. The lower bounds related to $M_{f_{1}}$ and $M_{f_{2}}$ are shown in (---) and (---). The upper bounds related to $M_{f_{1}}$ and $M_{f_{2}}$ are shown in (一) and (一). Since the upper bounds of the structured singular values are below $\gamma=1$ in the frequency range $\Omega$, the user-defined maximum disturbance sensitivity is satisfied.\}\end{center}
\end{figure}

achieving the performance specifications in $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ sense.\\
Application of the synthesis approach in Section 4.5 with a partially filled performance block $\Delta_{P_{\text {part }}}$, the optimal structured singular value $\mu_{\text {opt }, \text { part }}=0.99$ is found. This guarantees that $\left\|T_{\varepsilon \tilde{d}}\right\|_{\infty}<1$ and $\left\|T_{\tilde{\varepsilon} f}\right\|_{\infty}<1$ such that the requirements R1 and R2 are satisfied in line with Theorem 4.15. The corresponding residual filter is denoted by $Q_{\mu, \text { part }}$.

Finally, the structured $\mu$-synthesis approach from Section 4.6 is applied. The optimal structured singular value $\mu_{\text {opt }, \text { struc }}=0.17$ is found. This guarantees that $\left\|T_{\varepsilon \tilde{d}}\right\|_{\infty}<0.17$ and $\left\|T_{\tilde{\varepsilon} f}\right\|_{\infty}<0.17$ which implies that the requirements R1 and R2 are satisfied with significant margin. The corresponding residual filter is denoted by $Q_{\mu, \text { struc }}$, and turns out to be equal to $Q_{\mu, \text { part }}$. It is concluded that the optimization algorithm converged to the same critical point, however, $\mu_{\text {struc,opt }}< \mu_{\text {part,opt }}$ since the structured approach solely accounts for the diagonal entries of $T$ in (4.9), whereas the partial approach has a small but significant weighting due to crosstalk on the off-diagonals of $T$ in (4.9).

Next, robust performance of $Q_{\mu, \text { struc }}$, which equals $Q_{\mu, \text { part }}$, is analyzed in view of the original $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ conditions with respect to the original weightings $W_{d}$ and $W_{f}$. This analysis is performed using the generalization of the structured singular value $\mu_{g}$ as introduced in Section 4.7. Since the fault detection filter yields one residual for two faults, $M_{K K}$ in (4.16) is not square. Instead of computing the generalized structured singular value using convex optimization, two subsets are considered with $M_{K K}$ square, see Remark 4.30. First, the fault sensitivity and disturbance attenuation are computed with respect to $f_{1}$, and subsequently with respect to $f_{2}$, yielding two subsets that render $M_{K K}$ square.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-122}
\captionsetup{labelformat=empty}
\caption{Figure 4.15: Time response of the residual signal $\varepsilon$ in (-) together with the detection threshold $|\beta|=0.25$ shown in (一). In the bottom plot, the applied reference $r$ is shown in (-) together with the applied faults \$f\_\{1}\$ in (-) and $f_{2}$ in ( $\cdots$ ). Indeed, the residual remains within the bound if no fault is acting on the system and exceed the user-defined $\beta$-bounds when a fault is present in the system.\}\end{center}
\end{figure}

The corresponding matrices $M$, see Figure 4.8, are denoted by $M_{f_{1}}$ and $M_{f_{2}}$. For both cases $i=1,2$, lower and upper bounds of $\mu_{g, \tilde{\Delta}}\left(M_{f_{i}}(j \omega)\right)$ are computed [178]. These bounds are depicted in Figure 4.14. Clearly, the generalized structured singular values are below 1 within the frequency region $\Omega$, from which is concluded that the original $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ requirements in Problem 4.12 are met with significant margin.

\subsection*{4.8.4 Experimental time-domain results}
An experiment is performed on the prototype wafer stage to verify the obtained fault detection filter $Q_{\mu, \text { struc }}$. The reference and fault signals that are applied to the system are depicted at the bottom of Figure 4.15. The reference is a smooth $4^{\text {th }}$ order setpoint with a base frequency of 1 Hz and an amplitude of $1 \mathrm{e}-4 \mathrm{~m}$, starting from $T=5 \mathrm{~s}$. After $T=10 \mathrm{~s}$ the first fault $f_{1}$ is injected with a duration of 10 s , which is a square wave with an amplitude of 0.5 N and a frequency of 0.5 Hz . The second fault $f_{2}$ is injected at $T=30 \mathrm{~s}$, and remains present for 5 s which is a constant force of 0.1 N . There are no additional disturbances injected to the system.

The top plot in Figure 4.15 shows the residual generated by the fault detection filter $Q_{\mu, \text { struc }}$. The effect of the disturbance and setpoint are clearly suppressed and the fault is easily distinguished in the the residual signals. Suppressing the setpoint further is complicated since it acts in the same frequency as the faults $f_{1}$ and $f_{2}$. Yet, a suitable compromise between setpoint rejection and fault attenuation is achieved. Deriving time-domain bounds for residual evaluation is beyond the scope of this chapter, however, based on the signal between $t=5 \mathrm{~s}$ and $t=10 \mathrm{~s}$, a conservative threshold for fault detection is selected as $|\beta|=0.25$. This enables clear distinction between a faulty situation compared to a fault-free situation, regardless of the effect of model uncertainty and disturbances acting on the system.

\subsection*{4.9 Conclusion}
This study highlights a model-based fault diagnosis approach that offers a solution to balance fault sensitivity and disturbance rejection, despite inherent modeling uncertainties. The research introduces a novel formulation of the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ problem within an $\mathcal{H}_{\infty}$ framework using a structured complex perturbation for robust performance analysis and synthesis, significantly reducing conservatism. This solution is used as initial condition for fixed-structure synthesis, to reduce conservatism further. The approach is validated on a next-generation prototype wafer stage, demonstrating its practical applicability and potential to advance fault detection in complex industrial systems.

\section*{Appendices}
\section*{4.A Proof of theorem 4.15}
In this appendix, the proof of Theorem 4.15 is provided.\\
Proof. First, consider $\left\|T_{\varepsilon d} W_{d}^{-1}\right\|_{\infty}<1$ for disturbance suppression. This can be written as

$$
\bar{\sigma}\left(T_{\varepsilon d} W_{d}^{-1}\right)<1 \quad \forall \omega
$$

Multiplication of both sides with $\bar{\sigma}\left(W_{d}\right)$ gives

$$
\bar{\sigma}\left(T_{\varepsilon d} W_{d}^{-1}\right) \bar{\sigma}\left(W_{d}\right)<\bar{\sigma}\left(W_{d}\right) \quad \forall \omega .
$$

Considering that $\bar{\sigma}\left(T_{\varepsilon d} W_{d}^{-1} W_{d}\right) \leq \bar{\sigma}\left(T_{\varepsilon d} W_{d}^{-1}\right) \bar{\sigma}\left(W_{d}\right)$, gives

$$
\bar{\sigma}\left(T_{\varepsilon d}\right)<\bar{\sigma}\left(W_{d}\right) \quad \forall \omega
$$

$\left\|W_{d}\right\|_{\infty} \leq \gamma_{1}$, i.e., $\bar{\sigma}\left(W_{d}\right) \leq \gamma_{1}$ for all $\omega$, gives

$$
\left\|T_{\varepsilon d}\right\|_{\infty}<\gamma_{1} \quad \forall \omega
$$

Thus, the requirement $\left\|T_{\varepsilon d}\right\|_{\infty}<\gamma_{1}$ is satisfied.\\
Next, consider $\left\|T_{\varepsilon f}-W_{F}\right\|_{\infty}<1$ to impose fault sensitivity. From Weyl's inequality for singular values [132] it follows that

$$
\underline{\sigma}\left(W_{F}\right)-\bar{\sigma}\left(T_{\varepsilon f}-W_{F}\right) \leq \underline{\sigma}\left(T_{\varepsilon f}\right) \quad \forall \omega \in \Omega .
$$

Since $W_{F}$ is full rank, it has either a left or right inverse. In case $n_{f} \geq n_{\varepsilon}, W_{F}$ is right invertible, i.e., $W_{F} W_{F}^{+}=I$, with $W_{F}^{+}$the right inverse. If $n_{f} \leq n_{\varepsilon}, W_{F}$ is left invertible, i.e., $W_{F}^{+} W_{F}=I$, with $W_{F}^{+}$the left inverse. In either case, it holds that $\bar{\sigma}\left(W_{F}\right)=\frac{1}{\underline{\sigma}\left(W_{F}^{+}\right)}$and $\bar{\sigma}\left(W_{F}^{+}\right)=\frac{1}{\underline{\sigma}\left(W_{F}\right)}$, hence

$$
\left(1-\bar{\sigma}\left(T_{\varepsilon f}-W_{F}\right) \bar{\sigma}\left(W_{F}^{+}\right)\right) \underline{\sigma}\left(W_{F}\right) \leq \underline{\sigma}\left(T_{\varepsilon f}\right) \quad \forall \omega \in \Omega .
$$

Since $\left\|T_{\varepsilon f}-W_{F}\right\|_{\infty}<1$, and by design $\left\|W_{F}\right\|_{-}>1+\gamma_{2}$, which implies $\bar{\sigma}\left(W_{F}^{+}\right)< \frac{1}{1+\gamma_{2}}$, it holds that $\bar{\sigma}\left(T_{\varepsilon f}-W_{F}\right) \bar{\sigma}\left(W_{F}^{+}\right)<\frac{1}{1+\gamma_{2}}$. Substitution of the latter gives

$$
\frac{\gamma_{2}}{1+\gamma_{2}} \underline{\sigma}\left(W_{F}\right) \leq \underline{\sigma}\left(T_{\varepsilon f}\right) \quad \forall \omega \in \Omega
$$

Now using that $W_{F}$ is designed following $\left\|W_{f}\right\|_{-}=\frac{\gamma_{2}}{1+\gamma_{2}}\left\|W_{F}\right\|_{-}$, it holds that $\underline{\sigma}\left(W_{f}\right) \leq \underline{\sigma}\left(T_{\varepsilon f}\right)$ for all $\omega \in \Omega$, i.e.,

$$
\left\|T_{\varepsilon f}\right\|_{-} \geq\left\|W_{f}\right\|_{-}
$$

Hence, considering $\left\|W_{f}\right\|_{-} \geq \gamma_{2}$, gives $\left\|T_{\varepsilon f}\right\|_{-} \geq \gamma_{2}$.

\section*{4.B Proof of theorem 4.19}
In this appendix, the proof of Theorem 4.19 is provided. In addition, an overview of the proof of Theorem 4.19 is depicted in Figure 4.B.1, illustrating that RPP is implied by a special case of a RS test.

Proof. The definition of $\mu$ gives at each frequency

$$
\begin{array}{r}
\mu_{\hat{\Delta}_{\mathrm{part}}}(N(j \omega))<1 \Leftrightarrow \operatorname{det}\left(I-N(j \omega) \hat{\Delta}_{\mathrm{part}}(j \omega)\right) \neq 0 \\
\forall \hat{\Delta}_{\mathrm{part}}, \bar{\sigma}\left(\hat{\Delta}_{\mathrm{part}}(j \omega)\right) \leq 1
\end{array}
$$

Let $N$ be partitioned as

$$
N=\left[\begin{array}{ll}
N_{11} & \bar{N}_{12} \\
\bar{N}_{21} & \bar{N}_{22}
\end{array}\right]=\left[\begin{array}{lll}
N_{11} & N_{12} & N_{13} \\
N_{21} & N_{22} & N_{23} \\
N_{31} & N_{32} & N_{33}
\end{array}\right]
$$

Application of Schur's formula for determinants gives that

$$
\begin{aligned}
& \operatorname{det}\left(I-N(j \omega) \hat{\Delta}_{\mathrm{part}}(j \omega)\right)=\operatorname{det}\left[\begin{array}{cc}
I-N_{11} \Delta & -\bar{N}_{12} \Delta_{P_{\mathrm{part}}} \\
-\bar{N}_{21} \Delta & I-\bar{N}_{22} \Delta_{P_{\mathrm{part}}}
\end{array}\right] \\
& \quad=\operatorname{det}\left(I-N_{11} \Delta\right) \operatorname{det}\left(I-\bar{N}_{22} \Delta_{P_{\mathrm{part}}}-\bar{N}_{21} \Delta\left(I-N_{11} \Delta\right)^{-1} \bar{N}_{12} \Delta_{P_{\mathrm{part}}}\right) \\
& \quad=\operatorname{det}\left(I-N_{11} \Delta\right) \operatorname{det}\left(I-\left(\bar{N}_{22}+\bar{N}_{21} \Delta\left(I-N_{11} \Delta\right)^{-1} \bar{N}_{12}\right) \Delta_{P_{\mathrm{part}}}\right) \\
& \quad=\operatorname{det}\left(I-N_{11} \Delta\right) \operatorname{det}\left(I-\mathcal{F}_{u}(N, \Delta) \Delta_{P_{\mathrm{part}}}\right)
\end{aligned}
$$

Note that $T=\mathcal{F}_{u}(N, \Delta)$. Further application of Schur's determinant formula yields that

$$
\begin{aligned}
& \operatorname{det}\left(I-T \Delta_{P_{\mathrm{part}}}\right)=\operatorname{det}\left[\begin{array}{cc}
I-T_{\varepsilon \tilde{d}} \Delta_{d} & -T_{\varepsilon f} \Delta_{f} \\
-T_{\tilde{\varepsilon} \tilde{d}} \Delta_{d} & I-T_{\tilde{\varepsilon} f} \Delta_{f}
\end{array}\right] \\
& =\operatorname{det}\left(I-T_{\varepsilon \tilde{d}} \Delta_{d}\right) \operatorname{det}\left(I-T_{\tilde{\varepsilon} f} \Delta_{f}-T_{\tilde{\varepsilon} \tilde{d}} \Delta_{d}\left(I-T_{\varepsilon \tilde{d}} \Delta_{d}\right)^{-1} T_{\varepsilon f} \Delta_{f}\right) \\
& =\operatorname{det}\left(I-T_{\varepsilon \tilde{d}} \Delta_{d}\right) \operatorname{det}\left(I-\left(T_{\tilde{\varepsilon} f}-T_{\tilde{\varepsilon} \tilde{d}} \Delta_{d}\left(I-T_{\varepsilon \tilde{d}} \Delta_{d}\right)^{-1} T_{\varepsilon f}\right) \Delta_{f}\right) \\
& =\operatorname{det}\left(I-T_{\varepsilon \tilde{d}} \Delta_{d}\right) \operatorname{det}\left(I-\mathcal{F}_{u}\left(T, \Delta_{d}\right) \Delta_{f}\right)
\end{aligned}
$$

Alternatively, Schur's formula for determinants allows to write the latter also as

$$
\begin{aligned}
& \operatorname{det}\left(I-T \Delta_{P_{\mathrm{part}}}\right)=\operatorname{det}\left(I-T_{\tilde{\varepsilon} f} \Delta_{f}\right) \operatorname{det}\left(I-T_{\varepsilon \tilde{d}} \Delta_{d}-T_{\varepsilon f} \Delta_{f}\left(I-T_{\tilde{\varepsilon} f} \Delta_{f}\right)^{-1} T_{\tilde{\varepsilon} \tilde{d}} \Delta_{d}\right) \\
& =\operatorname{det}\left(I-T_{\tilde{\varepsilon} f} \Delta_{f}\right) \operatorname{det}\left(I-\left(T_{\varepsilon \tilde{d}}-T_{\varepsilon f} \Delta_{f}\left(I-T_{\tilde{\varepsilon} f} \Delta_{f}\right)^{-1} T_{\tilde{\varepsilon} \tilde{d}}\right) \Delta_{d}\right) \\
& =\operatorname{det}\left(I-T_{\tilde{\varepsilon} f} \Delta_{f}\right) \operatorname{det}\left(I-\mathcal{F}_{l}\left(T, \Delta_{f}\right) \Delta_{d}\right)
\end{aligned}
$$

Hence, the definition of $\mu$ gives at each frequency

$$
\begin{gathered}
\mu_{\hat{\Delta}_{\text {part }}}(N(j \omega))<1 . \\
\Leftrightarrow \\
\operatorname{det}\left(I-N_{11} \Delta\right) \operatorname{det}\left(I-T_{\varepsilon \tilde{d}} \Delta_{d}\right) \operatorname{det}\left(I-\mathcal{F}_{u}\left(T, \Delta_{d}\right) \Delta_{f}\right) \neq 0, \\
\forall \hat{\Delta}_{\text {part }}, \bar{\sigma}\left(\hat{\Delta}_{\text {part }}(j \omega)\right) \leq 1 . \\
\Leftrightarrow \\
\operatorname{det}\left(I-N_{11} \Delta\right) \operatorname{det}\left(I-T_{\tilde{\varepsilon} f} \Delta_{f}\right) \operatorname{det}\left(I-\mathcal{F}_{l}\left(T, \Delta_{f}\right) \Delta_{d}\right) \neq 0, \\
\forall \hat{\Delta}_{\text {part }}, \bar{\sigma}\left(\hat{\Delta}_{\text {part }}(j \omega)\right) \leq 1 .
\end{gathered}
$$

Since the latter two may not be zero, all terms must be non-zero at each frequency, i.e.,

$$
\operatorname{det}\left(I-N_{11} \Delta\right) \neq 0 \forall \Delta \Leftrightarrow \mu_{\Delta}\left(N_{11}\right)<1 \forall \omega \text { (RS) }
$$

and for all $\Delta$

$$
\begin{aligned}
\operatorname{det}\left(I-T_{\varepsilon \tilde{d}} \Delta_{d}\right) \neq 0 \forall \Delta_{d} & \Leftrightarrow \mu_{\Delta_{d}}\left(T_{\varepsilon \tilde{d}}\right)<1 \forall \omega \\
& \Leftrightarrow\left\|T_{\varepsilon \tilde{d}}\right\|_{\infty}<1(\mathrm{RPP}) \\
\operatorname{det}\left(I-T_{\tilde{\varepsilon} f} \Delta_{f}\right) \neq 0 \forall \Delta_{f} & \Leftrightarrow \mu_{\Delta_{f}}\left(T_{\tilde{\varepsilon} f}\right)<1 \forall \omega \\
& \Leftrightarrow\left\|T_{\tilde{\varepsilon} f}\right\|_{\infty}<1(\mathrm{RPP})
\end{aligned}
$$

The condition is sufficient since additionally, $\operatorname{det}\left(I-\mathcal{F}_{u}\left(T, \Delta_{d}\right) \Delta_{f}\right) \neq 0$, and $\operatorname{det}\left(I-\mathcal{F}_{l}\left(T, \Delta_{f}\right) \Delta_{d}\right) \neq 0$ due to the interaction between the $\Delta_{d}$ and $\Delta_{f}$ loops, where

$$
\begin{aligned}
& \mathcal{F}_{u}\left(T, \Delta_{d}\right)=T_{\tilde{\varepsilon} f}+T_{\tilde{\varepsilon} \tilde{d}} \Delta_{d}\left(I-T_{\varepsilon \tilde{d}} \Delta_{d}\right)^{-1} T_{\varepsilon f}, \\
& \mathcal{F}_{l}\left(T, \Delta_{f}\right)=T_{\varepsilon \tilde{d}}+T_{\varepsilon f} \Delta_{f}\left(I-T_{\tilde{\varepsilon} f} \Delta_{f}\right)^{-1} T_{\tilde{\varepsilon} \tilde{d}} .
\end{aligned}
$$

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-127}
\captionsetup{labelformat=empty}
\caption{Figure 4.B.1: The robust partial performance (RPP) specification implied by a special case of structured robust stability (RS). Hence, a $\mu$-test of $N$ is required with respect to the structure of \$\textbackslash Delta\_\{P\_\{\textbackslash text \{part }\}\}\$ to test RPP.\}\end{center}
\end{figure}

\section*{4.C Proof of theorem 4.21}
In this appendix, the proof of Theorem 4.21 is provided.\\
Proof. The definition of $\mu$ gives at each frequency

$$
\begin{gathered}
\mu_{\hat{\Delta}_{\text {part-void }}}\left(N^{\star}(j \omega)\right)<1 \\
\Leftrightarrow \operatorname{det}\left(I-N^{\star}(j \omega) \hat{\Delta}_{\text {part-void }}(j \omega)\right) \neq 0 \\
\forall \hat{\Delta}_{\text {part-void }}, \bar{\sigma}\left(\hat{\Delta}_{\text {part-void }}(j \omega)\right) \leq 1
\end{gathered}
$$

Note that $\hat{\Delta}_{\text {part-void }}$ is can be written as

$$
\hat{\Delta}_{\text {part-void }}=\left[\begin{array}{ccc}
\Delta & 0 & 0 \\
0 & \Delta_{P_{\text {part }}} & 0 \\
0 & 0 & \Delta_{P_{\text {void }}}
\end{array}\right]=\left[\begin{array}{cc}
\hat{\Delta}_{\text {part }} & 0 \\
0 & \Delta_{P_{\text {void }}}
\end{array}\right]
$$

Application of the Schur complement gives that

$$
\operatorname{det}\left(I-N^{\star}(j \omega) \hat{\Delta}_{\mathrm{part}-\mathrm{void}}\right)=\operatorname{det}\left[\begin{array}{cc}
I-N_{11}^{\star}(j \omega) \hat{\Delta}_{\mathrm{part}} & -N_{12}^{\star}(j \omega) \Delta_{P_{\mathrm{void}}} \\
-N_{21}^{\star}(j \omega) \hat{\Delta}_{\mathrm{part}} & I-N_{22}^{\star}(j \omega) \Delta_{P_{\mathrm{void}}}
\end{array}\right]
$$

Note that $N_{12}^{\star}(j \omega), N_{21}^{\star}(j \omega)$, and $N_{22}^{\star}(j \omega)$ relate to the void channels and thus equal 0 . Additionally, $N_{11}^{\star}(j \omega)=N(j \omega)$. Hence, this results in the expression

$$
\begin{aligned}
\operatorname{det}\left(I-N^{\star}(j \omega) \hat{\Delta}_{\mathrm{part}-\mathrm{void}}\right) & =\operatorname{det}\left[\begin{array}{cc}
I-N(j \omega) \hat{\Delta}_{\mathrm{part}} & 0 \\
0 & I
\end{array}\right] \\
& =\operatorname{det}\left(I-N(j \omega) \hat{\Delta}_{\mathrm{part}}\right)
\end{aligned}
$$

Since this expression should not be zero,

$$
\operatorname{det}\left(I-N(j \omega) \hat{\Delta}_{\text {part }}\right) \neq 0 \quad \forall \hat{\Delta}_{\text {part }} \Leftrightarrow \mu_{\hat{\Delta}_{\text {part }}}(N(j \omega))<1
$$

Theorem 4.21 is proven by reading the above in opposite direction, which concludes the proof.

\section*{4.D Proof of theorem 4.24}
In this appendix, the proof of Theorem 4.24 is provided.\\[0pt]
Proof. (based on [178, Theorem 5]) The first condition follows directly from the definition of robust performance, see Theorem 4.18, which states that $\sup _{\omega} \mu_{\Delta_{J}}\left(M_{J J}(j \omega)\right)<1 \Leftrightarrow\left\|\mathcal{F}_{u}\left(M_{J J}, \Delta\right)\right\|_{\infty}<1$ for all $\Delta:\|\Delta\|_{\infty} \leq 1$, where $\mathcal{F}_{u}\left(M_{J J}, \Delta\right)=T_{\varepsilon d} W_{d}^{-1}$.

The remainder follows from the definition of $\mu_{g}$. Let $\left[\begin{array}{c}z_{j} \\ z_{k}\end{array}\right]=M\left[\begin{array}{c}w_{j} \\ w_{k}\end{array}\right]$, corresponding to the structure in Figure 4.8. Then the following are equivalent.

\begin{enumerate}
  \item $\mu_{g, \tilde{\Delta}}(M(j \omega))<1$.
  \item There do not exist ( $z_{j}, z_{k}, w_{j}, w_{k}$ ) such that $\left\|w_{j}\right\| \leq\left\|z_{j}\right\| \forall j \in J$, and $\left\|z_{k}\right\| \leq\left\|w_{k}\right\|$.
  \item If $\left\|w_{j}\right\| \leq\left\|z_{j}\right\| \forall j \in J$, then $\left\|z_{k}\right\|>\left\|w_{k}\right\|$.
  \item $\min _{\Delta \in \Delta, \bar{\sigma}(\Delta) \leq 1} \underline{\sigma}\left(\mathcal{F}_{u}\left(M, \Delta_{J}\right)\right)>1$, where $\mathcal{F}_{u}\left(M, \Delta_{J}\right)=T_{\varepsilon f} W_{f}^{-1}$.
\end{enumerate}

\section*{Part III}
\section*{Fault Diagnosis for Nominal Systems}
\section*{Chapter 5}
\section*{Closed-loop Aspects in MIMO Fault Diagnosis with Application to Precision Mechatronics ${ }^{1}$ }
\begin{abstract}
Fault detection is essential in precision mechatronics to facilitate maintenance and minimize operational downtime. The aim of this chapter is to develop a systematic procedure from identification to accurate nullspace-based fault diagnosis, accounting for the influence of noise and interaction in multivariable closed-loop control configurations. The influence of noise and interaction on the model estimate and fault diagnosis system are investigated through the use of closed-loop operators and by means of an illustrative case study.
\end{abstract}

\footnotetext{${ }^{1}$ The results in this chapter constitute Contribution IV in Section 1.6. The chapter is based on: [37] K. Classens, W. P. M. H. Heemels, and T. Oomen, "Closed-loop Aspects in MIMO Fault Diagnosis with Application to Precision Mechatronics," in 2021 IEEE American Control Conference (ACC), New Orleans, LA, USA, 2021, pp. 1756-1761.
}\subsection*{5.1 Introduction}
Predictive maintenance is essential for future precision mechatronics and is enabled by a combination of fault diagnosis and digital twins [ $7,15,82,175,205$, 209]. The attention for predictive maintenance through fault diagnosis is mainly driven by the high cost associated with unscheduled downtime. Fault diagnosis is widely used in many application domains finding its origin in safety-critical domains such as aerospace, automotive and chemical industries, whereas this chapter focuses on the domain of precision mechatronics. Fault diagnosis amounts to three tasks, that is, fault detection, fault isolation and fault identification.

Nullspace-based fault detection and isolation (FDI) methods have been developed [81, 263, 264] and enable fault diagnosis for large-scale complex systems. One of the main appeals of the nullspace-based approach is that it is generally applicable and is based on numerically reliable and computationally efficient techniques.

Model-based FDI methods, such as the nullspace-based method, rely on models of the underlying system dynamics. Two methods for modeling can be pursued: first principles modeling or data-driven modeling. A key question is how to obtain an accurate model which is suitable to serve as a basis for the fault diagnosis system. For precision mechatronics, data-driven modeling as opposed to first principles modeling, is fast, accurate, and inexpensive [161, 184, 202]. Moreover, these systems typically operate in closed-loop configuration, e.g., due to safety constraints. Closed-loop identification problems have been thoroughly analyzed in [256]. In particular, the non-parametric frequency response function-based (FRF) identification for multi-input multi-output (MIMO) complex motion systems has recently been investigated in [74]. Many methods exist to acquire accurate parametric models from FRFs [198, 255]. Despite the major development of identification for control, at present the identification of models for fault detection is missing.

Although important progress has been made in fault detection for complex engineered systems, at present closed-loop aspects in identification for fault diagnosis and the closed-loop aspects in fault diagnosis have not been clarified. The present research is driven by a lack of integral procedure for closed-loop controlled mechatronic systems. Hence, the main contribution of this chapter is the development of a systematic procedure for fault diagnosis for MIMO systems in closed-loop configuration, starting from the identification of an accurate model. The relevance of this framework is highlighted through the investigation of closed-loop operators as well as an illustrative case study. An experimental case study, using an overactuated motion system, is described in [43].

\subsection*{5.2 Problem formulation}
The operation of systems in closed-loop configuration has major implications for FDI and related system identification. In this chapter, these implications are illustrated by answering the following questions.\\
(i) How to accurately estimate a model for FDI despite presence of finite time noisy observations? (Sections 5.3.1 and 5.3.2)\\
(ii) What are the implications of MIMO and closed-loop aspects for model estimation for FDI? (Section 5.3.2)\\
(iii) How to minimize the impact of noise on the fault diagnosis system? (Section 5.3.3)\\
(iv) What are the implications of MIMO and closed-loop aspects for fault detection? (Section 5.3.4)

The answers to these questions form a procedure for fault diagnosis design for MIMO systems, starting with the estimation of an accurate model. The main focus lies on the fault detection (FD) task, i.e., fault isolation and identification are not considered in this chapter.

First, the closed-loop setting for FRF estimation is introduced. Thereafter, the setting for FD is introduced.

\subsection*{5.2.1 Closed-loop identification problem}
The closed-loop identification problem is considered as a two-step approach. First, a non-parametric FRF is estimated. Thereafter, a parametric model is obtained through an appropriate fitting procedure. First, the setting for nonparametric FRF estimation is introduced. For the second step, the reader is referred to, e.g., [198, 255].

Consider the continuous time linear time-invariant (LTI) MIMO system described with transfer function matrix (TFM) $G_{u}$ with input signal $u: \mathbb{R}_{+} \mapsto \mathbb{R}^{k}$, defined at time $t \in \mathbb{R}_{+}$, as depicted in Figure 5.1. The output trajectory $y: \mathbb{R}_{+} \mapsto \mathbb{R}^{m}$ is given by


\begin{equation*}
y=v+G_{u} u \tag{5.1}
\end{equation*}


where the additive zero-mean stationary stochastic noise contribution takes values $v \in \mathbb{R}^{m}$ with spectral density $\Phi_{v}$. This can be formulated as noise model $v=G_{d} d$, where $d \in \mathbb{R}^{p}$ takes values from zero-mean white noise. In the closedloop configuration, the output is equal to


\begin{equation*}
y=\left(I+G_{u} C\right)^{-1} G_{u} r+\left(I+G_{u} C\right)^{-1} v \tag{5.2}
\end{equation*}


and the input is given by


\begin{equation*}
u=\left(I+C G_{u}\right)^{-1} r-\left(I+C G_{u}\right)^{-1} C v \tag{5.3}
\end{equation*}


\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-135}
\captionsetup{labelformat=empty}
\caption{Figure 5.1: Closed-loop configuration used for system identification. The open loop configuration is highlighted by ( $\quad$ ).}
\end{center}
\end{figure}

The first goal of the closed-loop identification problem is to obtain a nonparametric model of $G_{u}$ through the known input $u$, the measured output $y$ and the signal $r: \mathbb{R}_{+} \mapsto \mathbb{R}^{k}$. Depending on the application, also the controller $C$ might be known. Note that by setting $C=0$ and considering $r=u$, the standard open loop identification problem is found [161], as highlighted in Figure 5.1.

Assumption 5.1. It is assumed that in case the estimate $\widehat{G}_{u}\left(e^{j \omega}\right)$ is an accurate representation of $G_{u}\left(e^{j \omega}\right)$, no additional modeling error is introduced through fitting procedures in order to obtain a parametric model.

\subsection*{5.2.2 Closed-loop fault detection problem}
The configuration for closed-loop fault detection, illustrated in Figure 5.2, is highly similar to that used for system identification, cf. Figure 5.1. However, now additive faults $f \in \mathbb{R}^{q}$ affect the output $y$ through the TFM $G_{f}(s)$. Hence, the output is given by


\begin{equation*}
y=\left(I+G_{u} C\right)^{-1} G_{u} r+\left(I+G_{u} C\right)^{-1} G_{f} f+\left(I+G_{u} C\right)^{-1} G_{d} d \tag{5.4}
\end{equation*}


and the input is given by


\begin{equation*}
u=\left(I+C G_{u}\right)^{-1} r-\left(I+C G_{u}\right)^{-1} C G_{f} f-\left(I+C G_{u}\right)^{-1} C G_{d} d \tag{5.5}
\end{equation*}


Moreover, the system is augmented by a residual generator formed by a proper and stable TFM $Q:=\left[\begin{array}{ll}Q_{y} & Q_{u}\end{array}\right]$. The residual $\varepsilon$, used for fault detection, is equal to


\begin{equation*}
\varepsilon=Q_{u} u+Q_{y} y . \tag{5.6}
\end{equation*}


Loosely speaking, the fault detection goal is that the residual $\varepsilon \not \approx 0$, i.e., is sufficiently larger than zero in the presence of faults and the residual $\varepsilon \approx 0$, i.e., is sufficiently small in the absence of faults. Note that by setting $C=0$ and

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-136}
\captionsetup{labelformat=empty}
\caption{Figure 5.2: Closed-loop configuration used for fault detection through the residual signal $\varepsilon$. The residual generator, consisting of the filter $Q(s)$, is highlighted by ( $\square$ ) and the open loop configuration is highlighted by ( $\square$ ).}
\end{center}
\end{figure}

considering $r=u$, the standard open loop fault detection problem [263], where $y$ is given by


\begin{equation*}
y=G_{u} u+G_{f} f+G_{d} d, \tag{5.7}
\end{equation*}


is obtained as highlighted in Figure 5.2.

\subsection*{5.2.3 Illustrative multivariable setup}
The system identification and fault detection problem are demonstrated on a representative model that consists of two DC motors that are interconnected by an elastic element as depicted in Figure 5.3, and illustrated in Figure 5.4. The elastic element enforces a strong cross-coupling between the two inputs and two outputs of the system. The system is represented by the state-space description


\begin{align*}
& \dot{x}=A x+B u,  \tag{5.8a}\\
& y=C x, \tag{5.8b}
\end{align*}


with

$$
\begin{aligned}
A & =\left[\begin{array}{cccc}
0 & 1 & 0 & 0 \\
-\frac{k_{1}+k_{2}}{J_{1}} & -\frac{d_{1}+d_{2}}{J_{1}} & \frac{k_{2}}{J_{1}} & \frac{d_{2}}{J_{1}} \\
0 & 0 & 0 & 1 \\
\frac{k_{2}}{J_{1}} & \frac{d_{2}}{J_{1}} & -\frac{k_{2}+k_{3}}{J_{1}} & -\frac{d_{2}+d_{3}}{J_{1}}
\end{array}\right], B=\left[\begin{array}{cc}
0 & 0 \\
\frac{1}{J_{1}} & 0 \\
0 & 0 \\
0 & \frac{1}{J_{2}}
\end{array}\right], \\
C & =\left[\begin{array}{llll}
1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0
\end{array}\right]
\end{aligned}
$$

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-137(1)}
\captionsetup{labelformat=empty}
\caption{Figure 5.3: Illustrative setup with two DC motors (a) connected to two rotating masses (b) that are interconnected by a flexible band (c).}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-137}
\captionsetup{labelformat=empty}
\caption{Figure 5.4: Schematic drawing of a multivariable setup that consists of two DC motors and an elastic connection.}
\end{center}
\end{figure}

where the output $y:=\left[\begin{array}{ll}y_{1} & y_{2}\end{array}\right]^{\top} \in \mathbb{R}^{2}$ consists of the angular positions of both masses measured through optical encoders and the input $u:=\left[\begin{array}{ll}u_{1} & u_{2}\end{array}\right]^{\top} \in \mathbb{R}^{2}$ consists of the torques exerted by the motors. The state vector, denoted by $x:=\left[\begin{array}{llll}x_{1} & x_{2} & x_{3} & x_{4}\end{array}\right]^{\top} \in \mathbb{R}^{4}$, corresponds to the angular positions $x_{1}$ and $x_{3}$, and angular velocities $x_{2}$ and $x_{4}$. The TFM of the system is equal to $G_{u}(s)= C(s I-A)^{-1} B$. The multivariable system is controlled with a decentralized controller, i.e., a diagonal controller of the form $C(s):=\operatorname{diag}\left(C_{11}(s), C_{22}(s)\right)$. A decentralized controller for this system can, for instance, be designed through sequential loop closing.

\subsection*{5.3 Procedure for closed-loop fault detection based on identified models}
In this section, a procedure is presented for the closed-loop fault detection problem described in Section 5.2.2 based on identified models from the problem described in Section 5.2.1. This is pursued systematically on the basis of answering the questions (i) to (iv) proposed in Section 5.2.

\subsection*{5.3.1 Standard open loop identification solution}
In this subsection, question (i) from Section 5.2 is addressed. First, we consider a single-input single-output (SISO) system operating in open loop. The implications of considering a MIMO system and a closed-loop configuration are addressed in 5.3.2.

To obtain an accurate estimate $\widehat{G}_{u}\left(e^{j \omega}\right)$ of the system $G_{u}\left(e^{j \omega}\right)$, despite the presence of noise, consider one of the various commonly accepted solutions for the open loop problem. For arbitrary signals $u$ and $y$, e.g., generated through applying white noise input $u$, spectral analysis is a commonly applied solution. Here, after averaging over $i=N$ time-frames, the cross-power spectrum of the output and input, $\Phi_{y u}(\omega)$, is divided by the auto-power spectrum of the input, denoted by $\Phi_{u u}(\omega)$. This gives


\begin{equation*}
\widehat{G}_{u}\left(e^{j \omega}\right)=\frac{\widehat{\Phi}_{y u}(\omega)}{\widehat{\Phi}_{u u}(\omega)}=\frac{\frac{1}{N} \sum_{i=1}^{N} Y_{i}(\omega) U_{i}(\omega)^{H}}{\frac{1}{N} \sum_{i=1}^{N} U_{i}(\omega) U_{i}(\omega)^{H}} \tag{5.9}
\end{equation*}


where $H$ denotes the Hermitian transpose and $Y_{i}(\omega)$ and $U_{i}(\omega)$ are the Fourier transforms of $y_{i}$ and $u_{i}$ corresponding to the $i^{\text {th }}$ frame. For smoothing purposes, typically a window, e.g., a Hamming window, is applied to each frame. Moreover, more frames may be created through overlap. This approach is often referred to as Welch's modified periodogram method [274]. The coherence function can be considered as a quality certificate for the estimate. For estimates of the covariance on the transfer function estimate, the reader is referred to [226]. Despite noise entering the open loop system, Welch's modified periodogram method provides an accurate solution to minimize the effect of noise so that an accurate FRF estimate is obtained.

\subsection*{5.3.2 MIMO and closed-loop implications on model estimate}
In this subsection, question (i) and (ii) from Section 5.2 are addressed. Consider the closed-loop configuration from Figure 5.1, where a SISO system is excited through the signal $r$. Using the signals $u$ and $y$, similar to (5.9) gives the estimate [236, Chapter 10]


\begin{equation*}
\widehat{G}_{u}\left(e^{j \omega}\right)=\frac{\widehat{\Phi}_{y u}(\omega)}{\widehat{\Phi}_{u u}(\omega)}=\frac{G_{u}\left(e^{j \omega}\right) \Phi_{r r}(\omega)-C^{H}\left(e^{j \omega}\right) \Phi_{v v}(\omega)}{\Phi_{r r}(\omega)+\left|C\left(e^{j \omega}\right)\right|^{2} \Phi_{v v}(\omega)} \tag{5.10}
\end{equation*}


As a result of the correlation between $u$ and $v$, a biased estimate is obtained, which may lead to severe consequences for control and fault diagnosis system design. In the worst-case, if the system is solely driven by $v$, the obtained estimate is equal to the inverse controller.

A method to overcome this bias, is to estimate the FRF indirectly through the sensitivity function $S$ and the process sensitivity function $G S$. These estimates are given by


\begin{equation*}
\widehat{S}\left(e^{j \omega}\right)=\frac{\widehat{\Phi}_{u r}(\omega)}{\widehat{\Phi}_{r r}(\omega)}=\frac{\frac{1}{N} \sum_{i=1}^{N} U_{i}(\omega) R_{i}(\omega)^{H}}{\frac{1}{N} \sum_{i=1}^{N} R_{i}(\omega) R_{i}(\omega)^{H}}, \tag{5.11}
\end{equation*}


and


\begin{equation*}
\widehat{G_{u} S}\left(e^{j \omega}\right)=\frac{\widehat{\Phi}_{y r}(\omega)}{\widehat{\Phi}_{r r}(\omega)}=\frac{\frac{1}{N} \sum_{i=1}^{N} Y_{i}(\omega) R_{i}(\omega)^{H}}{\frac{1}{N} \sum_{i=1}^{N} R_{i}(\omega) R_{i}(\omega)^{H}} \tag{5.12}
\end{equation*}


respectively. Similarly, to estimate the sensitivity and the process sensitivity, Welch's modified periodogram method may be applied [274]. Then, the estimate of the plant $G_{u}$ may be obtained through


\begin{equation*}
\widehat{G}_{u}\left(e^{j \omega}\right)=\frac{\widehat{G_{u} S}\left(e^{j \omega}\right)}{\widehat{S}\left(e^{j \omega}\right)} \tag{5.13}
\end{equation*}


For SISO systems, this provides an accurate estimate.\\
The approach for open loop systems through (5.9) can easily be extended to the MIMO case of the system $G_{u}$ with $k$ inputs by performing $k$ experiments, where each experiment independently excites one of in the inputs in order to estimate a column of $G_{u}$. For alternative approaches, requiring a single experiment, see for instance [202, Chapter 7].

However, for closed-loop systems, the relation (5.13) has to be handled with care. To illustrate this, consider the MIMO configuration with two inputs and two outputs with a decentralized control configuration, depicted in Figure 5.5. The relation between input $u_{1}$ and output $y_{1}$ is now the equivalent plant defined as


\begin{equation*}
G_{u, 11}^{\mathrm{eq}}:=G_{u, 11}-\underbrace{\frac{G_{u, 12} C_{22} G_{u, 21}}{1+C_{22} G_{u, 22}}}_{\text {cross-coupling }} \tag{5.14}
\end{equation*}


Similarly,


\begin{equation*}
G_{u, 22}^{\mathrm{eq}}:=G_{u, 22}-\underbrace{\frac{G_{u, 21} C_{11} G_{u, 12}}{1+C_{11} G_{u, 11}}}_{\text {cross-coupling }} \tag{5.15}
\end{equation*}


gives the equivalent plant between input $u_{2}$ and output $y_{2}$. Clearly, neglecting the interaction due to cross-coupling leads to a biased estimate. In fault diagnosis, working with an equivalent plant has its advantages. For instance, for systems with considerable amount of inputs and outputs, where the engineer is only interested in faults in a specific location or in case not all inputs and outputs are available to the fault diagnosis system.

If an estimate of the true MIMO plant is desired, $k$ independent excitation experiments have to be performed for each input. Each experiment allows to

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-140}
\captionsetup{labelformat=empty}
\caption{Figure 5.5: MIMO closed-loop configuration with decentralized control architecture. The equivalent plants \$G\_\{u, 11}\^{}\{\textbackslash mathrm\{eq\}\}(\textbackslash square)\$ and $G_{u, 22}^{\mathrm{eq}}(\square)$ are highlighted.\}\end{center}
\end{figure}

identify a column of the sensitivity estimate $\widehat{S}\left(e^{j \omega}\right)$ and process sensitivity estimate $\widehat{G_{u} S}\left(e^{j \omega}\right)$. Then, the estimate for the true MIMO plant can be obtained through the matrix product


\begin{equation*}
\widehat{G}_{u}\left(e^{j \omega}\right)=\widehat{G_{u} S}\left(e^{j \omega}\right) \widehat{S}\left(e^{j \omega}\right)^{-1} . \tag{5.16}
\end{equation*}


Even though the difference between (5.13) and (5.16) is subtle, the obtained estimate may be very different as a result of interaction.

Despite noise entering the closed-loop system during identification, there exist appropriate solutions to minimize its effect. However, the identification solution should carefully be selected such that biases do not enter the estimate on which the fault diagnosis system is designed, i.e., (5.13) for a SISO plant or the equivalent plants and (5.16) for the MIMO plant.

\subsection*{5.3.3 Open loop fault detection solution}
In this subsection, question (iii) from Section 5.2 is addressed. To design a linear residual generator $Q$ and minimize the impact of noise on the residual signal, consider the configuration from Figure 5.2. First, a MIMO system is considered in open loop configuration. Closed-loop aspects are addressed in Section 5.3.4. The linear residual generator processing the measurable outputs $y$ and known\\
inputs $u$ has input-output relation

\[
\varepsilon=Q\left[\begin{array}{l}
y  \tag{5.17}\\
u
\end{array}\right]
\]

Substitution of the open loop relation for $y$, i.e., (5.4) with $C=0$ and $r=u$ gives

\[
\varepsilon=Q\left[\begin{array}{ccc}
G_{u} & G_{d} & G_{f}  \tag{5.18}\\
I & 0 & 0
\end{array}\right]\left[\begin{array}{l}
u \\
d \\
f
\end{array}\right],
\]

or equivalently


\begin{equation*}
\varepsilon=\underbrace{\left(Q_{u}+Q_{y} G_{u}\right)}_{:=G_{\varepsilon u}} u+\underbrace{\left(Q_{y} G_{f}\right)}_{:=G_{\varepsilon f}} f+\underbrace{\left(Q_{y} G_{d}\right)}_{:=G_{\varepsilon d}} d \tag{5.19}
\end{equation*}


The residual generator $Q$ can always be parameterized such that the residual $\varepsilon$ is decoupled from the input $u$ through the design criterion\\
а) $G_{\varepsilon u}=0$,\\
which implies that $Q_{u}=-Q_{y} G_{u}$. The effects of the noise $d$ can usually not be decoupled from $\varepsilon$. Hence, $Q_{y}$ should be designed to achieve that the residual $\varepsilon$ is significantly influenced by all fault entries $f_{i}$, where $i=1, \ldots, q$, and the influence of the noise signal $d$ is negligible. Thus, loosely speaking the two additional design conditions, which have to be fulfilled, are\\
b) $G_{\varepsilon f} \neq 0$,\\
c) $G_{\varepsilon d} \approx 0$.

Specifically, the objective is to maximize the gap between the fault detectability and noise attenuation. An optimization-based approach is employed following [264, Chapter 5]. Consider the admissible noise level $\gamma>0$, imposed through


\begin{equation*}
\left\|G_{\varepsilon d}\right\|_{\infty} \leq \gamma \tag{5.20}
\end{equation*}


where $\|\cdot\|_{\infty}$ denotes the $\mathcal{H}_{\infty}$-norm. For $\gamma>0$, a normalized value of $\gamma=1$ can be used via suitable scaling of the residual filter $Q$. To characterize fault sensitivity, the index


\begin{equation*}
\left\|G_{\varepsilon f}\right\|_{\infty-}:=\min _{1 \leq i \leq q}\left\|G_{\varepsilon f_{i}}\right\|_{\infty} \tag{5.21}
\end{equation*}


is used as a sensitivity measure, covering globally all fault inputs. Other indexes can be used, e.g., based on the least singular values of the frequency-response of $G_{\varepsilon f}$ [62, 142]. Hence, given $\gamma \geq 0$, a stable and proper optimal fault detection filter $Q$ and corresponding optimal fault sensitivity level $\beta>0$ need to be determined such that


\begin{equation*}
\beta=\max _{Q}\left\{\left\|G_{\varepsilon f}\right\|_{\infty-} \mid\left\|G_{\varepsilon d}\right\|_{\infty} \leq \gamma\right\} . \tag{5.22}
\end{equation*}


The gap $\frac{\beta}{\gamma}$ can be used as measure to assess the quality of the fault detection filter. Through coprime factorization, the poles of the residual generator can be assigned through which the speed of the FD process can be regulated.

In case the residual generator is based on an accurate fit of the non-parametric FRFs obtained through the methods described in Section 5.3.2, the conditions a) to c) are accurately met. Moreover, the effect of noise during residual generation is minimized through the employed optimization-based approach.

\subsection*{5.3.4 MIMO and closed-loop implications on fault detection}
In this subsection, question (iv) from Section 5.2 is addressed. As described in Section 5.3.2, closing the control loop has substantial consequences for model estimation. The impact on the fault diagnosis system investigated using closedloop operators.

Theorem 5.2. Let $G_{u}, G_{d}$ and $G_{f}$ be given transfer function matrices for the system described by (5.7) with residual generator (5.6), shown in Figure 5.2 (■) and let $G_{u}, G_{d}$ and $G_{f}$ be the same transfer function matrices for the system described by (5.4) with residual generator (5.6), shown in Figure 5.2. Then, decoupling the residual $\varepsilon$ from $u$ and maximizing (5.22) for the open loop configuration leads to the exact same residual generator $Q$ as decoupling the residual $\varepsilon$ from $r$ and maximizing (5.22) for the closed-loop configuration. Hence, the residual generation problem is invariant to the feedback-control loop, i.e., invariant to the controller $C$.

Proof. The proof is provided in Appendix 5.A.

Hence, the design criteria for the closed-loop configuration are the exact same as a) to c) in the open loop setting. It is worth emphasizing that this does not hold in case modeling uncertainty is taken into account, see Chapters 2 to 4.

This result implies that when the entire MIMO plant $G_{u}$ is known, the same residual generator can be used in an open loop as well as in a closed-loop configuration.

In case the equivalent plant is used for fault diagnosis, e.g., because either the full MIMO plant is unknown or since it is of large dimensions, the fault detection problem is not invariant to the controller located in the equivalent plant.

Moreover, this result shows that despite the controller attenuating the effect of additive faults, it is equally detectable in the residual, which can be considered as a nice property. Indeed, the residual is thus not a metric that describes the severity of the fault in regard to the performance of the closed-loop system.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-143}
\captionsetup{labelformat=empty}
\caption{Figure 5.6: Estimation of the MIMO transfer function matrix and equivalent plants, compared to the true \$G\_\{u}\$ (---) and the true equivalent plants $G_{u, 11}^{\mathrm{eq}}$ (---) and $G_{u, 22}^{\text {eq }}(-\cdot-)$. The MIMO estimate $\widehat{G}_{u}(-)$ is indirectly identified with matrix wise multiplication of the process sensitivity and inverse sensitivity. The equivalent plant estimates $\widehat{G}_{u, 11}^{\mathrm{eq}}(-)$ and $\widehat{G}_{u, 22}^{\mathrm{eq}}(-)$ are estimated via elementwise division of the process sensitivity and inverse sensitivity. Moreover, the biased estimate (-), identified through the use of solely $u$ and $y$ is depicted.\}\end{center}
\end{figure}

\subsection*{5.4 Illustrative case study}
Through this simulation-based case study, the implications of closed-loop feedback on different fault detection filter designs are illustrated. The first filter is designed neglecting the influence of the noise $d$, the second filter is based on the assumption that interaction may be neglected, and the third filter takes both into account.

Consider the data-generating system described by (5.8) with $J_{1}=J_{2}= 0.02, k_{1}=k_{3}=0.01, d_{1}=d_{3}=0.02, k_{2}=500$ and $d_{2}=0.5$ with transfer function matrix $G_{u}(s)$ and decentralized controller as depicted in Figure 5.5, with $C_{11}(s)=\frac{450 s+14140}{s}$ and $C_{22}(s)=1.75 s+110$. Moreover, $G_{d_{1}}(s)=G_{d_{2}}(s)= \frac{s}{s+600 \pi}$, representing high frequency noise with $d_{1}, d_{2} \sim \mathcal{N}\left(0,10^{-2}\right)$.

First, the system dynamics are identified through two independent identical experiments with $r_{1}, r_{2} \sim \mathcal{N}(0,1)$, of which the result is depicted in Figure 5.6. The $2 \times 2$ MIMO plant is estimated with (5.16) and the equivalent plants are estimated with (5.13). Moreover, the bias obtained through estimating with (5.10) is illustrated through comparison with the true equivalent plants.

Assumption 5.3. It is assumed that the estimates $\widehat{G}_{u}, \widehat{G}_{u, 11}^{\mathrm{eq}}, \widehat{G}_{u, 22}^{\mathrm{eq}}$ are fit ei that the true plants $G_{u}, G_{u, 11}^{\mathrm{eq}}$ and $G_{u, 22}^{\mathrm{eq}}$ are obtained. These parametric models are used in the following fault detection case study.

The latter is assumed in order to separate the effect of noise during model estimation and during fault detection for a fair comparison of the filter designs. Hence, the effect of biases as a result of model estimation are negated.\\
Assumption 5.4. It is assumed that only the first actuator is sensitive to faults. Hence, it is assumed that only additive faults can enter the system at $u_{1}$, shown in Figure 5.5.

Since only faults enter the system at $u_{1}$, there is no direct interest in the remaining part of the system, i.e., only the equivalent plant $G_{u, 11}^{\mathrm{eq}}$ as depicted in Figure 5.5 is considered. Another motivation to work with this equivalent plant might be that $u_{2}$ and $y_{2}$ are not directly available for residual generation. Considering $G_{u, 11}^{\mathrm{eq}}$, the actuator fault enters $y_{1}$ through $G_{f}=G_{u, 11}^{\mathrm{eq}}$. Note that the disturbance $d_{2}$ enters $y_{1}$ through


\begin{equation*}
G_{d_{2}}^{\mathrm{eq}}:=-\frac{G_{u, 12} C_{22}}{1+C_{22} G_{u, 22}} G_{d_{2}} \tag{5.23}
\end{equation*}


The first mass is required to follow a sinusoidal setpoint, implemented through the signal $r_{1}=C_{11} \bar{r}_{1}$ with $\bar{r}_{1}(t)=\sin (0.2 \pi t)$. In the remainder, three residual generators are compared on the basis of different assumptions.\\
(1) Neglecting the influence of noise, i.e., based on $G_{u, 11}^{\mathrm{eq}}, G_{f}=G_{u, 11}^{\mathrm{eq}}$ and assuming $G_{d}=0$.\\
(2) Neglecting interaction, i.e., based on $G_{u, 11}, G_{f}=G_{u, 11}$ and $G_{d_{1}}$.\\
(3) Including interaction, i.e., based on $G_{u, 11}^{\mathrm{eq}}, G_{f}=G_{u, 11}^{\mathrm{eq}}, G_{d_{1}}$ and $G_{d 2}^{\mathrm{eq}}$.

The considered additive fault is depicted in Figure 5.7, as well as the inputs, outputs and reference signal. The normalized residual signals for each of the filter designs are depicted in Figure 5.8. The residual signal of the first design (-) performs relatively poorly since noise is neglected, i.e., only the design condition $G_{\varepsilon r}$ is used, leading to the simple residual generator of the form $Q_{u}=-G_{u, 11}^{\mathrm{eq}}$ and $Q_{y}=1$. Clearly, the noise is dominating the residual signal, but some effect of the faults can be observed, e.g., most clearly from $25 \leq t \leq 75$.

The second residual generator (—), obtained through the optimization-based approach described in Section 5.3.3 performs poorly because the signal $r$ is not decoupled from the residual since $G_{\varepsilon r} \neq 0$. The underlying reason is that the parameterization $Q_{u}=-Q_{y} G_{u, 11}$ is used, whereas $G_{u, 11}^{\mathrm{eq}}$ is the true plant.

The third residual generator (一), obtained through the optimization-based approach described in Section 5.3.3 performs well as the interaction is taken into account and the fault to noise ratio is maximized. A simple fault flag (-) is obtained by taking the absolute value of and a moving average of the residual $\varepsilon$.

This illustrative example shows that for this MIMO closed-loop controlled systems, an interpretable residual signal is only obtained through taking the interaction and noise into account for the fault diagnosis system design.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-145(1)}
\captionsetup{labelformat=empty}
\caption{Figure 5.7: The actuator fault signal (-), input \$u\_\{1}\$ (-), input $u_{2}$ (-), output $y_{1}$ (一), output $y_{2}$ (一) and reference $\bar{r}_{1}$ (---). Due to closed-loop control, the effect of the fault is attenuated and is not visible in the output.\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-145}
\captionsetup{labelformat=empty}
\caption{Figure 5.8: Comparison of different normalized residual signals, based on the assumptions (1) (—), (2) (—) and (3) (—). A simple fault flag is obtained by taking the absolute value and moving average of the third filter design (-).}
\end{center}
\end{figure}

\subsection*{5.5 Conclusion}
The presented identification and fault detection framework in this chapter addresses major ambiguities in closed-loop systems. This allows for fault detection in complex systems such as precision mechatronics where only submodules are identified and used in fault diagnosis system. Through an illustrative case study, it is shown that care should be taken with respect to interacting submodules. The presented framework provides a solid basis for fault detection, predictive maintenance and digital twins for complex mechatronic systems.

\section*{Appendices}
\section*{5.A Proof of theorem 5.2}
In this appendix, the proof of Theorem 5.2 is provided.\\
Proof. Substituting the closed-loop operators from (5.4) and (5.5) into (5.6) gives


\begin{align*}
\varepsilon= & \underbrace{\left(Q_{u}\left(I+C G_{u}\right)^{-1}+Q_{y}\left(I+G_{u} C\right)^{-1} G_{u}\right)}_{G_{\varepsilon r}} r \\
& +\underbrace{\left(-Q_{u}\left(I+C G_{u}\right)^{-1} C G_{f}+Q_{y}\left(I+G_{u} C\right)^{-1} G_{f}\right)}_{G_{\varepsilon f}} f  \tag{5.24}\\
& +\underbrace{\left(-Q_{u}\left(I+C G_{u}\right)^{-1} C G_{d}+Q_{y}\left(I+G_{u} C\right)^{-1} G_{d}\right)}_{G_{\varepsilon d}} d
\end{align*}


Rearrangement by employing the push-through rule gives


\begin{align*}
\varepsilon= & \left(Q_{u}\left(I+C G_{u}\right)^{-1}+Q_{y} G_{u}\left(I+C G_{u}\right)^{-1}\right) r \\
& +\left(-Q_{u} C\left(I+G_{u} C\right)^{-1} G_{f}+Q_{y}\left(I+G_{u} C\right)^{-1} G_{f}\right) f  \tag{5.25}\\
& +\left(-Q_{u} C\left(I+G_{u} C\right)^{-1} G_{d}+Q_{y}\left(I+G_{u} C\right)^{-1} G_{d}\right) d
\end{align*}


Employing the parameterization $Q_{u}=-Q_{y} G_{u}$ gives


\begin{align*}
\varepsilon= & \left(Q_{y} G_{u} C\left(I+G_{u} C\right)^{-1} G_{f}+Q_{y}\left(I+G_{u} C\right)^{-1} G_{f}\right) f  \tag{5.26}\\
& +\left(Q_{y} G_{u} C\left(I+G_{u} C\right)^{-1} G_{d}+Q_{y}\left(I+G_{u} C\right)^{-1} G_{d}\right) d,
\end{align*}


such that the residual $\varepsilon$ is always decoupled from the exogenous signal $r$, i.e., $G_{\varepsilon r}=0$. Now, (5.26) can be rewritten into


\begin{equation*}
\varepsilon=\underbrace{\left(Q_{y} G_{f}\right)}_{=G_{\varepsilon f}} f+\underbrace{\left(Q_{y} G_{d}\right)}_{=G_{\varepsilon d}} d \tag{5.27}
\end{equation*}


from which the remaining design criteria $G_{\varepsilon f} \neq 0$ and $G_{\varepsilon d} \approx 0$ are obtained.

\section*{Chapter \\
 6}
\section*{Nullspace-based Fault Diagnosis for Closed-Loop Mechatronic Systems with Application to Semiconductor Equipment ${ }^{1}$}
\begin{abstract}
Fault diagnosis systems are critical for modern, demanding and complex mechatronic production equipment. The continuous operation and efficiency of these systems is dependent on the ability to detect and isolate faults in a timely and efficient manner. This chapter transforms generic fault detection and isolation conditions into design conditions, uncovering the potential possibilities and fundamental limitations for fault diagnosis systems, particularly tailored to mechatronic systems subjected to actuator and sensor faults. Additionally, nullspace-based fault detection and isolation filters are synthesized and applied to a prototype wafer stage used in the semiconductor industry, providing a demonstration of their effectiveness.
\end{abstract}

\footnotetext{${ }^{1}$ The results in this chapter constitute Contribution V in Section 1.6. The chapter is based on: [49] K. Classens, J. van de Wijdeven, W. P. M. H. Heemels, and T. Oomen, "Nullspacebased Fault Diagnosis for Closed-Loop Mechatronic Systems with Application to Semiconductor Equipment," submitted for journal publication.
}\subsection*{6.1 Introduction}
The high-tech production industry is moving towards a new approach for managing maintenance, with a focus on predictive strategies aimed at reducing the high expenses related to unplanned downtime. In this transition, real-time diagnosis of closed-loop controlled multi-input multi-output (MIMO) systems plays a crucial role in facilitating targeted maintenance and optimizing downtime scheduling. The use of fault diagnosis systems finds its roots in safety-critical environments such as the aerospace, automotive, and chemical industry. In contrast, this chapter specifically focuses on its application in the field of precision mechatronics. The importance of fault detection and isolation (FDI) systems for modern complex control applications cannot be understated, as they are critical in ensuring reliable and efficient production processes [38].

Over the past decades, significant progress has been made in the areas of fault diagnosis and fault-tolerance, as reported in numerous studies such as $[84,136$, 140]. In particular progress has been made in the aforementioned safety-critical domains, but hardly focused on mechatronic production equipment. Developed methods for instance rely on observers [79, 270, 283], parameter estimators [39, 93], factorization-based approaches [62], robust detection filters [125], statistical methods [276], or machine learning [153]. Nullspace-based fault FDI methods have been developed [263,264], which enable detection of faults in large complex systems and allow to identify the root cause based on a set of observed symptoms, see, e.g., [63, 251, 264]. The nullspace-based approach is appealing because it is widely applicable and based on computationally efficient and numerically reliable algorithms.

Despite recent advancements in fault detection and isolation for complex engineered systems, fault diagnosis of mechatronic production equipment continues to pose several challenges. Although generically applicable methods have been developed, thorough understanding and implications of the solvability conditions for mechatronic systems and its design is lacking. To this end, generic fault diagnosis conditions are examined and transformed to uncover fundamental limitations and opportunities. Additionally, the nullspace-based method is applied to a multi-input multi-output mechatronic system, serving as proof for its viability for application in large-scale industrial systems.

The main contributions are be outlined as follows.\\
C1 The solvability requirements for approximate fault detection and isolation filter synthesis are transformed into criteria specifically tailored to systems subjected to actuator and sensor faults.

C2 The proposed approach is applied to a large-scale industrial mechatronic system, showcasing the effectiveness of the proposed approach.

This chapter is organized as follows. The problem is formulated in Section 6.2. In particular, the approximate fault detection and isolation problem\\
(AFDIP). The proposed solution to the fault diagnosis problem is outlined in Section 6.3. Subsequently, solvability requirements are examined and transformed to expose fundamental properties in Section 6.4, followed by a concise discussion and several examples. Experimental results are presented in Section 6.5, which illustrate the effectiveness of the proposed approach. Finally, the findings are summarized and conclusions drawn in Section 6.6.

\subsection*{6.2 Problem formulation}
Consider the closed-loop relation for multi-input multi-output (MIMO) systems affected by additive faults, described by


\begin{align*}
y=(I & \left.+G_{u} C\right)^{-1} G_{u} C r+\left(I+G_{u} C\right)^{-1} G_{d} d \\
& +\left(I+G_{u} C\right)^{-1} G_{w} w+\left(I+G_{u} C\right)^{-1} G_{f} f \tag{6.1}
\end{align*}


where the control input is described by


\begin{align*}
u=(I & \left.+C G_{u}\right)^{-1} C r-\left(I+C G_{u}\right)^{-1} C G_{d} d \\
& -\left(I+C G_{u}\right)^{-1} C G_{w} w-\left(I+C G_{u}\right)^{-1} C G_{f} f \tag{6.2}
\end{align*}


see Figure 6.1. Here, the output Laplace-transformed time-dependent output is denoted by $y \in \mathbb{R}^{n_{y}}$, the control input by $u \in \mathbb{R}^{n_{u}}$, the disturbance input by $d \in \mathbb{R}^{n_{d}}$, the noise by $w \in \mathbb{R}^{n_{w}}$, and the fault vector by $f \in \mathbb{R}^{n_{f}}$. The continuous-time transfer functions matrices (TFMs) $G_{u}, G_{d}, G_{w}, G_{f}$, and the feedback controller $C$ are of corresponding dimensions.

Remark 6.1. The disturbance inputs from which the transfer function to the output is exactly known are contained in $G_{d}$ whereas noise and disturbances which are in excess of these are contained in $G_{w}$.

A residual generator, described by a proper and stable TFM $Q:=\left[\begin{array}{ll}Q_{y} & Q_{u}\end{array}\right]$, augments the closed-loop controlled system, and processes the input-output data, i.e., the known control input $u$ and the measurable output signals $y$. The residual $\varepsilon \in \mathbb{R}^{q}$ is described by

\[
\varepsilon=\left[\begin{array}{ll}
Q_{y} & Q_{u}
\end{array}\right]\left[\begin{array}{l}
y  \tag{6.3}\\
u
\end{array}\right],
\]

and serves for decision making on the presence or absence of faults. The TFM $Q:=\left[\begin{array}{ll}Q_{y} & Q_{u}\end{array}\right]$ is referred to as the implementation representation.

For nominal systems, under the assumption of a perfect model, the open-loop system may be employed for fault diagnosis filter synthesis [37]. Instead of the closed-loop relations (6.1) and (6.2), consider the open-loop output relation


\begin{equation*}
y=G_{u} u+G_{d} d+G_{w} w+G_{f} f \tag{6.4}
\end{equation*}


\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-151}
\captionsetup{labelformat=empty}
\caption{Figure 6.1: Closed-loop controlled system equipped with a residual generator, highlighted in ( $\square$ ). The open-loop system which can equivalently be considered for fault diagnosis filter synthesis under the assumption of a perfect model is highlighted in ( $\square$ ).}
\end{center}
\end{figure}

Substitution of the open-loop relation (6.4) into (6.3) gives

\[
\varepsilon=\left[\begin{array}{ll}
Q_{y} & Q_{u}
\end{array}\right]\left[\begin{array}{cccc}
G_{u} & G_{d} & G_{w} & G_{f}  \tag{6.5}\\
I_{n_{u}} & 0 & 0 & 0
\end{array}\right]\left[\begin{array}{c}
u \\
d \\
w \\
f
\end{array}\right]=R\left[\begin{array}{c}
u \\
d \\
w \\
f
\end{array}\right] .
\]

This is referred to as the internal representation where $R:=\left[\begin{array}{llll}R_{u} & R_{d} & R_{w} & R_{f}\end{array}\right]$ and

\[
\left[\begin{array}{l|l|l|l}
R_{u} & R_{d} & R_{w} & R_{f}
\end{array}\right]:=\left[\begin{array}{ll}
Q_{y} & Q_{u}
\end{array}\right]\left[\begin{array}{c|c|c|c}
G_{u} & G_{d} & G_{w} & G_{f}  \tag{6.6}\\
I_{n_{u}} & 0 & 0 & 0
\end{array}\right] .
\]

The column of the fault-to-residual TFM, corresponding to fault $f_{j}$, is denoted by $R_{f_{j}}$ where $j=1, \ldots, n_{f}$. With a properly designed residual generator $Q$, the internal representation is proper and stable and fulfills specific properties as described next.

\subsection*{6.2.1 Approximate fault detection problem (AFDP)}
An effectively designed fault detection filter $Q$, solving the approximate fault detection problem, has a proper and stable internal representation that achieves the following requirements:\\
а) $R_{u}=0$,\\
b) $R_{d}=0$,\\
c) $R_{w} \approx 0$, with $\operatorname{Re}\left(\lambda\left(R_{w}\right)\right)<0$,\\
d) $R_{f_{j}} \neq 0, j=1, \ldots, n_{f}$, with $\operatorname{Re}\left(\lambda\left(R_{f}\right)\right)<0$,\\
where $\operatorname{Re}(\lambda(\cdot))$ denotes the real part of the poles. The first requirements imply full decoupling of the control input $u$ and disturbance $d$. In addition, it is desired to simultaneously minimize the TFM from $w$ to $\varepsilon$ while having the TFM from $f$ to $\varepsilon$ significantly nonzero. The requirements c) and d) are formalized as part of an optimization problem in Section 6.3.

Remark 6.2. In the closed-loop formulation, it is desired that $\varepsilon$ is invariant to the external reference $r$, whereas in the open-loop formulation $\varepsilon$ is desired to be decoupled from the input $u$. In case there are no modeling errors, decoupling of u leads to decoupling of $r$ [37].

Consider the following theorems which formalize that the AFDP is solvable if the system is completely fault detectable.

Theorem 6.3. The system (6.4) is completely fault detectable if and only if $\operatorname{rank}\left[\begin{array}{ll}G_{f_{j}} & G_{d}\end{array}\right]>\operatorname{rank} G_{d}$ for $j=1, \ldots, n_{f}$, where $G_{f_{j}}$ denotes the $j$ th column of $G_{f}$.

Proof. The proof is provided in [264, Theorem 3.2].\\
Theorem 6.4. For the system (6.4) the $A F D P$ is solvable if and only if the system (6.4) is completely fault detectable.

Proof. The proof is obtained by merging the proofs of Theorems 3.2, 3.7, and 3.9 from [264].

\subsection*{6.2.2 Approximate fault detection and isolation problem (AFDIP)}
To effectively isolate faults, a dedicated structure is enforced to the residual signals, aiming to ensure that the residual $\varepsilon_{i}$ is influenced by the fault $f_{j}$. To this end, consider the $q$-dimensional residual vector $\varepsilon$ which is structured as a bank of filters $Q^{(1)}$ to $Q^{(q)}$ as

\[
Q=\left[\begin{array}{c}
Q^{(1)}  \tag{6.7}\\
\vdots \\
Q^{(q)}
\end{array}\right] .
\]

The corresponding fault-to-residual TFM is similarly structured as

\[
R_{f}=\left[\begin{array}{ccc}
R_{f_{1}}^{(1)} & \ldots & R_{f_{n_{f}}}^{(1)}  \tag{6.8}\\
\vdots & \ddots & \vdots \\
R_{f_{1}}^{(q)} & \ldots & R_{f_{n_{f}}}^{(q)}
\end{array}\right] .
\]

let $S_{R_{f}}$ be the corresponding $q \times n_{f}$ matrix which defines the structure of $R_{f}$ as

\[
\begin{array}{lll}
S_{R_{f}}(i, j)=1 & \text { if } & R_{f_{j}}^{(i)} \neq 0 \\
S_{R_{f}}(i, j)=0 & \text { if } & R_{f_{j}}^{(i)}=0 \tag{6.9b}
\end{array}
\]

The approximate fault detection and isolation problem (AFDIP) requires to determine for a given structure matrix $S$, a stable and proper filter $Q$ such that the following condition\\
e) $S_{R_{f}}=S$,\\
is additionally fulfilled.\\[0pt]
According to the nomenclature of [94], for a desired structure matrix $S$ the $i^{\text {th }}$ row is referred to as the $i^{\text {th }}$ specification, while the $j^{\text {th }}$ column is known as the $j^{\text {th }}$ fault signature. The specifications are primarily used for synthesis, whereas the fault signatures are mainly utilized for isolating specific faults. This is achieved by comparing the signature of fired (non-zero) residuals with those encoded in the columns of $S$.

Remark 6.5. The achievable structure matrices for the system 6.4 can be determined a priori using the algorithm proposed in [260].

Consider the following theorems which formalize that the AFDIP is solvable if the system is $S$-fault isolable.

Theorem 6.6. For a given $q \times n_{f}$ structure matrix $S$, the system (6.4) is $S$-fault isolable if and only if for $i=1, \ldots, q$

\[
\operatorname{rank}\left[\begin{array}{lll}
G_{d} & \hat{G}_{d}^{(i)} & G_{f_{j}}
\end{array}\right]>\operatorname{rank}\left[\begin{array}{ll}
G_{d} & \hat{G}_{d}^{(i)} \tag{6.10}
\end{array}\right], \forall j, S_{i j} \neq 0
\]

where $\hat{G}_{d}^{(i)}$ is formed from the columns $G_{f_{j}}$ of $G_{f}$ for which $S_{i j}=0$.\\[0pt]
Proof. The proof is provided in [264, Theorem 3.5].\\
Theorem 6.7. For a given structure matrix $S$, the AFDIP is solvable if and only if the system (6.4) is $S$-fault isolable.

Proof. The proof is obtained by merging the proofs of Theorems 3.10 and 3.10 from [264].

Ideally, an arbitrary number of faults are isolated simultaneously. This concept is defined by strong fault isolability, for which the structure $S=I_{n_{f}}$ is enforceable.

Theorem 6.8. The system (6.4) is strongly fault isolable if and only if

\[
\operatorname{rank}\left[\begin{array}{ll}
G_{d} & G_{f} \tag{6.11}
\end{array}\right]=\operatorname{rank} G_{d}+n_{f}
\]

Proof. The proof is provided in [264, Theorem 3.6].\\
Often, strong isolation is not realizable, particularly when the number of sensors is low compared to the number of possible faults. In such scenarios, weak fault isolation may still be achievable due to its less stringent requirements. This approach involves enforcing a structure matrix $S$ with pairwise distinct columns. The least restrictive structure that still allows to isolate all faults is a hollow structure, represented as $S=J_{n_{f}}-I_{n_{f}}$, where $J$ is a matrix filled with ones. In this case, successful fault isolation is only possible when a single fault occurs at a time.

Corollary 6.9. For a given $n_{f} \times n_{f}$ hollow structure matrix $S=J_{n_{f}}-I_{n_{f}}$, the system (6.4) is weakly isolable if and only if for $i, j=1, \ldots, n_{f}$

\[
\operatorname{rank}\left[\begin{array}{lll}
G_{d} & G_{f_{i}} & G_{f_{j}}
\end{array}\right]>\operatorname{rank}\left[\begin{array}{ll}
G_{d} & G_{f_{i}} \tag{6.12}
\end{array}\right], \forall i, j, i \neq j .
\]

Proof. The proof follows directly from Theorem 6.6. For each $i=1, \ldots, q$, the structure matrix $S_{i j}=0$ if $i=j$ and $S_{i j}=1$ if $i \neq j$. From this it follows that for each $i, \hat{G}_{d}^{(i)}$ is formed by $\hat{G}_{d}^{(i)}=G_{f_{i}}$, which directly yields (6.12).

Next, the solution to the approximate fault detection and isolation problem is provided, after which particular special cases are examined.

\subsection*{6.3 Solution to the AFDIP}
In order to synthesize a fault detection and isolation filter that solves the AFDIP for an $S$-fault isolable system according to Theorem 6.6, each filter $i=1, \ldots, q$ in the bank of filter (6.7), is factorized as


\begin{equation*}
Q^{(i)}=Q_{3}^{(i)} Q_{2}^{(i)} Q_{1}^{(i)} . \tag{6.13}
\end{equation*}


Each factor is interpreted as a partial synthesis result addressing specific requirements. First, for a specific $i$, define a new fault input $\hat{f}^{(i)}$ that contains the components $f_{j}$ for which $S_{i j}=1$. Define $\hat{G}_{f}^{(i)}$ as the TFM formed by the columns $G_{f_{j}}$ for which $S_{i j}=1$. Now consider $Q^{(i)}=\bar{Q}_{1}^{(i)} Q_{1}^{(i)}$, where $\bar{Q}_{1}^{(i)}=Q_{3}^{(i)} Q_{2}^{(i)}$ and $Q_{1}^{(i)}$ is the first partial synthesis result which forms a proper left rational nullspace basis satisfying


\begin{equation*}
Q_{1}^{(i)} G^{(i)}=0, \tag{6.14}
\end{equation*}


where

\[
G^{(i)}:=\left[\begin{array}{ccc}
G_{u} & G_{d} & \hat{G}_{d}^{(i)}  \tag{6.15}\\
I_{n_{u}} & 0 & 0
\end{array}\right] .
\]

The residual that remains can be written as


\begin{equation*}
\varepsilon_{i}=\bar{Q}_{1}^{(i)} \bar{G}_{f}^{(i)} \hat{f}^{(i)}+\bar{Q}_{1}^{(i)} \bar{G}_{w}^{(i)} w, \tag{6.16}
\end{equation*}


where $\bar{G}_{f}^{(i)}:=Q_{1}^{(i)}\left[\begin{array}{c}\hat{G}_{f}^{(i)} \\ 0\end{array}\right]$ and $\bar{G}_{w}^{(i)}:=Q_{1}^{(i)}\left[\begin{array}{c}G_{w} \\ 0\end{array}\right]$. The factor $Q_{1}^{(i)}$ is computed using the approach in [261], ensuring that $\bar{Q}_{1}^{(i)} \bar{G}_{f}$ and $\bar{Q}_{1}^{(i)} \bar{G}_{w}$ are proper and stable. Computing $Q_{1}^{(i)}$ for all $i=1, \ldots, q$ suffices to satisfy requirements a), b), and e). The resulting $\bar{Q}_{1}^{(i)}$ can be factored as $\bar{Q}_{1}^{(i)}=Q_{3}^{(i)} Q_{2}^{(i)}$, where $Q_{2}^{(i)}$ is a rational vector to construct a linear combination of the basis vectors of $Q_{1}^{(i)}$ which can be chosen such that $Q_{2}^{(i)} Q_{1}^{(i)}$ is least possible McMillan degree. To this end, minimum dynamic cover algorithms are deployed [148, 259]. Alternatively, a different linear combination of basis vectors can be chosen via $Q_{2}^{(i)}$.

Next, $Q_{3}^{(i)}$ is determined to maximize the fault-to-noise gap $\eta:=\frac{\beta}{\gamma}$. An optimization-based approach is used to achieve the largest gap between fault detectability and noise attenuation. Let $\gamma>0$ be an admissible level for the influence of $w$ on $\varepsilon_{i}$. An optimization problem to minimize the effect of $w$ and maximize the effect of $\hat{f}^{(i)}$ is posed as follows. Given $\gamma>0$, determine $\beta>0$ and a stable and proper fault detection filter $Q_{3}^{(i)}$ such that


\begin{equation*}
\beta=\max _{Q_{3}^{(i)}}\left\{\left\|Q_{3}^{(i)} Q_{2}^{(i)} \bar{G}_{f}\right\|_{\infty-} \mid\left\|Q_{3}^{(i)} Q_{2}^{(i)} \bar{G}_{w}\right\|_{\infty} \leq \gamma\right\} \tag{6.17}
\end{equation*}


where


\begin{equation*}
\left\|Q_{3}^{(i)} Q_{2}^{(i)} \bar{G}_{f}\right\|_{\infty-}:=\min _{1 \leq i \leq q}\left\|Q_{3}^{(i)} Q_{2}^{(i)} \bar{G}_{f}\right\|_{\infty} \tag{6.18}
\end{equation*}


This optimization problem is solved by determining $Q_{3}^{(i)}$ from a co-inner-outer factorization, $Q_{2}^{(i)} \bar{G}_{f}=G_{w o} G_{w i}$, where $G_{w o}$ is an invertible TFM which has only stable zeros and $G_{w i}$ is co-inner (i.e., $G_{w i} G_{w i}^{H}=I$ where $H$ denotes the Hermitian transpose). Setting $Q_{3}^{(i)}=\gamma G_{w o}^{-1}$ yields the optimal solution to (6.17). Computing this outer factor generally involves solving just a single Riccati equation [101, 157].

Remark 6.10. The conditions of the AFDIP can be relaxed by replacing requirement e) such that the TFMs $R_{f_{j}}^{(i)}$ corresponding to $S_{i j}=0$ are not required to be exactly zero, i.e., $R_{f_{j}}^{(i)}=0$, but should be small $R_{f_{j}}^{(i)} \approx 0$. To this end, $\hat{G}_{d}^{(i)}$ is considered as part of $G_{w}$ which includes its contribution in the optimization problem (6.17), instead of enforcing its contribution to zero via (6.14).

Next, several special cases are considered as well as its implications relating to the solvability of the AFDIP.

\subsection*{6.4 Design for actuator and sensor faults}
In this section, thee common cases are considered, each having a distinct set of faults. First, solely actuator faults are considered. Next, solely sensor faults, and finally, a combination of both is considered. For each case, the solvability requirements for the AFDIP are translated into admissible structure matrices $S$ as well as its consequences for fault diagnosis system design.

To uncover the fundamental limitations, $G_{d}$ is assumed to be empty for the remainder of this section. This assumption is made because these disturbances can alternatively be captured in $G_{w}$, thereby removing them from the solvability conditions and addressing them through the optimization problem (6.17), see Remark 6.1 [264].

Corollary 6.11. For a given $q \times n_{f}$ structure matrix $S$ and $n_{u}=0$, the model is $S$-fault isolable if and only if for $i=1, \ldots, q$


\begin{equation*}
\operatorname{rank}\left[\hat{G}_{d}^{(i)} \quad G_{f_{j}}\right]>\operatorname{rank}\left[\hat{G}_{d}^{(i)}\right], \forall j, S_{i j} \neq 0, \tag{6.19}
\end{equation*}


where $\hat{G}_{d}^{(i)}$ is formed from the columns $G_{f_{j}}$ of $G_{f}$ for which $S_{i j}=0$.\\
Proof. The proof follows directly from Theorem 6.6 by setting $G_{d}=\{ \}$.

\subsection*{6.4.1 Actuator faults}
Let the system described by (6.4), be subjected to actuator faults, i.e., with $n_{f}=n_{u}$, and consider the $n_{y} \times n_{u}$ fault model $G_{f}^{\text {act }}=G_{u}$, without loss of generality factorized as

\[
G_{f}^{\mathrm{act}}(s)=\frac{1}{d(s)}\left[\begin{array}{ccc}
N_{11}(s) & \ldots & N_{1 n_{u}}(s)  \tag{6.20}\\
\vdots & \ddots & \vdots \\
N_{n_{y}}(s) & \ddots & N_{n_{y} n_{u}}(s)
\end{array}\right]
\]

with $d(s)$ a common denominator polynomial and $N(s)$ matrix with the numerator polynomials.

Corollary 6.12. A system with only actuator faults is strongly fault isolable, i.e., $S=I_{n_{f}}$ is achievable, if and only if all fault columns in $G_{f}$ are linearly independent and $n_{y} \geq n_{u}$.

Proof. The proof follows directly from Theorem 6.8. Setting $G_{d}=\{ \}$ implies that the system is strongly fault isolable if and only if rank $G_{f}=n_{f}$. To achieve this, $G_{f}$ must be full column rank with atleast as many sensors as faults, i.e., $n_{y} \geq n_{f}=n_{u}$.

Hence, the number of sensors must be larger or equal to the number of actuators which can become faulty. Additionally, $G_{f}$ must be full column rank. Often $n_{y} \not \geq n_{u}$. To this end, consider the most relaxed isolation requirement.

Corollary 6.13. A system with only actuator faults is weakly isolable, where $S=J_{n_{f}}-I_{n_{f}}$, if and only if all fault columns $G_{f_{j}}$ with $j=1, \ldots n_{f}$ are pairwise independent and $n_{y} \geq 2$.

Proof. The proof follows from Theorem 6.6. First set $G_{d}=\{ \}$. Now, consider (6.10), where $\hat{G}_{d}^{(i)}$ is formed by the columns $G_{f_{j}}$ of $G_{f}$ for which $S_{i j}=0$. In this case for every $i=1, \ldots, q$, only $S_{i i}=0$, which means that for every row (6.10) results in testing

$$
\operatorname{rank}\left[\begin{array}{ll}
G_{f_{i}} & G_{f_{j}}
\end{array}\right]>\operatorname{rank}\left[G_{f_{i}}\right]=1 \quad \forall j \neq i .
$$

The latter is equivalent to verifying whether all fault columns $G_{f_{j}}$ with $j= 1, \ldots n_{f}$ are pairwise independent and $n_{y} \geq 2$.

Hence, to be able to isolate all faults with the least restrictive structure, only pairwise independence is required. At least two sensors are required to isolate an arbitrary number of actuator faults. Note that the case with a single actuator $n_{u}=1$ is captured in Corollary 6.12, where $S=1$, and thus $n_{y} \geq 1$ suffices.

Example 6.14. Consider a system with two sensors and five actuators that are prone to faults. Hence, $p=2$ and $n_{u}=n_{f}=5$. Such system is never strongly fault isolable since $n_{u}>n_{y}$, i.e., $S=I_{n_{f}}$ is not achievable. However, the system is weakly fault isolable with $S=J_{n_{f}}-I_{n_{f}}$ if and only if all fault columns $G_{f_{j}}$ are pairwise independent.

\subsection*{6.4.2 Sensor faults}
Next, let the system described by (6.4), be subjected to sensor faults, i.e., with $n_{f}=n_{y}$, and consider the $n_{y} \times n_{y}$ fault model

\[
G_{f}^{\mathrm{sens}}(s)=\left[\begin{array}{ccc}
1 & \cdots & 0  \tag{6.21}\\
\vdots & \ddots & \vdots \\
0 & \ddots & 1
\end{array}\right]
\]

Corollary 6.15. A system with only sensor faults is inherently strongly fault isolable.

Proof. The proof follows directly from Theorem 6.8. Setting $G_{d}=\{ \}$ implies that the system is strongly fault isolable if and only if rank $G_{f}=n_{f}$. Since $G_{f}=I_{n_{f}}$, this condition always holds true.

Note that the columns of $G_{f}^{\text {sens }}(s)$ are inherently linearly independent. There are no additional requirements on the number of actuators. Evidently, if a system is strongly fault isolable, a less stringent structure $S$ is also viable such as the hollow structure from Corollary 6.9.

\subsection*{6.4.3 Actuator and sensor faults}
Finally, let the system described by (6.4), be subjected to actuator and sensor faults, i.e., with $n_{f}=n_{u}+n_{y}$, and consider the $n_{y} \times\left(n_{u}+n_{y}\right)$ fault model $G_{f}=\left[\begin{array}{ll}G_{f}^{\text {act }}(s) & G_{f}^{\text {sens }}(s)\end{array}\right]$, without loss of generality factorized as

\[
G_{f}(s)=\frac{1}{d(s)}\left[\begin{array}{cccccc}
N_{11}(s) & \ldots & N_{1 n_{u}}(s) & d(s) & \ldots & 0  \tag{6.22}\\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
N_{n_{y}}(s) & \ddots & N_{n_{y} n_{u}}(s) & 0 & \ldots & d(s)
\end{array}\right]
\]

Corollary 6.16. A system with sensor and actuator faults is never strongly fault isolable.

Proof. This follows directly from Theorem 6.8 with $n_{f}=n_{u}+n_{y}$. Setting $G_{d}= \left\}\right.$ shows that the system is strongly fault isolable if and only if rank $G_{f}=n_{f}$. Since, $\operatorname{rank} G_{f} \leq n_{y}$ and $n_{f}=n_{u}+n_{y}$, a system with all actuators and sensors prone to faults is never strongly fault isolable.

Corollary 6.17. A system with all sensor sand actuators prone to faults is weakly isolable with $S=J_{n_{f}}-I_{n_{f}}$ if and only if all fault columns $G_{f_{j}}$ with $j=1, \ldots n_{f}$ are pairwise independent and $n_{y} \geq 2$.

Proof. The proof follows from Theorem 6.6. First set $G_{d}=\{ \}$. Now, consider (6.10), where $\hat{G}_{d}^{(i)}$ is formed by the columns $G_{f_{j}}$ of $G_{f}$ for which $S_{i j}=0$. In this case for every $i=1, \ldots, q$, only $S_{i i}=0$, which means that for every row (6.10) results in testing

\[
\operatorname{rank}\left[\begin{array}{ll}
G_{f_{i}} & G_{f_{j}} \tag{6.23}
\end{array}\right]>\operatorname{rank}\left[G_{f_{i}}\right]=1 \quad \forall j \neq i
\]

The latter is equivalent to verifying whether all fault columns $G_{f_{j}}$ with $j= 1, \ldots n_{f}$ are pairwise independent and $n_{y} \geq 2$.

Hence, to be able to isolate all faults with the least restrictive structure, only pairwise independence is required. At least two sensors are required to isolate an arbitrary number of faults. Note that the case with a single sensor $n_{y}=1$ never suffices as both sides of the inequality 6.23 are rank 1 .

Example 6.18. A system with one sensor and $n_{u}$ actuators which all can go faulty is never fault isolable. Neither $S=I_{n_{f}}$ nor $S=J_{n_{f}}-I_{n_{f}}$ is achievable, as the columns in $G_{f}$ are always pairwise dependent.

Example 6.19. A system with multiple sensors and multiple actuators, e.g., $n_{y}=3, n_{u}=3$, which all can go faulty is fault isolable if and only if the columns $G_{f_{j}}$ with $j=n_{y}+n_{u}=1, \ldots, 6$ are pairwise independent.

In case the AFDIP is infeasible, analysis such as presented in this section may give insight where to place additional sensors to be able to solve the AFDIP. Next, the proposed approach is applied to a large-scale high-precision mechatronic system with 17 distinct actuator and sensor faults.

\subsection*{6.5 Experimental results}
In this section, the presented approach is validated using a prototype wafer stage from the semiconductor industry. The system is equipped with 4 sensors and 13 actuators, which all are assumed to be prone to faults, hence, the analysis presented in Section 6.4.3 applies. First, the system and its control algorithm are introduced. Following this, an accurate model of the system is presented. Subsequently, a tailored FDI filter design is synthesized to demonstrate the effectiveness of the fault diagnosis approach. The primary objective is to detect and isolate faults in each actuator and sensor.

\subsection*{6.5.1 Prototype experimental wafer stage and control algorithm}
Consider an overview of the prototype wafer stage depicted in Figure 6.2. A close-up of the force frame and the bottom of the stage are depicted in Figure 6.3 and Figure 6.4. The stage is the only moving part in the setup and is suspended by gravity compensators to reduce the required actuator forces. For this experimental case study only the actuators and sensors in the out-of-plane direction are considered. In this direction, the position of the stage is measured by four linear encoders with nanometer resolution of which one is indicated in Figure 6.2. Two of these encoders are visible in Figure 6.3. The system is actuated in this direction by 13 Lorentz acuators. The 13 coils are visible in Figure 6.3 and the interface on the chuck is shown in Figure 6.4.

The system is controlled in all six degrees of freedom (DOFs) by a decentralized PID controller. To this end, the system is decoupled using input and output transformation matrices $T_{u}$ and $T_{y}$ respectively, see Figure 6.5. Each DOF is controlled by a dedicated PID controller in $C$ operating at a sampling frequency of 10 kHz . The system follows a smooth $4^{\text {th }}$-order setpoint with a stroke of $100 \mu \mathrm{~m}$ at a frequency of 1 Hz in the out-of-plane direction. The other 2 translational DOFs and 3 rotational DOFs are regulated to zero. The main aim is to detect and isolate faults in all the 13 out-of-plane actuators and the 4 out-of-plane sensors.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-160}
\captionsetup{labelformat=empty}
\caption{Figure 6.2: Prototype experimental wafer stage setup. The moving part, the chuck, is indicated by (a) and is suspended by gravitaty compensators on the force frame (b) which is on top of the base frame (c). Currently, the chuck and force frame are slid out of the machine, whereas during operation, it is underneath the metroframe (d). The metroframe is isolated from the fixed world through air mounts (e). There are four Lorentz actuators in the horizontal plane of which one is indicated by (f). These actuators apply a tangential force to the chuck. The actuators in the vertical plane are positioned between the chuck and the force frame. The position of the chuck is measured in the horizontal plane by means of capacitive sensors and measured in the vertical plane by means of linear encoders. The chuck has four scales (g) measured by the encoders (h) on the metroframe.}
\end{center}
\end{figure}

\section*{6}
\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-160(1)}
\captionsetup{labelformat=empty}
\caption{Figure 6.3: Close-up of the force frame underneath the chuck. The interface of the gravitaty compensator to the force frame is indicted by (a). One of the thirteen Lorentz actuators that move the chuck in vertical direction is indicated by (b). The position of one of the in-plane actuators is indicated by (c).}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-161}
\captionsetup{labelformat=empty}
\caption{Figure 6.4: Close-up of the bottom of the chuck. The interface of the gravitaty compensator to the chuck is indicated by (a) and the interface of the Lorentz actuator is indicated by (b). The interface for the in-plane actuators is indicted by (c).}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-161(1)}
\captionsetup{labelformat=empty}
\caption{Figure 6.5: Block diagram of the closed-loop modal controlled prototype wafer stage system. The 13 actuator and 4 sensor faults are indicted by \$f\_\{1}\$ to $f_{17}$ respectively. The augmented residual generator is highlighted in ( $\square$ ).\}\end{center}
\end{figure}

\subsection*{6.5.2 System identification}
A closed-loop multisine identification experiment is performed to obtain a best linear approximation (BLA) using the robust method [202]. This frequency response function (FRF), is used to fit a modal parametric modal model of order 20, considered as $G_{u}$. To this end, a novel modal identification algorithm is used, based on the simplified refined instrumental variable method (SRIVC) and integrated prediction error minimization (IPEM). The algorithm, including detailed analysis, will be published elsewhere. This algorithm is a MIMO frequency domain extension of [112], see [46] for details regarding the FRF identification.

The 20th-order model, $\hat{G}_{u}:\left[\begin{array}{lll}u_{1} & \ldots & u_{13}\end{array}\right]^{\top} \rightarrow\left[\begin{array}{lll}y_{1} & \ldots & y_{4}\end{array}\right]^{\top}$ is depicted in Figure 6.6 and it closely matches the measured FRF, in particular up to at least $\approx 2000 \mathrm{~Hz}$, and serves as a basis for FDI filter synthesis.

\subsection*{6.5.3 FDI Design}
The low-order model $\hat{G}_{u}$ is used to synthesize the FDI filter. The aim is to detect all possible faults, indicated in the block diagram in Figure 6.5. Transferring the actuator faults to the sensor side of the plant gives the fault model

\[
G_{f}=\left[\begin{array}{ll}
G_{f}^{\mathrm{act}} & G_{f}^{\mathrm{sens}} \tag{6.24}
\end{array}\right]
\]

where $G_{f}^{\text {act }}=G_{u}$ and $G_{f}^{\text {sens }}=I_{4}$. Since the sensors are of high quality, no disturbance and noise contributions are taken into account, hence, $G_{d}$ and $G_{w}$ are chosen to be void. The analysis in Section 6.4, in particular Corollary 6.16, shows that strong fault isolability is not achievable. Hence, a less stringent $S$ is used. In this case, an almost hollow signature matrix is used, equal to

\[
S=\left[\begin{array}{lllllllllllllllll}
0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 & 1  \tag{6.25}\\
1 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 \\
1 & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 \\
1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 1 \\
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 1 \\
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 1 \\
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 \\
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 \\
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 \\
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0
\end{array}\right] .
\]

The zeros in the top right corner of $S$ are included to decouple the sensor faults $f_{14}$ to $f_{17}$ in the first four residuals. Using this structure $S$ an approximate\\
fault detection and isolation filter is synthesized using the approach described in Section 6.3.

\subsection*{6.5.4 Experimental time-domain results}
The FDI filter is synthesized and integrated into the closed-loop control system of the wafer stage. Faults are artificially injected to the system at the positions indicted in Figure 6.5. First, each actuator fault is introduced sequentially as a step signal with magnitude $f_{1}, \ldots, f_{13}=0.1 \mathrm{~N}$. Following this, sensor faults are injected in the form of a step signal with magnitude $10 \mu \mathrm{~m}$. The first fault is injected at $t=2.5 \mathrm{~s}$ until $t=7.5 \mathrm{~s}$. Each subsquent fault is injected 5 s after the previous fault.

The absolute and normalized values of the residual signals are shown in Figure 6.7. Clearly, the faults are easily detectable and the rootcause of the fault can be observed by comparison of the triggered signals and the structure in $S$. E.g., around $t=5 \mathrm{~s}$ the residuals $\varepsilon_{2}$ to $\varepsilon_{17}$ react whereas $\varepsilon_{1} \approx 0$ resembling an additive fault at the first actuator. The obtained residual signals can easily be processed into a boolean signal indicating the location of the present fault.

The residual signals exhibit small oscillations in the fault-free case, particu- larly with a period similar to the reference. This undesired effect is attributed to a model-reality mismatch, resulting in a slight violation of the decoupling properties and causing a minor leakage of $r$ into the residual signals.

The actuator faults produce persistent residual signals in the case of persistent fault excitation. This implies an asymptotically non-vanishing residual signal in the case of persistent faults such as step or sinusoidal signal. The sensor faults lack strong fault detectability making detection slightly more difficult because its effect manifest in the residual only during the transient of the fault, thus the effect disappears in the residual over time, see [264, Chapter 3] for details. Yet, with proper processing of the residuals, all faults are detected and isolating showcasing the applicability of this approach to high-precision industrial equipment.

\subsection*{6.6 Conclusion}
This chapter presents the essential principles for fault detection and isolation for closed-loop controlled mechatronic systems. By translating generic solvability requirements, the fundamental limitations of systems with actuator and sensor faults have been uncovered. Additionally, a nullspace-based fault diagnosis system was synthesized, with experimental results demonstrating the effectiveness of the proposed method. This provides a solid foundation for applying fault diagnosis to large-scale industrial systems, such as the considered wafer stage used in the semiconductor industry.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-164}
\captionsetup{labelformat=empty}
\caption{Figure 6.6: Bode magnitude plot of the BLA in (-), measured using multisine excitation and the robust method, and the \$20\^{}\{\textbackslash text \{th }\}\$-order fit in (---), obtained using the frequency domain simplified refined instrumental variable (SRIVC) method with integrated prediction error minimization (IPEM).\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-165}
\captionsetup{labelformat=empty}
\caption{Figure 6.7: Absolute and normalized values of the residual signals \$\textbackslash varepsilon\_\{1}\$ to $\varepsilon_{17}$ (-). The faults can easily be localized using the structure in (6.25). The corresponding regions where the residuals are expected to be zero despite the presence of a fault are indicated with the vertical lines (一).\}\end{center}
\end{figure}

\section*{Chapter}
\section*{Direct Shaping of Minimum and Maximum \\
 Singular Values: An $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ Synthesis Approach for Fault Detection Filters ${ }^{1}$ }
\begin{abstract}
The performance of fault detection filters relies on a high sensitivity to faults and a low sensitivity to disturbances. The aim of this chapter is to develop an approach to directly shape these sensitivities, expressed in terms of minimum and maximum singular values. The developed method offers an alternative solution to the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ synthesis problem, building upon traditional multiobjective synthesis results. The result is an optimal filter synthesized via iterative convex optimization and the approach is particularly useful for fault diagnosis as illustrated by a numerical example.
\end{abstract}

\footnotetext{${ }^{1}$ The results in this chapter constitute Contribution VI in Section 1.6. The chapter is based on: [44] K. Classens, W. P. M. H. M. Heemels, and T. Oomen, "Direct Shaping of Minimum and Maximum Singular Values: An $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ Synthesis Approach for Fault Detection Filters," in Proceedings of the IFAC 22nd Triennial World Congress, Yokohama, Japan, 2023.
}\subsection*{7.1 Introduction}
Fault detection and isolation (FDI) is highly important in many control applications which are becoming increasingly more demanding and more complex. In particular, FDI is important for the high-tech production industry which is shifting towards predictive maintenance strategies. This paradigm shift is motivated as a result of the high costs associated with unscheduled downtime. In this context, real-time fault diagnosis of complex closed-loop controlled multi-input multi-output (MIMO) systems is the foundation for effective targeted maintenance and optimal scheduling of downtime.

It is commonly recognized that satisfactory performance of model-based FDI systems is only achievable by a balanced trade-off. Techniques based on $\mathcal{H}_{\infty}$ optimization and $\mu$ design have been developed and applied, see, e.g., [62, 216, 285]. However, these $\mathcal{H}_{\infty}$ approaches do not directly account for this trade-off as $\mathcal{H}_{\infty}$ is only a measure for maximum gain. Apart from rejecting disturbance, noise and being insensitive to model uncertainties, the fault diagnosis system needs to be as sensitive to faults as possible. Hence, fault sensitivity needs to be addressed explicitly during design.

One way to enforce sensitivity to faults is to reformulate the problem, see [125], or by reformulating the problem as a fault estimation problem, see [242]. In this way, the problem can still be solved as a fictitious $\mathcal{H}_{\infty}$ problem, however, undesired conservatism may be introduced. Alternatively, more direct $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ approaches are attractive as the trade-off is explicitly embedded in the problem formulation. $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ filter design can be pursued via factorization approaches as in [62], or, e.g., via Riccati equations, see [157]. In particular LMI formulations are of interest due to the relative ease to incorporate additional design objectives, see [134, 270]. In addition, LMI methods are well established for controller synthesis and observer design as demonstrated in [219, 220]. However, these methods are not specifically tailored to FDI problems.

Although important progress has been made in fault detection for complex engineered systems, at present accurate FDI for complex systems is hampered by lack of compatible synthesis tools. The aim of this chapter is to develop an alternative $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ synthesis algorithm, building upon traditional multiobjective synthesis results originating from controller design. The theory builds upon the notion of minimum gain and allows to directly shape the minimum and maximum singular values of the complete system. Hence, the contribution of this chapter is twofold:

C1 The development of an alternative approach, based on convex optimization with LMIs, to design fault detection and isolation filters.

C2 The development of the associated $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ synthesis problem.\\
This chapter is organized as follows. The chapter proceeds with notation and the required preliminaries, listed in Section 7.2. The underlying subproblem for\\
multiobjective filter design is presented in a generic manner in Section 7.3. Subsequently, the design specifications for fault diagnosis filter design, the relevant matrix inequalities, and its synthesis procedure are described in Section 7.4. A numerical fault diagnosis case study, presented in Section 7.5, illustrates the effectiveness of the proposed approach and finally, a conclusion is drawn in Section 7.6.

\subsection*{7.2 Preliminaries}
\subsection*{7.2.1 Notation}
Positive definiteness and positive semidefiniteness of a matrix $A$ are denoted by $A \succ 0$ and $A \succeq 0$, respectively. Similarly, $A \prec 0$ and $A \preceq 0$ denote negative definite and negative semidefinite matrices, respectively. The sets of all nonnegative and positive integers are denoted $\mathbb{N}$ and $\mathbb{N} \geq 0$. The sets of real numbers and nonnegative real numbers are indicated by $\mathbb{R}$ and $\mathbb{R} \geq 0$. The set of $n$ by $n$ symmetric matrices is denoted as $\mathbb{S}^{n}$. By $\|\cdot\|$ the Euclidean norm is defined. Repeated blocks within symmetric matrices are replaced by $*$ for brevity and clarity. The identity matrix is written as $I$ and a matrix of zeros is written as 0 . The maximum and minimum singular values of the matrix $A$ are denoted by $\bar{\sigma}(A)$ and $\underline{\sigma}(A)$, respectively. The real rational subspace of $\mathcal{H}_{\infty}$ is denoted by $\mathcal{R} \mathcal{H}_{\infty}$. $y \in \mathcal{L}_{2}$ if $\|y\|_{2}^{2}=\int_{0}^{\infty} y^{\top}(t) y(t) \mathrm{d} t<\infty . y \in \mathcal{L}_{2 e}$ if $\|y\|_{2 T}^{2}=\int_{0}^{T} y^{\top}(t) y(t) \mathrm{d} t<\infty$, $T \in \mathbb{R}_{\geq 0}$.

\subsection*{7.2.2 Minimum and maximum gain}
Lemma 7.1. Minimum gain ([157, 270]) The smallest gain of the continuoustime LTI system $G: \mathcal{L}_{2 e} \rightarrow \mathcal{L}_{2 e}$, that is the $\mathcal{H}_{-}$index, is defined as


\begin{equation*}
\|G\|_{-}=\inf _{\omega \in \mathbb{R}_{\geq 0}} \underline{\sigma}(G(j \omega)) . \tag{7.1}
\end{equation*}


The minimum gain is not a norm and therefor named the $\mathcal{H}_{-}$index.\\[0pt]
Definition 7.2. Maximum gain ([233]) The $\mathcal{H}_{\infty}$ norm of the continuous-time LTI system $G$ : $\mathcal{L}_{2 e} \rightarrow \mathcal{L}_{2 e}$, denoted as $\|G\|_{\infty}$, is given by


\begin{equation*}
\|G\|_{\infty}=\sup _{\omega \in \mathbb{R}_{\geq 0}} \bar{\sigma}(G(j \omega)) \tag{7.2}
\end{equation*}


Lemma 7.3. Minimum Gain Lemma ([28, 32]) Consider a continuous-time LTI system $G: \mathcal{L}_{2 e} \rightarrow \mathcal{L}_{2 e}$, with state space realization ( $\mathcal{A}, \mathcal{B}, \mathcal{C}, \mathcal{D}$ ), where $\mathcal{A} \in \mathbb{R}^{n \times n}, \mathcal{B} \in \mathbb{R}^{n \times m}, \mathcal{C} \in \mathbb{R}^{p \times n}$, and $\mathcal{D} \in \mathbb{R}^{p \times m}$. The system $G$ has minimum gain

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-169}
\captionsetup{labelformat=empty}
\caption{Figure 7.1: Generalized plant \$P:\textbackslash left[\textbackslash begin\{array}\{ll\}\textbackslash tilde\{w\}\^{}\{\textbackslash top\} \& \textbackslash tilde\{u\}\textsuperscript{\{\textbackslash top\}\textbackslash end\{array\}\textbackslash right]}\{\textbackslash top\} \textbackslash rightarrow\textbackslash left[\[
\begin{array}{ll}\tilde{z}^{\top} & \tilde{y}^{\top}\end{array}
\]\textbackslash right]\^{}\{\textbackslash top\}\$ and filter $Q: \tilde{y} \rightarrow \tilde{u}$. The closed-loop system with performance channel $T: \tilde{w} \rightarrow \tilde{z}$ is highlighted in (■).\}\end{center}
\end{figure}

$\nu$ under the following sufficient condition. There exists $X \in \mathbb{S}^{n}$ and $\nu \in \mathbb{R}_{\geq 0}$, where $X \succeq 0$, such that

\[
\left[\begin{array}{ccc}
X \mathcal{A}+\mathcal{A}^{\top} X-\mathcal{C}^{\top} \mathcal{C} & X \mathcal{B}-\mathcal{C}^{\top} \mathcal{D} & 0  \tag{7.3}\\
* & -\mathcal{D}^{\top} \mathcal{D} & \nu I \\
* & * & -I
\end{array}\right] \prec 0 .
\]

Lemma 7.4. Bounded Real Lemma ([83]) Consider a continuous-time LTI system $G: \mathcal{L}_{2 e} \rightarrow \mathcal{L}_{2 e}$, with state space realization ( $\mathcal{A}, \mathcal{B}, \mathcal{C}, \mathcal{D}$ ), where $\mathcal{A} \in \mathbb{R}^{n \times n}$, $\mathcal{B} \in \mathbb{R}^{n \times m}, \mathcal{C} \in \mathbb{R}^{p \times n}$, and $\mathcal{D} \in \mathbb{R}^{p \times m}$. The inequality $\|G\|_{\infty}<\gamma$ holds under the following necessary and sufficient conditions. There exists $X \in \mathbb{S}^{n}$ and $\gamma \in \mathbb{R}_{>0}$, where $X \succ 0$, such that

\[
\left[\begin{array}{ccc}
X \mathcal{A}+\mathcal{A}^{\top} X & X \mathcal{B} & \mathcal{C}^{\top}  \tag{7.4}\\
* & -\gamma^{2} I & \mathcal{D}^{\top} \\
* & * & -I
\end{array}\right] \prec 0 .
\]

The remaining and indirectly used auxiliary results are presented in Appendix 7.B.

\section*{$7.3 \mathcal{H}_{-} / \mathcal{H}_{\infty}$ approach to fault detection}
This section introduces the generalized plant formulation and introduces the relevant notation for transformation of variables and objective channel selection. In addition, the underlying multiobjective synthesis subproblem is described.

\subsection*{7.3.1 General closed-loop interconnection}
Consider the generalized plant $P$, as depicted in Figure 7.1, with generalized disturbance channel $\tilde{w}$, generalized performance channel $\tilde{z}$, input $\tilde{u}$, and output $\tilde{y}$, which admits the state-space realization

\[
\left[\begin{array}{c}
\dot{x}  \tag{7.5}\\
\hline \tilde{z} \\
\tilde{y}
\end{array}\right]=\left[\begin{array}{c|cc}
A & B_{1} & B_{2} \\
\hline C_{1} & D_{11} & D_{12} \\
C_{2} & D_{21} & D_{22}
\end{array}\right]\left[\begin{array}{c}
x \\
\hline \tilde{w} \\
\tilde{u}
\end{array}\right] .
\]

The to be designed filter $Q$ is any finite dimensional LTI system described as

\[
\left[\begin{array}{c}
\dot{x}_{c}  \tag{7.6}\\
\tilde{u}
\end{array}\right]=\left[\begin{array}{cc}
A_{c} & B_{c} \\
C_{c} & D_{c}
\end{array}\right]\left[\begin{array}{c}
x_{c} \\
\tilde{y}
\end{array}\right] .
\]

In particular, the state dimension of the filter $Q, x_{c}$, is not decided upon in advance. Let $T: \tilde{w} \rightarrow \tilde{z}$ denote the closed-loop transfer function, formed by the lower linear fractional transformation (LFT) between $P$ and $Q$. The closed-loop system $T=\mathcal{F}_{l}(P, Q)$, and admits the description


\begin{align*}
\dot{x}_{\mathrm{cl}} & =\mathcal{A} x_{\mathrm{cl}}+\mathcal{B} \tilde{w},  \tag{7.7a}\\
\tilde{z} & =\mathcal{C} x_{\mathrm{cl}}+\mathcal{D} \tilde{w}, \tag{7.7b}
\end{align*}


where


\begin{align*}
\mathcal{A} & =\left[\begin{array}{cc}
A+B_{2} D_{c} \bar{D} C_{2} & B_{2}\left(I+D_{c} \bar{D} D_{22}\right) C_{c} \\
B_{c} \bar{D} C_{2} & A_{c}+B_{c} \bar{D} D_{22} C_{c}
\end{array}\right]  \tag{7.8a}\\
\mathcal{B} & =\left[\begin{array}{c}
B_{1}+B_{2} D_{c} \bar{D} D_{21} \\
B_{c} \bar{D} D_{21}
\end{array}\right]  \tag{7.8b}\\
\mathcal{C} & =\left[\begin{array}{ll}
C_{1}+D_{12} D_{c} \bar{D} C_{2} & D_{12}\left(I+D_{c} \bar{D} D_{22}\right) C_{c}
\end{array}\right]  \tag{7.8c}\\
\mathcal{D} & =D_{11}+D_{12} D_{c} \bar{D} D_{21} \tag{7.8d}
\end{align*}


where $\bar{D}=\left(I-D_{22} D_{c}\right)^{-1}$.

\subsection*{7.3.2 Change of variables}
The system is nonlinear in $A_{c}, B_{c}, C_{c}$, and $D_{c}$. To obtain an affine relation, the following change of variables is deployed


\begin{align*}
A_{c 2} & =A_{c}+B_{c} \bar{D} D_{22} C_{c}  \tag{7.9a}\\
B_{c 2} & =B_{c} \bar{D}  \tag{7.9b}\\
C_{c 2} & =\left(I+D_{c} \bar{D} D_{22}\right) C_{c}  \tag{7.9c}\\
D_{c 2} & =D_{c} \bar{D} \tag{7.9d}
\end{align*}


which renders the closed-loop system matrices


\begin{align*}
\mathcal{A} & =\left[\begin{array}{cc}
A+B_{2} D_{c 2} C_{2} & B_{2} C_{c 2} \\
B_{c 2} C_{2} & A_{c 2}
\end{array}\right]  \tag{7.10a}\\
\mathcal{B} & =\left[\begin{array}{c}
B_{1}+B_{2} D_{c 2} D_{21} \\
B_{c 2} D_{21}
\end{array}\right]  \tag{7.10b}\\
\mathcal{C} & =\left[\begin{array}{ll}
C_{1}+D_{12} D_{c 2} C_{2} & D_{12} C_{c 2}
\end{array}\right]  \tag{7.10c}\\
\mathcal{D} & =D_{11}+D_{12} D_{c 2} D_{21} \tag{7.10d}
\end{align*}


which are affine in $A_{c 2}, B_{c 2}, C_{c 2}$, and $D_{c 2}$. The reverse change of variables is


\begin{align*}
& A_{c}=A_{c 2}-B_{c 2} D_{22}\left(I+D_{c 2} D_{22}\right)^{-1} C_{c 2},  \tag{7.11a}\\
& B_{c}=B_{c 2}\left(I+D_{22} D_{c 2}\right)^{-1},  \tag{7.11b}\\
& C_{c}=\left(I+D_{c 2} D_{22}\right)^{-1} C_{c 2},  \tag{7.11c}\\
& D_{c}=\left(I+D_{c 2} D_{22}\right)^{-1} D_{c 2} . \tag{7.11d}
\end{align*}


To reconstruct the filter, the change of variables must be invertible, i.e., ( $I+D_{c 2} D_{22}$ ) must be nonsingular.

\subsection*{7.3.3 Selecting channels and imposing objectives}
The objective is to compute a dynamical filter that meets various specifications on the closed-loop system behavior $T: \tilde{w} \rightarrow \tilde{z}$. Typically, these specifications are defined for particular channels $T_{j}: \tilde{w}_{j} \rightarrow \tilde{z}_{j}$ or combinations of channels. Specification $j$ of the objective is formulated relative to the closed-loop transfer function of the form


\begin{equation*}
T_{j}=L_{j} T R_{j} \tag{7.12}
\end{equation*}


where the matrices $L_{j}, R_{j}$ select the appropriate input/output (I/O) channels or channel combinations. I.e., this merely boils down to $w=R_{j} w_{j}$ and $z_{j}=L_{j} z$. Hence, $T_{j}$ admits the realization


\begin{align*}
\mathcal{A} & =\left[\begin{array}{cc}
A+B_{2} D_{c 2} C_{2} & B_{2} C_{c 2} \\
B_{c 2} C_{2} & A_{c 2}
\end{array}\right]  \tag{7.13a}\\
\mathcal{B}_{j} & =\left[\begin{array}{c}
B_{1, j}+B_{2} D_{c 2} D_{21, j} \\
B_{c 2} D_{21, j}
\end{array}\right]  \tag{7.13b}\\
\mathcal{C}_{j} & =\left[\begin{array}{ll}
C_{1, j}+D_{12, j} D_{c 2} C_{2} & D_{12, j} C_{c 2}
\end{array}\right]  \tag{7.13c}\\
\mathcal{D}_{j} & =D_{11, j}+D_{12, j} D_{c 2} D_{21, j} \tag{7.13d}
\end{align*}


where $B_{1, j}:=B_{1} R_{j}, C_{1, j}:=L_{j} C_{1}, D_{11, j}:=L_{j} D_{11} R_{j}, D_{12, j}:=L_{j} D_{12}$, and $D_{21, j}:=D_{21} R_{j}$.

The LMI approach expresses each specification or objective as a constraint on the closed-loop transfer functions $T_{j}(s)$ with a realization described by (7.13). Various objectives can be imposed on the isolated channels, see [219] for details. In particular, the minimum and a maximum gain constraint are imposed on the to be selected channels for fault diagnosis.

\subsection*{7.4 Design specifications and synthesis for fault diagnosis}
Next, the fault diagnosis problem is considered. To this end, the setting depicted in Figure 7.1 is considered, with a generalized disturbance channel consisting of\\
disturbances $\tilde{d}$ and faults $\tilde{f}$, i.e., $\tilde{w}=\left[\begin{array}{cc}\tilde{d}^{\top} & \tilde{f}^{\top}\end{array}\right]^{\top}$. The generalized performance channel consists of the residual, i.e., $\tilde{z}=\varepsilon$. First, the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ specifications are defined. Subsequently, weighting filters are introduced for direct shaping of the singular values and it is illustrated how to deal with strictly proper systems. Next, the problem is posed as an optimization problem and the solution is presented in terms of matrix inequalities. Finally, the synthesis procedure is outlined.

Definition 7.5. Consider the system (7.5) and $\gamma>0, \nu>0$. The fault diagnosis filter $Q$ is said to satisfy $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ specifications if

\begin{enumerate}
  \item $Q$ is proper and asymptotically stable;
  \item $\left\|T_{\epsilon \tilde{f}}(s)\right\|_{-}>\nu$;
  \item $\left\|T_{\epsilon \tilde{d}}(s)\right\|_{\infty}<\gamma$.
\end{enumerate}

The objective considered in this chapter is to find an admissible residual generator $Q$ which minimizes the sensitivity to disturbance $\gamma$, while simultaneously maximizing the sensitivity to faults $\nu$.

Various mixed $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ performance criteria can be considered, see, e.g., [62, 125, 270]. It is clear that better performance is achieved when the gap $\mathcal{J}:=\frac{\nu}{\gamma}$ increases. In this chapter, the ratio $\mathcal{J}$ is indirectly maximized through maximizing $\nu$, while constraining $\gamma$. Hence, the formal optimization problem is defined as


\begin{align*}
& \max _{Q \in \mathcal{R} \mathcal{H}_{\infty}, X \succ 0, \nu>0, \gamma=\gamma_{0}} \nu  \tag{7.14}\\
& \text { subject to } \quad(7.3),(7.4),
\end{align*}


where the maximum disturbance gain $\gamma=\gamma_{0}$ is set.\\
Remark 7.6. Typically, the residual generator is connected to the system in open loop, $\tilde{z}=\tilde{u}$, which implies that the residual is directly fed through $P$. For that reason, scaling the residual generator, scales $\left\|T_{\epsilon \tilde{f}}(s)\right\|_{-}$and $\left\|T_{\epsilon \tilde{d}}(s)\right\|_{\infty}$ equally, leaving the ratio $\mathcal{J}$, and thus performance, unchanged.\\
Remark 7.7. A common Lyapunov variable $X$ in both the Bounded Real Lemma and the Minimum Gain Lemma enables to restrict the order of the resulting filter $Q$. By alleviating this constraint, additional freedom may result in a lower criterion.

Remark 7.8. Note that stability of the overall system is embedded in the Bounded Real Lemma (7.4).

The Bounded Real Lemma and Minimum Gain Lemma are both a function of the closed-loop matrices and are transformed into the inequalities for synthesis next.

\subsection*{7.4.1 Direct shaping of singular values}
Next to bounding the singular values of particular channels, the singular values can also be shaped. Consider for instance invertible shaping filters on the generalized disturbance channel $G_{w_{j}}: \tilde{w}_{j} \rightarrow w_{j}$. In particular with diagonal shaping filters $G_{f}: \tilde{f} \rightarrow f$ and $G_{d}: \tilde{d} \rightarrow d$, specification (2) and (3) can be written as $\left\|T_{\epsilon f}(s) G_{f}(s)\right\|_{-}>\nu$ and $\left\|T_{\epsilon d}(s) G_{d}(s)\right\|_{\infty}<\gamma$ and equivalently as $\left|T_{\epsilon f_{j}}(j \omega)\right|>\frac{\nu}{\left|G_{f_{j}}(j \omega)\right|} \forall \omega$ and $\left|T_{\epsilon d_{j}}(j \omega)\right|<\frac{\gamma}{\left|G_{d_{j}}(j \omega)\right|} \forall \omega$. From the latter property follows that the inverse of $G_{f}$ can be used to shape the minimum singular value of $T_{\varepsilon f}$ and the inverse of $G_{d}$ can be used to shape the maximum singular value of $T_{\varepsilon d}$.

\subsection*{7.4.2 Strictly proper systems}
The minimum gain used in specification (2) is zero for strictly proper systems, see Lemma 7.1. This results in infeasibility of (7.14). With an accommodation using shaping filters, the proposed method can still be applied at the cost of reduced fault sensitivity at higher frequencies.

Suppose that the output of the filter $Q$ directly forms the residual, i.e., $\tilde{z}=\tilde{u}$. Since $Q$ should be implementable, i.e., proper and stable, the transfer $T_{\tilde{y} \tilde{w}_{j}}$ must be proper and stable. If a weighting filter is present in the latter, e.g., $T_{\tilde{y} \tilde{w}_{j}}=T_{\tilde{y} w_{j}} G_{w_{j}}$, the transfer function can be made proper with appropriate choice of improper $G_{w_{j}}$. Since in that case $G_{w_{j}}^{-1}$ goes to zero for high frequency, fault sensitivity at higher frequencies is lost as the minimum gain constraint is relaxed at these frequencies.

\subsection*{7.4.3 Synthesis matrix inequalities}
First, the transformed result is stated, after which the transformed optimization problem is posed.

Theorem 7.9. If there exist $\nu>0, \gamma>0, X_{1}=X_{1}^{\top} \succ 0, Y_{1}=Y_{1}^{\top} \succ 0, A_{n}$, $B_{n}, C_{n}, D_{n}, \mathcal{X}, \mathcal{Y}, \mathcal{Z}$ such that the maximum gain LMI

\[
\left[\begin{array}{cccc}
M_{11} & M_{12} & M_{13} & M_{14}  \tag{7.15}\\
* & M_{22} & M_{23} & M_{24} \\
* & * & M_{33} & M_{34} \\
* & * & * & M_{44}
\end{array}\right] \prec 0,
\]

the minimum gain bilinear matrix inequality (BMI)

\[
\left[\begin{array}{cccc}
N_{11} & N_{12} & N_{13} & N_{14}  \tag{7.16}\\
* & N_{22} & N_{23} & N_{24} \\
* & * & N_{33} & N_{34} \\
* & * & * & N_{44}
\end{array}\right] \prec 0,
\]

and

\[
\left[\begin{array}{cc}
X_{1} & I  \tag{7.17}\\
* & Y_{1}
\end{array}\right] \succ 0
\]

where the entries of $M$ and $N$ are given in Appendix 7.A, hold, then $Q= \left[\begin{array}{c|c}\mathcal{A}_{c} & \mathcal{B}_{c} \\ \hline \mathcal{C}_{c} & \mathcal{D}_{c}\end{array}\right] \in \mathcal{R} \mathcal{H}_{\infty}$ exists such that $\left\|T_{\epsilon \tilde{f}}(s)\right\|_{-}>\nu$ and $\left\|T_{\epsilon \tilde{d}}(s)\right\|_{\infty}<\gamma$.

A brief outline of the proof is given below. The full derivation of the synthesis matrix inequalities is presented in Appendix 7.C and 7.D.

Proof. According to the transformation lemma, there exists a matrix completion $X_{2}, Y_{2}, X_{3}, Y_{3}$ and a half dual variable $Y_{\mathrm{CL}}$ which is full rank. With the matrix completion and by definition,


\begin{align*}
& {\left[\begin{array}{cc}
A_{c 2} & B_{c 2} \\
C_{c 2} & D_{c 2}
\end{array}\right]=\left[\begin{array}{cc}
X_{2} & X_{1} B_{2} \\
0 & I
\end{array}\right]^{-1}}  \tag{7.18}\\
& \quad\left(\left[\begin{array}{cc}
A_{n} & B_{n} \\
C_{n} & D_{n}
\end{array}\right]-\left[\begin{array}{cc}
X_{1} A Y_{1} & 0 \\
0 & 0
\end{array}\right]\right)\left[\begin{array}{cc}
Y_{2}^{\top} & 0 \\
C_{2} Y_{1} & I
\end{array}\right]^{-1}
\end{align*}


From this, the filter $Q$ can be reconstructed through the reverse change of variables (7.11). Writing (7.13) in the form of (7.11) and substitution of (7.18) gives a description of the particular isolated closed-loop channel.

Considering the maximum gain constraint first, the aim is to show that the resulting description satisfies $\left\|T_{1}(s)\right\|_{\infty}<\gamma$, where $T_{1}(s)$ denotes the transfer function corresponding to the realization $\left(\mathcal{A}, \mathcal{B}_{1}, \mathcal{C}_{1}, \mathcal{D}_{1}\right)$. Substitution of this realization into the Bounded Real Lemma and applying a congruence transformation with $\operatorname{diag}\left(Y_{\mathrm{CL}}^{\top}, I, I\right)$ gives

\[
\left[\begin{array}{ccc}
Y_{\mathrm{CL}}^{\top} \mathcal{A}^{\top} X_{\mathrm{CL}}^{\top}+X_{\mathrm{CL}} \mathcal{A} Y_{\mathrm{CL}} & X_{\mathrm{CL}} \mathcal{B}_{1} & Y_{\mathrm{CL}}^{\top} \mathcal{C}_{1}^{\top}  \tag{7.19}\\
* & -\gamma^{2} I & \mathcal{D}_{1}^{\top} \\
* & * & -I
\end{array}\right] \prec 0,
\]

where $X_{\text {CL }}=X Y_{\text {CL }}$. After substituting all the variables, it can be shown that (7.19) is equivalent to (7.15) and thus proving that indeed $\left\|T_{1}(s)\right\|_{\infty}>\gamma$.

Next, consider the minimum gain constraint. Similarly, the aim is to show that the resulting description satisfies $\left\|T_{2}(s)\right\|_{-}>\nu$, where $T_{2}(s)$ denotes the transfer function corresponding to the realization ( $\mathcal{A}, \mathcal{B}_{2}, \mathcal{C}_{2}, \mathcal{D}_{2}$ ). Substitution into the Minimum Gain Lemma after taking the Schur complement gives

\[
\left[\begin{array}{cc}
X \mathcal{A}+\mathcal{A}^{\top} X-\mathcal{C}_{2}^{\top} \mathcal{C}_{2} & X \mathcal{B}_{2}-\mathcal{C}_{2}^{\top} \mathcal{D}_{2}  \tag{7.20}\\
\mathcal{B}_{2}^{\top} X-\mathcal{D}_{2}^{\top} \mathcal{C}_{2} & \nu^{2} I-\mathcal{D}_{2}^{\top} \mathcal{D}_{2}
\end{array}\right] \prec 0,
\]

which can be written as

\[
\left[\begin{array}{cc}
X \mathcal{A}+\mathcal{A}^{\top} X & X \mathcal{B}  \tag{7.21}\\
\mathcal{B}^{\top} X & \nu^{2} I
\end{array}\right] \prec\left[\begin{array}{l}
\mathcal{C}^{\top} \\
\mathcal{D}^{\top}
\end{array}\right]\left[\begin{array}{ll}
\mathcal{C} & \mathcal{D}
\end{array}\right] .
\]

Using Youngs relation with $F=\left[\begin{array}{ll}\mathcal{C} & \mathcal{D}\end{array}\right]$ and auxiliary slack variables $G= \left[\begin{array}{ll}\mathcal{Y} & \mathcal{X}\end{array}\right]$, and thereafter the Schur complement,

\[
\left[\begin{array}{ccc}
X \mathcal{A}+\mathcal{A}^{\top} X-\mathcal{C}^{\top} \mathcal{Y}-\mathcal{Y}^{\top} \mathcal{C} & X \mathcal{B}-\mathcal{C}^{\top} \mathcal{X}-\mathcal{Y}^{\top} \mathcal{D} & \mathcal{Y}^{\top}  \tag{7.22}\\
* & \nu^{2} I-\mathcal{X}^{\top} \mathcal{D}-\mathcal{D}^{\top} \mathcal{X} & \mathcal{X}^{\top} \\
* & * & -I
\end{array}\right] \prec 0
\]

is obtained. Now applying the same congruence transformation with $\operatorname{diag}\left(Y_{\mathrm{CL}}^{\top}, I, I\right)$ and substituting all the variables, it can be shown that (7.22) is equivalent to (7.16) and thus proving that indeed $\left\|T_{2}(s)\right\|_{-}>\nu$.

See also [33, 219] for related results.\\
Remark 7.10. The minimum gain BMI can be used in conjunction with classical multiobjective LMI formulations such as generalized $\mathcal{H}_{2}$ performance, peak amplitude constraints, or regional pole constraints, since the same change of variables is employed, see [219] for details.

The transformed optimization problem is then given by

\[
\begin{array}{cl}
X_{1} \succ 0, Y_{1} \succ 0, \nu>0, \gamma=\gamma_{0}, A_{n}, B_{n}, C_{n}, D_{n}, \mathcal{X}, \mathcal{Y}, \mathcal{Z} & \nu  \tag{7.23}\\
\text { subject to } & (7.15) \text { to }(7.17) .
\end{array}
\]

The minimum gain matrix inequality remains bilinear whereas the maximum gain inequality is affine in the free variables. For that reason the synthesis is performed iteratively through the following procedure.

\subsection*{7.4.4 Synthesis}
There are many methods to solve BMIs, see, e.g., [120]. The BMI is solved iteratively by solving sequential LMI problems, where each iterate is denoted by $k$. First, the variable $\gamma=\gamma_{0}$ is set.

0 . For $k=0$, set $\mathcal{X}_{k}=I, \mathcal{Y}_{k}=0, \mathcal{Z}_{k}=0$

\begin{enumerate}
  \item Fix $\mathcal{X}_{k}, \mathcal{Y}_{k}, \mathcal{Z}_{k}$, and solve for $A_{n, k}, B_{n, k}, C_{n, k}, D_{n, k}, X_{1, k}=X_{1, k}^{\top} \succ 0$, $Y_{1, k}=Y_{1, k}^{\top} \succ 0$ and $0<\nu_{k}^{2}<\infty$ such that $\mathcal{J}=\nu_{k}^{2}$ is maximized, subject to (7.15), (7.16), and (7.17).
  \item Fix the just obtained $A_{n, k}, B_{n, k}, C_{n, k}, D_{n, k}, X_{1, k}=X_{1, k}^{\top}>0, Y_{1, k}= Y_{1, k}^{\top}>0$, and solve for $\mathcal{X}_{k}, \mathcal{Y}_{k}, \mathcal{Z}_{k}$ and $0<\nu_{k}^{2}<\infty$ such that $\mathcal{J}=\nu_{k}^{2}$ is maximized, subject to (7.15).
  \item Return to 1 if $\left|\nu_{k-1}^{2}-\nu_{k}^{2}\right|>\mu$, where $\mu$ specifies a tolerance.
\end{enumerate}

Next, the proposed algorithm is applied to an example.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-176}
\captionsetup{labelformat=empty}
\caption{Figure 7.2: Closed-loop controlled regulator problem in generalized plant form, c.f., Figure 7.1. The generalized plant is highlighted ( $\square$ ).}
\end{center}
\end{figure}

\subsection*{7.5 Numerical example}
The following example illustrates the proposed approach on a fault diagnosis problem. All LMI-related computations are performed with YALMIP [162] and solved with MOSEK [6]. Consider the closed-loop control configuration in Figure 7.2, with single-input single-output plant $G(s)=\frac{(s+25)(s+15)(s+5)}{(s+40)(s+10)(s+3)}$ and controller $C(s)=15 s+25$. The system is subjected to disturbances $d$ and actu- ator faults $f$, which are weighted by $G_{d}(s)$ and $G_{f}(s)$, respectively. Consider $\tilde{w}:=\left[\begin{array}{cc}\tilde{d} & \tilde{f}\end{array}\right]^{\top}, \tilde{z}:=\varepsilon, \tilde{y}:=\left[\begin{array}{cc}y & u\end{array}\right]^{\top}$, and $\tilde{u}:=\varepsilon$. The transfer function matrix of the generalized plant from $\left[\begin{array}{ll}\tilde{w} & \tilde{u}\end{array}\right]^{\top}$ to $\left[\begin{array}{ll}\tilde{z} & \tilde{y}\end{array}\right]^{\top}$, see Figure 7.2, is given by

\[
\left[\begin{array}{c}
\tilde{z}  \tag{7.24}\\
\tilde{y}
\end{array}\right]=\left[\begin{array}{cc|c}
0 & 0 & I \\
\hline S_{O} G_{d} & S_{O} G_{u} G_{f} & 0 \\
-S_{I} C G_{d} & -S_{I} C G_{u} G_{f} & 0
\end{array}\right]\left[\begin{array}{c}
\tilde{w} \\
\tilde{u}
\end{array}\right],
\]

where the input sensitivity $S_{I}=(I+C G)^{-1}$, and the output sensitivity $S_{O}= (I+G C)^{-1}$. The generalized disturbance input is split into $\tilde{w}=\left[\begin{array}{ll}\tilde{w}_{\infty} & \tilde{w}_{-}\end{array}\right]^{\top}= \left[\begin{array}{ll}\tilde{d} & \tilde{f}\end{array}\right]^{\top}$. It is aimed to impose $\mathcal{H}_{\infty}$ performance from $\tilde{w}_{\infty}$ to $\tilde{z}$ and impose $\mathcal{H}_{-}$sensitivity from $\tilde{w}_{-}$to $\tilde{z}$. Two arbitrary weighting filters are chosen to emphasize the shaping capability of the proposed method. To that end $G_{d}= \frac{s^{4}+62.8 s^{3}+1392 s^{2}+1.43 \cdot 10^{4} s+4.87 \cdot 10^{4}}{s^{4}+332 s^{3}+2724 s^{2}+8.10 \cdot 10^{4} s+1.22 \cdot 10^{5}}$ and $G_{f}=\frac{0.92 s^{4}+43.25 s^{3}+1911 s^{2}+5976 s+1.75 \cdot 10^{4}}{s^{4}+13.19 s^{3}+3966 s^{2}+2605 s+3.90 \cdot 10^{4}}$.

Consider the multiobjective optimization problem, described by (7.23), where $\gamma_{0}=1, T=\mathcal{F}_{l}(P, Q)=\left[\begin{array}{ll}T_{\tilde{z} \tilde{w}_{\infty}} & T_{\tilde{z} \tilde{w}_{-}}\end{array}\right]^{\top}=\left[\begin{array}{ll}T_{\varepsilon \tilde{d}} & T_{\varepsilon \tilde{f}}\end{array}\right]^{\top}$, and the transfer matrices $T_{\tilde{z} \tilde{w}_{\infty}}$ and $T_{\tilde{z} \tilde{w}_{-}}$admit the realization, described in Section 7.3.3, and are used to impose the maximum and minimum gain constraint, respectively.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-177}
\captionsetup{labelformat=empty}
\caption{Figure 7.3: Residual filter \$Q:\textbackslash left[\textbackslash begin\{array}\{ll\}y \& u\textbackslash end\{array\}\textbackslash right]\^{}\{\textbackslash top\} \textbackslash rightarrow \textbackslash varepsilon\$ with optimal ratio $\mathcal{J}$ (一). The updated filter $Q_{2}$, with the same $\mathcal{J}$, is shown as (-).\}\end{center}
\end{figure}

Solving this problem with the synthesis method, described in Section 7.4.4, yields the optimum $\nu=0.76$. So the ratio $\mathcal{J}=\frac{\nu}{\gamma}=0.76$ and the obtained residual generator $Q$ is depicted in Figure 7.3. Hence, with this particular residual generator it is guaranteed that there is disturbance suppression $\left\|T_{\varepsilon \tilde{d}}\right\|_{\infty}<1$ and fault sensitivity $\left\|T_{\varepsilon \tilde{f}}\right\|_{-}>0.76$, see Figure 7.4. Additionally, the channels $T_{\varepsilon d}$ and $T_{\varepsilon f}$ are depicted with their found bounds $\gamma G_{d}^{-1}$ and $\nu G_{f}^{-1}$.

From Figure 7.4 and Figure 7.5 it can be concluded that there is still margin to improve the fault sensitivity while simultaneously satisfying $\left\|T_{\varepsilon \tilde{d}}\right\|_{\infty}<1$. Updating the filter $Q$ with this difference, i.e. $Q_{2}=\frac{\gamma G_{d}^{-1}}{T_{\varepsilon d}} Q_{1}$, yields equivalent results to [157]. Note that the objective $\mathcal{J}=0.76$ remains equal.

\subsection*{7.6 Conclusion}
In this chapter, a new method to solve the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ problem is presented. In particular, a method is proposed to shape the minimum and maximum singular value of the closed-loop performance channel and its effectiveness is illustrated in the context of fault diagnosis. A bilinear matrix inequality is derived which can directly be implemented in combination with various multiobjective matrix inequalities and applied to synthesize filters for a wide range of control and estimation problems.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-178(1)}
\captionsetup{labelformat=empty}
\caption{Figure 7.4: Closed-loop system, i.e., \$T:\textbackslash left[\textbackslash begin\{array}\{ll\}\textbackslash tilde\{w\}\textit{\{\textbackslash infty\} \& \textbackslash tilde\{w\}}\{-\}\textbackslash end\{array\}\textbackslash right]\^{}\{\textbackslash top\} \textbackslash rightarrow \textbackslash varepsilon\$, depicted as (一). The obtained bounds $\gamma$ and $\nu$ are indicated as (---), showing tight bounds near $\omega=2.5 \mathrm{~Hz}$. In this region, the fault to disturbance ratio is close to $\mathcal{J}$, whereas at other frequencies, fault sensitivity to disturbance attenuation ratio is higher. The closed-loop system with $Q_{2}$ is shown as (-).\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-178}
\captionsetup{labelformat=empty}
\caption{Figure 7.5: Closed-loop system without weighting filters, i.e., \$T\_\{\textbackslash varepsilon d}\$ and $T_{\varepsilon f}$, depicted as (-). The obtained bounds $\gamma G_{d}^{-1}$ and $\nu G_{f}^{-1}$ are depicted as (---). Indeed, a filter $Q$ is found that satisfies the shaped upperbound and lowerbound. The closed-loop system with $Q_{2}$ is shown as (-).\}\end{center}
\end{figure}

\section*{Appendices}
\section*{7.A Synthesis inequalities}
The entries of the maximum gain synthesis LMI are


\begin{align*}
& M_{11}=A Y_{1}+Y_{1} A^{\top}+B_{2} C_{n}+C_{n}^{\top} B_{2}^{\top}  \tag{7.25a}\\
& M_{12}=A+A_{n}^{\top}+B_{2} D_{n} C_{2}  \tag{7.25b}\\
& M_{13}=B_{1, j}+B_{2} D_{n} D_{21, j}  \tag{7.25c}\\
& M_{14}=Y_{1} C_{1, j}^{\top}+C_{n}^{\top} D_{12, j}^{\top}  \tag{7.25d}\\
& M_{22}=X_{1} A+A^{\top} X_{1}+B_{n} C_{2}+C_{2}^{\top} B_{n}^{\top}  \tag{7.25e}\\
& M_{23}=X_{1} B_{1, j}+B_{n} D_{21, j}  \tag{7.25f}\\
& M_{24}=C_{1, j}^{\top}+C_{2}^{\top} D_{n}^{\top} D_{12, j}^{\top}  \tag{7.25~g}\\
& M_{33}=-\gamma^{2} I  \tag{7.25h}\\
& M_{34}=D_{11, j}^{\top}+D_{21, j}^{\top} D_{n}^{\top} D_{12, j}^{\top}  \tag{7.25i}\\
& M_{44}=-I \tag{7.25j}
\end{align*}


The entries of the minimum gain synthesis BMI are


\begin{align*}
N_{11}= & A Y_{1}+B_{2} C_{n}+Y_{1} A^{\top}+C_{n}^{\top} B_{2}^{\top}-\left(Y_{1} C_{1, j}^{\top}+C_{n}^{\top} D_{12, j}^{\top}\right) \mathcal{Y}  \tag{7.26a}\\
& -\mathcal{Y}^{\top}\left(C_{1, j} Y_{1}+D_{12, j} C_{n}\right) \\
N_{12}= & A+A_{n}^{\top}+B_{2} D_{n} C_{2}-\left(Y_{1} C_{1, j}^{\top}+C_{n}^{\top} D_{12, j}^{\top}\right) \mathcal{Z}  \tag{7.26b}\\
& -\mathcal{Y}^{\top}\left(C_{1, j}+D_{12, j} D_{n} C_{2}\right) \\
N_{13}= & B_{1, j}+B_{2} D_{n} D_{21, j}-\mathcal{Y}^{\top}\left(D_{11, j}+D_{12, j} D_{n} D_{21, j}\right) \\
& -\left(Y_{1} C_{1, j}^{\top}+C_{n}^{\top} D_{12, j}^{\top}\right) \mathcal{X}  \tag{7.26c}\\
N_{14}= & \mathcal{Y}^{\top}  \tag{7.26d}\\
N_{22}= & X_{1} A+B_{n} C_{2}+A^{\top} X_{1}+C_{2}^{\top} B_{n}^{\top}-\left(C_{1, j}^{\top}+C_{2}^{\top} D_{n}^{\top} D_{12, j}^{\top}\right) \mathcal{Z}  \tag{7.26e}\\
& -\mathcal{Z}^{\top}\left(C_{1, j}+D_{12, j} D_{n} C_{2}\right)
\end{align*}



\begin{align*}
N_{23}= & X_{1} B_{1, j}+B_{n} D_{21, j}-\mathcal{Z}^{\top}\left(D_{11, j}+D_{12, j} D_{n} D_{21, j}\right)  \tag{7.26f}\\
& -\left(C_{1, j}^{\top}+C_{2}^{\top} D_{n}^{\top} D_{12, j}^{\top}\right) \mathcal{X} \\
N_{24}= & \mathcal{Z}^{\top}  \tag{7.26~g}\\
N_{33}= & \nu^{2} I-\mathcal{X}^{\top}\left(D_{11}+D_{12} D_{n} D_{21}\right)  \tag{7.26h}\\
& -\left(D_{11, j}^{\top}+D_{21, j}^{\top} D_{n}^{\top} D_{12, j}^{\top}\right) \mathcal{X}  \tag{7.26i}\\
N_{34}= & \mathcal{X}^{\top}  \tag{7.26j}\\
N_{44}= & -I \tag{7.26k}
\end{align*}


\section*{7.B Auxiliary results}
To derive the synthesis inequalities, the following auxiliary results are used.\\
Definition 7.11. Linear fractional transformation For matrices $N$ and $M=\left[\begin{array}{ll}M_{11} & M_{12} \\ M_{21} & M_{22}\end{array}\right]$ of appropriate partitioning, the lower linear fractional transformation (LFT) is defined as $\mathcal{F}_{l}=M_{11}+M_{12} N\left(I-M_{22} N\right)^{-1} M_{21}$ and the upper LFT as $\mathcal{F}_{u}=M_{33}+M_{21} N\left(I-M_{11} N\right)^{-1} M_{12}$, under the assumption that the involved matrix inverses exists.

Lemma 7.12. Schur complement ([220]) Consider

\[
F(x)=\left[\begin{array}{ll}
F_{11}(x) & F_{12}(x)  \tag{7.27}\\
F_{21}(x) & F_{22}(x)
\end{array}\right]
\]

where $F_{11} \in \mathbb{S}^{n}, F_{12}=F_{21}^{\top} \in \mathbb{R}^{n \times m}$, and $F_{22} \in \mathbb{S}^{m}$. Then $F(x) \prec 0$ if and only if

\[
\left\{\begin{array}{l}
F_{11}(x) \prec 0  \tag{7.28}\\
F_{22}(x)-F_{21}(x) F_{11}(x)^{-1} F_{12}(x) \prec 0
\end{array}\right.
\]

Similarly, if and only if

\[
\left\{\begin{array}{l}
F_{22}(x) \prec 0  \tag{7.29}\\
F_{11}(x)-F_{12}(x) F_{22}(x)^{-1} F_{21}(x) \prec 0
\end{array}\right.
\]

Lemma 7.13. Congruence transformation ([220]) Consider $Q \in \mathbb{S}^{n}$ and $W \in \mathbb{R}^{n \times n}$, where $\operatorname{rank}(W)=n$. The matrix inequality $Q \prec 0$ is satisfied if and only if $W Q W^{\top} \prec 0$, or equivalently $W^{\top} Q W \prec 0$.

Lemma 7.14. Youngs relation ([34]) Consider $F, G \in \mathbb{R}^{n \times m}$ and $S \in \mathbb{S}^{n}$, where $S \succ 0$. Youngs inequality is given by


\begin{equation*}
F^{\top} G+G^{\top} F \preceq F^{\top} S^{-1} F+G^{\top} S G . \tag{7.30}
\end{equation*}


Lemma 7.15. Transformation lemma [69, Chap. 7] Suppose $X_{1}$ and $Y_{1}$ are symmetric, positive definite matrices in $\mathbb{R}^{n \times n}$, and $n_{K} \in \mathbb{N}$. Then there exist matrices $X_{2}, Y_{2} \in \mathbb{R}^{n \times n_{K}}$ and symmetric matrices $X_{3}, Y_{3} \in \mathbb{R}^{n_{K} \times n_{K}}$, satisfying

\[
X=\left[\begin{array}{cc}
X_{1} & X_{2}  \tag{7.31}\\
X_{2}^{\top} & X_{3}
\end{array}\right] \succ 0,
\]

and

\[
X^{-1}=\left[\begin{array}{ll}
X_{1} & X_{2}  \tag{7.32}\\
X_{2}^{\top} & X_{3}
\end{array}\right]^{-1}=\left[\begin{array}{cc}
Y_{1} & Y_{2} \\
Y_{2}^{\top} & Y_{3}
\end{array}\right]=Y \succ 0,
\]

if and only if

\[
\left[\begin{array}{cc}
Y_{1} & I  \tag{7.33}\\
I & X_{1}
\end{array}\right] \succ 0,
\]

and

\[
\operatorname{rank}\left[\begin{array}{cc}
X_{1} & I  \tag{7.34}\\
I & Y_{1}
\end{array}\right] \leq n+n_{K} .
\]

\section*{7.C Derivation of the maximum gain synthesis LMI}
Considering the maximum gain constraint, the aim is to show that the resulting description satisfies $\|T(s)\|_{\infty}<\gamma$, where $T(s)$ denotes the transfer function corresponding to the realization ( $\mathcal{A}, \mathcal{B}, \mathcal{C}, \mathcal{D}$ ). To select a particular channel replace the latter realization with the matrices in Section 7.3.3. Substitution into the maximum gain constraint (7.4) of the Bounded Real Lemma gives

$$
\left[\begin{array}{ccc}
\mathcal{A}^{\top} X+X \mathcal{A} & X \mathcal{B} & \mathcal{C}^{\top} \\
\mathcal{B}^{\top} X & -\gamma^{2} I & \mathcal{D}^{\top} \\
\mathcal{C} & \mathcal{D} & -I
\end{array}\right] \prec 0 .
$$

Next, introduce the matrix $Y_{\text {CL }}$ and apply the congruence transformation

$$
\left[\begin{array}{ccc}
Y_{\mathrm{CL}}{ }^{\top} & & \\
& I & \\
& & I
\end{array}\right]\left[\begin{array}{ccc}
\mathcal{A}^{\top} X+X \mathcal{A} & X \mathcal{B} & \mathcal{C}^{\top} \\
\mathcal{B}^{\top} X & -\gamma^{2} I & \mathcal{D}^{\top} \\
\mathcal{C} & \mathcal{D} & -I
\end{array}\right]\left[\begin{array}{ccc}
Y_{\mathrm{CL}} & & \\
& I & \\
& & I
\end{array}\right] \prec 0,
$$

which is written as

$$
\left[\begin{array}{ccc}
Y_{\mathrm{CL}}^{\top} \mathcal{A}^{\top} X Y_{\mathrm{CL}}+Y_{\mathrm{CL}}^{\top} X \mathcal{A} Y_{\mathrm{CL}} & Y_{\mathrm{CL}}^{\top} X \mathcal{B} & Y_{\mathrm{CL}}^{\top} \mathcal{C}^{\top} \\
\mathcal{B}^{\top} X Y_{\mathrm{CL}} & -\gamma^{2} I & \mathcal{D}^{\top} \\
\mathcal{C} Y_{\mathrm{CL}} & \mathcal{D} & -I
\end{array}\right] \prec 0 .
$$

Rewriting $X Y_{\mathrm{CL}}=X_{\mathrm{CL}}{ }^{\top}$, and $Y_{\mathrm{CL}}{ }^{\top} X=X_{\mathrm{CL}}$, and using the transformation Lemma [69, Lemma 7.9], yields

\[
\left[\begin{array}{ccc}
Y_{\mathrm{CL}}^{\top} \mathcal{A}^{\top} X_{\mathrm{CL}}^{\top}+X_{\mathrm{CL}} \mathcal{A} Y_{\mathrm{CL}} & X_{\mathrm{CL}} \mathcal{B} & Y_{\mathrm{CL}}^{\top} \mathcal{C}^{\top}  \tag{7.35}\\
\mathcal{B}^{\top} X_{\mathrm{CL}}^{\top} & -\gamma^{2} I & \mathcal{D}^{\top} \\
\mathcal{C} Y_{\mathrm{CL}} & \mathcal{D} & -I
\end{array}\right] \prec 0 .
\]

To linearize this matrix inequality, start from the observation that

$$
\left[\begin{array}{cc}
X_{\mathrm{CL}} & 0 \\
0 & I
\end{array}\right]\left[\begin{array}{ll}
\mathcal{A} & \mathcal{B} \\
\mathcal{C} & \mathcal{D}
\end{array}\right]\left[\begin{array}{cc}
Y_{\mathrm{CL}} & 0 \\
0 & I
\end{array}\right]=\left[\begin{array}{cc}
X_{\mathrm{CL}} \mathcal{A} Y_{\mathrm{CL}} & X_{\mathrm{CL}} \mathcal{B} \\
\mathcal{C} Y_{\mathrm{CL}} & \mathcal{D}
\end{array}\right]
$$

Substitution of the closed-loop relations (7.10) gives

$$
\begin{aligned}
& {\left[\begin{array}{cc}
X_{\mathrm{CL}} & 0 \\
0 & I
\end{array}\right]\left(\left[\begin{array}{cc|c}
A & 0 & B_{1} \\
0 & 0 & 0 \\
\hline C_{1} & 0 & D_{11}
\end{array}\right]+\left[\begin{array}{cc}
0 & B_{2} \\
I & 0 \\
\hline 0 & D_{12}
\end{array}\right]\left[\begin{array}{cc}
A_{c 2} & B_{c 2} \\
C_{c 2} & D_{c 2}
\end{array}\right]\right.} \\
& \left.\quad \times\left[\begin{array}{cc|c}
0 & I & 0 \\
C_{2} & 0 & D_{21}
\end{array}\right]\right)\left[\begin{array}{cc}
Y_{\mathrm{CL}} & 0 \\
0 & I
\end{array}\right]=\left[\begin{array}{cc}
X_{\mathrm{CL}} \mathcal{A} Y_{\mathrm{CL}} & X_{\mathrm{CL}} \mathcal{B} \\
\mathcal{C} Y_{\mathrm{CL}} & \mathcal{D}
\end{array}\right]
\end{aligned}
$$

that is rewritten to


\begin{align*}
& {\left[\begin{array}{ccc}
A Y_{1} & A & B_{1} \\
X_{1} A Y_{1} & X_{1} A & X_{1} B_{1} \\
C_{1} Y_{1} & C_{1} & D_{11}
\end{array}\right]+\left[\begin{array}{cc}
0 & B_{2} \\
I & 0 \\
\hline 0 & D_{12}
\end{array}\right]\left[\begin{array}{cc}
X_{2} & X_{1} B_{2} \\
0 & I
\end{array}\right]\left[\begin{array}{ll}
A_{c 2} & B_{c 2} \\
C_{c 2} & D_{c 2}
\end{array}\right]}  \tag{7.36}\\
& \quad \times\left[\begin{array}{cc}
Y_{2}{ }^{\top} & 0 \\
C_{2} Y_{1} & I
\end{array}\right]\left[\begin{array}{cc|c}
I & 0 & 0 \\
0 & C_{2} & D_{21}
\end{array}\right]=\left[\begin{array}{cc}
X_{\mathrm{CL}} \mathcal{A} Y_{\mathrm{CL}} & X_{\mathrm{CL}} \mathcal{B} \\
\mathcal{C} Y_{\mathrm{CL}} & \mathcal{D}
\end{array}\right]
\end{align*}


Next, consider the change of variables

$$
\left[\begin{array}{ll}
A_{n} & B_{n} \\
C_{n} & D_{n}
\end{array}\right]=\left[\begin{array}{cc}
X_{2} & X_{1} B_{2} \\
0 & I
\end{array}\right]\left[\begin{array}{cc}
A_{c 2} & B_{c 2} \\
C_{c 2} & D_{c 2}
\end{array}\right]\left[\begin{array}{cc}
Y_{2}^{\top} & 0 \\
C_{2} Y_{1} & I
\end{array}\right]+\left[\begin{array}{cc}
X_{1} A Y_{1} & 0 \\
0 & 0
\end{array}\right]
$$

and its inverse operation

$$
\left[\begin{array}{cc}
A_{c 2} & B_{c 2} \\
C_{c 2} & D_{c 2}
\end{array}\right]=\left[\begin{array}{cc}
X_{2} & X_{1} B_{2} \\
0 & I
\end{array}\right]^{-1}\left(\left[\begin{array}{cc}
A_{n} & B_{n} \\
C_{n} & D_{n}
\end{array}\right]-\left[\begin{array}{cc}
X_{1} A Y_{1} & 0 \\
0 & 0
\end{array}\right]\right)\left[\begin{array}{cc}
Y_{2}^{\top} & 0 \\
C_{2} Y_{1} & I
\end{array}\right]^{-1} .
$$

Substitution of the latter into (7.36) gives


\begin{align*}
& {\left[\begin{array}{ccc}
A Y_{1} & A & B_{1} \\
X_{1} A Y_{1} & X_{1} A & X_{1} B_{1} \\
C_{1} Y_{1} & C_{1} & D_{11}
\end{array}\right]+\left[\begin{array}{cc}
0 & B_{2} \\
I & 0 \\
\hline 0 & D_{12}
\end{array}\right]\left(\left[\begin{array}{cc}
A_{n} & B_{n} \\
C_{n} & D_{n}
\end{array}\right]-\left[\begin{array}{cc}
X_{1} A Y_{1} & 0 \\
0 & 0
\end{array}\right]\right)}  \tag{7.37}\\
& \quad \times\left[\begin{array}{cc|c}
I & 0 & 0 \\
0 & C_{2} & D_{21}
\end{array}\right]=\left[\begin{array}{cc}
X_{\mathrm{CL}} \mathcal{A} Y_{\mathrm{CL}} & X_{\mathrm{CL}} \mathcal{B} \\
\mathcal{C} Y_{\mathrm{CL}} & \mathcal{D}
\end{array}\right]
\end{align*}


which is equal to\\
$\left[\begin{array}{ccc}A Y_{1}+B_{2} C_{n} & A+B_{2} D_{n} C_{2} & B_{1}+B_{2} D_{n} D_{21} \\ A_{n} & X_{1} A+B_{n} C_{2} & X_{1} B_{1}+B_{n} D_{21} \\ C_{1} Y_{1}+D_{12} C_{n} & C_{1}+D_{12} D_{n} C_{2} & D_{11}+D_{12} D_{n} D_{21}\end{array}\right]=\left[\begin{array}{cc}X_{\mathrm{CL}} \mathcal{A} Y_{\mathrm{CL}} & X_{\mathrm{CL}} \mathcal{B} \\ \mathcal{C} Y_{\mathrm{CL}} & \mathcal{D}\end{array}\right]$.\\
Adding zeros yields a structure similar to (7.35) as


\begin{align*}
& {\left[\begin{array}{cccc}
A Y_{1}+B_{2} C_{n} & A+B_{2} D_{n} C_{2} & B_{1}+B_{2} D_{n} D_{21} & 0 \\
A_{n} & X_{1} A+B_{n} C_{2} & X_{1} B_{1}+B_{n} D_{21} & 0 \\
0 & 0 & 0 & 0 \\
C_{1} Y_{1}+D_{12} C_{n} & C_{1}+D_{12} D_{n} C_{2} & D_{11}+D_{12} D_{n} D_{21} & 0
\end{array}\right]}  \tag{7.38}\\
& =\left[\begin{array}{ccc}
X_{\mathrm{CL}} \mathcal{A} Y_{\mathrm{CL}} & X_{\mathrm{CL}} \mathcal{B} & 0 \\
{\left[\begin{array}{cc}
0 & 0
\end{array}\right]} & 0 & 0 \\
\mathcal{C} Y_{\mathrm{CL}} & \mathcal{D} & 0
\end{array}\right],
\end{align*}


and transposing gives


\begin{align*}
& {\left[\begin{array}{cccc}
Y_{1} A^{\top}+C_{n}{ }^{\top} B_{2}^{\top} & A_{n}{ }^{\top} & 0 & Y_{1} C_{1}^{\top}+C_{n}{ }^{\top} D_{12}^{\top} \\
A^{\top}+C_{2}^{\top} D_{n}{ }^{\top} B_{2}^{\top} & A^{\top} X_{1}+C_{2}^{\top} B_{n}{ }^{\top} & 0 & C_{1}^{\top}+C_{2}^{\top} D_{n}{ }^{\top} D_{12}^{\top} \\
B_{1}^{\top}+D_{21}^{\top} D_{n}^{\top} B_{2}^{\top} & B_{1}^{\top} X_{1}+D_{21}^{\top} B_{n}{ }^{\top} & 0 & D_{11}^{\top}+D_{21}^{\top} D_{n}^{\top} D_{12}^{\top} \\
0 & 0 & 0 & 0
\end{array}\right]} \\
& =\left[\begin{array}{ccc}
Y_{\mathrm{CL}}{ }^{\top} \mathcal{A}^{\top} X_{\mathrm{CL}}{ }^{\top} & {\left[\begin{array}{l}
0 \\
0
\end{array}\right]} & Y_{\mathrm{CL}}{ }^{\top} \mathcal{C}^{\top} \\
\mathcal{B}^{\top} X_{\mathrm{CL}}{ }^{\top} & 0 & \mathcal{D}^{\top} \\
0 & 0 & 0
\end{array}\right] . \tag{7.39}
\end{align*}


Adding the latter two and including $-\gamma^{2} I$ and $-I$ in the last two diagonal entries gives

$$
\left[\begin{array}{cccc}
M_{11} & M_{12} & M_{13} & M_{14} \\
M_{12}^{\top} & M_{22} & M_{23} & M_{24} \\
M_{13}^{\top} & M_{23}^{\top} & -\gamma^{2} I & M_{34} \\
M_{14}^{\top} & M_{24}^{\top} & M_{34}^{\top} & -I
\end{array}\right]=\left[\begin{array}{ccc}
Y_{\mathrm{CL}}{ }^{\top} \mathcal{A}^{\top} X_{\mathrm{CL}}{ }^{\top}+X_{\mathrm{CL}} \mathcal{A} Y_{\mathrm{CL}} & X_{\mathrm{CL}} \mathcal{B} & Y_{\mathrm{CL}}{ }^{\top} \mathcal{C}^{\top} \\
\mathcal{B}^{\top} X_{\mathrm{CL}}{ }^{\top} & -\gamma^{2} I & \mathcal{D}^{\top} \\
\mathcal{C} Y_{\mathrm{CL}} & \mathcal{D} & -I
\end{array}\right],
$$

where

$$
\begin{aligned}
& M_{11}=A Y_{1}+Y_{1} A^{\top}+B_{2} C_{n}+C_{n}^{\top} B_{2}^{\top}, \\
& M_{12}=A+A_{n}^{\top}+B_{2} D_{n} C_{2}, \\
& M_{13}=B_{1}+B_{2} D_{n} D_{21}, \\
& M_{14}=Y_{1} C_{1}^{\top}+C_{n}^{\top} D_{12}^{\top}, \\
& M_{22}=X_{1} A+A^{\top} X_{1}+B_{n} C_{2}+C_{2}^{\top} B_{n}^{\top}, \\
& M_{23}=X_{1} B_{1}+B_{n} D_{21}, \\
& M_{24}=C_{1}^{\top}+C_{2}^{\top} D_{n}^{\top} D_{12}^{\top}, \\
& M_{34}=D_{11}^{\top}+D_{21}^{\top} D_{n}^{\top} D_{12}^{\top} .
\end{aligned}
$$

Hence, the synthesis LMI, which is affine in the variables $X_{1}, Y_{1}, A_{n}, B_{n}, C_{n}$, and $D_{n}$, is given by

\[
\left[\begin{array}{cccc}
M_{11} & M_{12} & M_{13} & M_{14}  \tag{7.41}\\
M_{12}^{\top} & M_{22} & M_{23} & M_{24} \\
M_{13}^{\top} & M_{23}^{\top} & -\gamma^{2} I & M_{34} \\
M_{14}^{\top} & M_{24}^{\top} & M_{34}^{\top} & -I
\end{array}\right] \prec 0 .
\]

Considering a particular channel $T_{j}$ as introduced in Section 7.3.3 gives the maximum gain synthesis LMI (7.15).

\section*{7.D Derivation of the minimum gain synthesis BMI}
Consider the minimum gain constraint. The aim is to show that the resulting description satisfies $\|T(s)\|_{-}>\nu$, where $T(s)$ denotes the transfer function corresponding to the realization ( $\mathcal{A}, \mathcal{B}, \mathcal{C}, \mathcal{D}$ ). To select a particular channel replace the latter realization with the matrices in Section 7.3.3. Substitution into the minimum gain constraint (7.3) of the Minimum Gain Lemma 7.3, gives

$$
\left[\begin{array}{cc}
X \mathcal{A}+\mathcal{A}^{\top} X-\mathcal{C}^{\top} \mathcal{C} & X \mathcal{B}-\mathcal{C}^{\top} \mathcal{D} \\
\mathcal{B}^{\top} X-\mathcal{D}^{\top} \mathcal{C} & \nu^{2} I-\mathcal{D}^{\top} \mathcal{D}
\end{array}\right] \prec 0
$$

after taking the Schur complement. This can be written as

\[
\left[\begin{array}{cc}
X \mathcal{A}+\mathcal{A}^{\top} X & X \mathcal{B}  \tag{7.42}\\
\mathcal{B}^{\top} X & \nu^{2} I
\end{array}\right]-\left[\begin{array}{c}
\mathcal{C}^{\top} \\
\mathcal{D}^{\top}
\end{array}\right]\left[\begin{array}{ll}
\mathcal{C} & \mathcal{D}
\end{array}\right] \prec 0 .
\]

Define $F=\left[\begin{array}{ll}\mathcal{C} & \mathcal{D}\end{array}\right]$ and auxiliary slack variables $G=\left[\begin{array}{ll}\mathcal{Y} & \mathcal{X}\end{array}\right]$. Substitution into Youngs relation

$$
F^{\top} F=F^{\top} G+G^{\top} F-G^{\top} G+(F-G)^{\top}(F-G),
$$

gives

$$
\begin{aligned}
{\left[\begin{array}{l}
\mathcal{C}^{\top} \\
\mathcal{D}^{\top}
\end{array}\right]\left[\begin{array}{ll}
\mathcal{C} & \mathcal{D}
\end{array}\right]=} & {\left[\begin{array}{cc}
\mathcal{C}^{\top} \mathcal{Y}+\mathcal{Y}^{\top} \mathcal{C}-\mathcal{Y}^{\top} \mathcal{Y} & \mathcal{C}^{\top} \mathcal{X}+\mathcal{Y}^{\top} \mathcal{D}-\mathcal{Y}^{\top} \mathcal{X} \\
* & \mathcal{X}^{\top} \mathcal{D}+\mathcal{D}^{\top} \mathcal{X}-\mathcal{X}^{\top} \mathcal{X}
\end{array}\right] } \\
& +\left[\begin{array}{cc}
(\mathcal{C}-\mathcal{Y})^{\top}(\mathcal{C}-\mathcal{Y}) & (\mathcal{C}-\mathcal{Y})^{\top}(\mathcal{D}-\mathcal{X}) \\
* & (\mathcal{D}-\mathcal{X})^{\top}(\mathcal{D}-\mathcal{X})
\end{array}\right]
\end{aligned}
$$

Using this to constrain (7.42) yields

$$
\left[\begin{array}{cc}
X \mathcal{A}+\mathcal{A}^{\top} X & X \mathcal{B}_{2} \\
* & \nu^{2} I
\end{array}\right]-\left[\begin{array}{cc}
\mathcal{C}^{\top} \mathcal{Y}+\mathcal{Y}^{\top} \mathcal{C}-\mathcal{Y}^{\top} \mathcal{Y} & \mathcal{C}^{\top} \mathcal{X}+\mathcal{Y}^{\top} \mathcal{D}-\mathcal{Y}^{\top} \mathcal{X} \\
* & \mathcal{X}^{\top} \mathcal{D}+\mathcal{D}^{\top} \mathcal{X}-\mathcal{X}^{\top} \mathcal{X}
\end{array}\right] \prec 0,
$$

which is written as

$$
\left[\begin{array}{cc}
X \mathcal{A}+\mathcal{A}^{\top} X-\mathcal{C}^{\top} \mathcal{Y}-\mathcal{Y}^{\top} \mathcal{C}+\mathcal{Y}^{\top} \mathcal{Y} & X \mathcal{B}_{2}-\mathcal{C}^{\top} \mathcal{X}-\mathcal{Y}^{\top} \mathcal{D}+\mathcal{Y}^{\top} \mathcal{X} \\
* & \nu^{2} I-\mathcal{X}^{\top} \mathcal{D}-\mathcal{D}^{\top} \mathcal{X}+\mathcal{X}^{\top} \mathcal{X}
\end{array}\right] \prec 0 .
$$

Taking the Schur complement with $F_{22}(x)=-I \prec 0, F_{12}(x)=\left[\begin{array}{l}\mathcal{Y}^{\top} \\ \mathcal{X}^{\top}\end{array}\right]$, $F_{21}(x)=\left[\begin{array}{ll}\mathcal{Y} & \mathcal{X}\end{array}\right], F_{11}(x)=\left[\begin{array}{cc}X \mathcal{A}+\mathcal{A}^{\top} X-\mathcal{C}^{\top} \mathcal{Y}-\mathcal{Y}^{\top} \mathcal{C} & X \mathcal{B}-\mathcal{C}^{\top} \mathcal{X}-\mathcal{Y}^{\top} \mathcal{D} \\ * & \nu^{2} I-\mathcal{X}^{\top} \mathcal{D}-\mathcal{D}^{\top} \mathcal{X}\end{array}\right]$, gives $F(x)=\left[\begin{array}{ll}F_{11}(x) & F_{12}(x) \\ F_{21}(x) & F_{22}(x)\end{array}\right] \prec 0$, i.e.,

\[
\left[\begin{array}{ccc}
X \mathcal{A}+\mathcal{A}^{\top} X-\mathcal{C}^{\top} \mathcal{Y}-\mathcal{Y}^{\top} \mathcal{C} & X \mathcal{B}-\mathcal{C}^{\top} \mathcal{X}-\mathcal{Y}^{\top} \mathcal{D} & \mathcal{Y}^{\top}  \tag{7.43}\\
* & \nu^{2} I-\mathcal{X}^{\top} \mathcal{D}-\mathcal{D}^{\top} \mathcal{X} & \mathcal{X}^{\top} \\
* & * & -I
\end{array}\right] \prec 0 .
\]

Next, introduce $Y_{\text {CL }}$ and apply the congruence transformation with $\bar{Y}= \operatorname{diag}\left(Y_{\mathrm{CL}}, I, I\right)$

$$
\bar{Y}^{\top}\left[\begin{array}{ccc}
X \mathcal{A}+\mathcal{A}^{\top} X-\mathcal{C}^{\top} \mathcal{Y}-\mathcal{Y}^{\top} \mathcal{C} & X \mathcal{B}-\mathcal{C}^{\top} \mathcal{X}-\mathcal{Y}^{\top} \mathcal{D} & \mathcal{Y}^{\top} \\
* & \nu^{2} I-\mathcal{X}^{\top} \mathcal{D}-\mathcal{D}^{\top} \mathcal{X} & \mathcal{X}^{\top} \\
* & * & -I
\end{array}\right] \bar{Y} \prec 0,
$$

to obtain

\[
\left[\begin{array}{ccc}
\bar{N}_{11}^{1} & \bar{N}_{12}^{1} & \bar{N}_{13}^{1}  \tag{7.44}\\
* & \bar{N}_{22}^{1} & \bar{N}_{23}^{1} \\
* & * & \bar{N}_{33}^{1}
\end{array}\right] \prec 0
\]

with

$$
\begin{aligned}
\bar{N}_{11}^{1} & =Y_{\mathrm{CL}}^{\top} \mathcal{A}^{\top} X Y_{\mathrm{CL}}+Y_{\mathrm{CL}}^{\top} X \mathcal{A} Y_{\mathrm{CL}}-Y_{\mathrm{CL}}^{\top} \mathcal{C}^{\top} \mathcal{Y} Y_{\mathrm{CL}}-Y_{\mathrm{CL}}^{\top} \mathcal{Y}^{\top} \mathcal{C} Y_{\mathrm{CL}} \\
\bar{N}_{12}^{1} & =Y_{\mathrm{CL}}^{\top}\left(X \mathcal{B}-\mathcal{C}^{\top} \mathcal{X}-\mathcal{Y}^{\top} \mathcal{D}\right) \\
\bar{N}_{13}^{1} & =Y_{\mathrm{CL}}^{\top} \mathcal{Y}^{\top} \\
\bar{N}_{22}^{1} & =\nu^{2} I-\mathcal{X}^{\top} \mathcal{D}-\mathcal{D}^{\top} \mathcal{X} \\
\bar{N}_{23}^{1} & =\mathcal{X}^{\top} \\
\bar{N}_{33}^{1} & =-I
\end{aligned}
$$

Writing $\mathcal{Y}=\overline{\mathcal{Y}} X$, and $\mathcal{Y}^{\top}=X \overline{\mathcal{Y}}^{\top}$, and substituting $X Y_{\mathrm{CL}}=X_{\mathrm{CL}}{ }^{\top}$, and $Y_{\mathrm{CL}}{ }^{\top} X=X_{\mathrm{CL}}$, yields

\[
\left[\begin{array}{ccc}
\bar{N}_{11}^{2} & \bar{N}_{12}^{2} & \bar{N}_{13}^{2}  \tag{7.46}\\
* & \bar{N}_{22}^{2} & \bar{N}_{23}^{2} \\
* & * & \bar{N}_{33}^{2}
\end{array}\right] \prec 0
\]

with

$$
\begin{aligned}
& \bar{N}_{11}^{2}=Y_{\mathrm{CL}}^{\top} \mathcal{A}^{\top} X_{\mathrm{CL}}^{\top}+X_{\mathrm{CL}} \mathcal{A} Y_{\mathrm{CL}}-Y_{\mathrm{CL}}^{\top} \mathcal{C}^{\top} \overline{\mathcal{Y}} X_{\mathrm{CL}}^{\top}-X_{\mathrm{CL}} \overline{\mathcal{Y}}^{\top} \mathcal{C} Y_{\mathrm{CL}} \\
& \bar{N}_{12}^{2}=X_{\mathrm{CL}}\left(\mathcal{B}-\overline{\mathcal{Y}}^{\top} \mathcal{D}\right)-Y_{\mathrm{CL}}^{\top} \mathcal{C}^{\top} \mathcal{X} \\
& \bar{N}_{13}^{2}=X_{\mathrm{CL}} \overline{\mathcal{Y}}^{\top} \\
& \bar{N}_{22}^{2}=\nu^{2} I-\mathcal{X}^{\top} \mathcal{D}-\mathcal{D}^{\top} \mathcal{X} \\
& \bar{N}_{23}^{2}=\mathcal{X}^{\top} \\
& \bar{N}_{33}^{2}=-I
\end{aligned}
$$

Now, the latter is split in two matrices

\[
\begin{array}{r}
{\left[\begin{array}{ccc}
Y_{\mathrm{CL}}^{\top} \mathcal{A}^{\top} X_{\mathrm{CL}}^{\top}+X_{\mathrm{CL}} \mathcal{A} Y_{\mathrm{CL}} & X_{\mathrm{CL}} \mathcal{B} & 0 \\
* & \nu^{2} I & 0 \\
* & * & -I
\end{array}\right]} \\
{\left[\begin{array}{ccc}
-Y_{\mathrm{CL}}^{\top} \mathcal{C}^{\top} \overline{\mathcal{Y}} X_{\mathrm{CL}}^{\top}-X_{\mathrm{CL}} \overline{\mathcal{Y}}^{\top} \mathcal{C} Y_{\mathrm{CL}} & -X_{\mathrm{CL}} \overline{\mathcal{Y}}^{\top} \mathcal{D}-Y_{\mathrm{CL}}^{\top} \mathcal{C}^{\top} \mathcal{X} & X_{\mathrm{CL}} \overline{\mathcal{Y}}^{\top} \\
* & -\mathcal{X}^{\top} \mathcal{D}-\mathcal{D}^{\top} \mathcal{X} & \mathcal{X}^{\top} \\
* & * & 0
\end{array}\right]} \tag{7.47b}
\end{array}
\]

First, consider the first matrix (7.47a). Recall from maximum gain synthesis LMI derivation, in particular (7.37), that

$$
\left[\begin{array}{cc}
X_{\mathrm{CL}} \mathcal{A} Y_{\mathrm{CL}} & X_{\mathrm{CL}} \mathcal{B} \\
\mathcal{C} Y_{\mathrm{CL}} & \mathcal{D}
\end{array}\right]=\left[\begin{array}{ccc}
A Y_{1}+B_{2} C_{n} & A+B_{2} D_{n} C_{2} & B_{1}+B_{2} D_{n} D_{21} \\
A_{n} & X_{1} A+B_{n} C_{2} & X_{1} B_{1}+B_{n} D_{21} \\
C_{1} Y_{1}+D_{12} C_{n} & C_{1}+D_{12} D_{n} C_{2} & D_{11}+D_{12} D_{n} D_{21}
\end{array}\right]
$$

Comparison to (7.43) shows that adding $\nu^{2} I$ and $-I$ to the last two diagonal entries gives

\[
\left[\begin{array}{ccc}
Y_{\mathrm{CL}}^{\top} \mathcal{A}^{\top} X_{\mathrm{CL}}^{\top}+X_{\mathrm{CL}} \mathcal{A} Y_{\mathrm{CL}} & X_{\mathrm{CL}} \mathcal{B} & 0  \tag{7.48}\\
* & \nu^{2} I & 0 \\
* & * & -I
\end{array}\right]=\left[\begin{array}{cccc}
\bar{N}_{11}^{3} & \bar{N}_{12}^{3} & \bar{N}_{13}^{3} & 0 \\
* & \bar{N}_{22}^{3} & \bar{N}_{23}^{3} & 0 \\
* & * & \nu^{2} I & 0 \\
* & * & * & -I
\end{array}\right]
\]

with

$$
\begin{aligned}
& \bar{N}_{11}^{3}=A Y_{1}+B_{2} C_{n}+Y_{1} A^{\top}+C_{n}^{\top} B_{2}^{\top} \\
& \bar{N}_{12}^{3}=A+A_{n}^{\top}+B_{2} D_{n} C_{2} \\
& \bar{N}_{13}^{3}=B_{1}+B_{2} D_{n} D_{21} \\
& \bar{N}_{22}^{3}=X_{1} A+B_{n} C_{2}+A^{\top} X_{1}+C_{2}^{\top} B_{n}^{\top} \\
& \bar{N}_{23}^{3}=X_{1} B_{1}+B_{n} D_{21}
\end{aligned}
$$

Now, consider the second contribution (7.47b). To this end let $Y=Y^{\top} \succ 0$ and $X=Y^{-1}$, be defined as

\[
Y=\left[\begin{array}{cc}
Y_{1} & Y_{2}  \tag{7.49}\\
Y_{2}^{\top} & Y_{3}
\end{array}\right], \quad X=\left[\begin{array}{cc}
X_{1} & X_{2} \\
X_{2}^{\top} & X_{3}
\end{array}\right] .
\]

From the transformation Lemma [69, Lemma 7.9], it is known that given $Y_{1}= Y_{1}^{\top} \succ 0$ and $X_{1}=X_{1}^{\top} \succ 0$, the matrices $Y$ and $X$ can be constructed to satisfy $Y X=I, Y=Y^{\top} \succ 0$ and $X=X^{\top} \succ 0$. This construction is possible if and only if

\[
\left[\begin{array}{cc}
X_{1} & I  \tag{7.50}\\
I & Y_{1}
\end{array}\right] \succ 0,
\]

and $X_{1}=Y_{1}^{-1}=X_{2} X_{2}^{\top} \succeq 0$. In the proof is shown that, setting $X_{3}=I[69$, Proof Lemma 7.9], results in


\begin{gather*}
Y=\left[\begin{array}{cc}
Y_{1} & Y_{2} \\
* & I
\end{array}\right]  \tag{7.51}\\
X=\left[\begin{array}{cc}
X_{1} & -X_{1} Y_{2} \\
* & Y_{2}^{\top} X_{1} Y_{2}+I
\end{array}\right] \tag{7.52}
\end{gather*}


In addition, it is shown that

\[
Y_{\mathrm{CL}}=\left[\begin{array}{cc}
Y_{1} & I  \tag{7.53}\\
Y_{2}^{\top} & 0
\end{array}\right], \quad \text { and } \quad X_{\mathrm{CL}}=\left[\begin{array}{cc}
I & 0 \\
X_{1} & X_{2}
\end{array}\right]
\]

are full rank. Using these parameterizations, can be concluded that


\begin{gather*}
Y_{\mathrm{CL}}^{\top} X=X_{\mathrm{CL}},  \tag{7.54}\\
{\left[\begin{array}{l}
\mathcal{Y}^{\top} \\
\mathcal{Z}^{\top}
\end{array}\right]=\left[\begin{array}{c}
Y_{1}^{\top} \\
X_{1} Y_{1}^{\top}+X_{2} Y_{2}^{\top}
\end{array}\right]=\left[\begin{array}{cc}
1 & 0 \\
X_{1} & X_{2}
\end{array}\right]\left[\begin{array}{l}
Y_{1}^{\top} \\
Y_{2}^{\top}
\end{array}\right]=X_{\mathrm{CL}} \overline{\mathcal{Y}}^{\top},} \tag{7.55}
\end{gather*}


and

\[
\left[\begin{array}{ll}
\mathcal{Y} & \mathcal{Z}
\end{array}\right]=\left[\begin{array}{ll}
Y_{1} & Y_{1} X_{1}+Y_{2} X_{2}^{\top}
\end{array}\right]=\left[\begin{array}{ll}
Y_{1} & Y_{2}
\end{array}\right]\left[\begin{array}{cc}
1 & X_{1}  \tag{7.56}\\
0 & X_{2}^{\top}
\end{array}\right]=\overline{\mathcal{Y}} X_{\mathrm{CL}}^{\top} .
\]

Using these two relations, the second contribution (7.47b) can be written as


\begin{gather*}
{\left[\begin{array}{c}
-Y_{\mathrm{CL}}^{\top} \mathcal{C}^{\top} \overline{\mathcal{Y}} X_{\mathrm{CL}}^{\top}-X_{\mathrm{CL}} \overline{\mathcal{Y}}^{\top} \mathcal{C} Y_{\mathrm{CL}} \\
*
\end{array} \quad \begin{array}{c}
-X_{\mathrm{CL}} \overline{\mathcal{Y}}^{\top} \mathcal{D}-Y_{\mathrm{CL}}^{\top} \mathcal{C}^{\top} \mathcal{X} \\
-\mathcal{X}^{\top} \mathcal{D}-\mathcal{D}^{\top} \mathcal{X}
\end{array}\right.} \\
\quad *  \tag{7.57}\\
\quad=\left[\begin{array}{cccc}
\bar{N}_{11}^{4} & \bar{N}_{12}^{4} & \bar{N}_{13}^{4} & \bar{N}_{\mathrm{CL}}^{4} \overline{\mathcal{Y}}^{\top} \\
* & \bar{N}_{22}^{4} & \bar{N}_{23}^{4} & \bar{N}_{24}^{4} \\
* & * & \bar{N}_{33}^{4} & \bar{N}_{34}^{4} \\
* & * & * & 0
\end{array}\right],
\end{gather*}


with

$$
\begin{aligned}
& \bar{N}_{11}^{4}=-\left(Y_{1} C_{1}^{\top}+C_{n}^{\top} D_{12}^{\top}\right) \mathcal{Y}-\mathcal{Y}^{\top}\left(C_{1} Y_{1}+D_{12} C_{n}\right), \\
& \bar{N}_{12}^{4}=-\left(Y_{1} C_{1}^{\top}+C_{n}^{\top} D_{12}^{\top}\right) \mathcal{Z}-\mathcal{Y}^{\top}\left(C_{1}+D_{12} D_{n} C_{2}\right), \\
& \bar{N}_{13}^{4}=-\mathcal{Y}^{\top}\left(D_{11}+D_{12} D_{n} D_{21}\right)-\left(Y_{1} C_{1}^{\top}+C_{n}^{\top} D_{12}^{\top}\right) \mathcal{X}, \\
& \bar{N}_{14}^{4}=\mathcal{Y}^{\top}, \\
& \bar{N}_{22}^{4}=-\left(C_{1}^{\top}+C_{2}^{\top} D_{n}^{\top} D_{12}^{\top}\right) \mathcal{Z}-\mathcal{Z}^{\top}\left(C_{1}+D_{12} D_{n} C_{2}\right), \\
& \bar{N}_{23}^{4}=-\mathcal{Z}^{\top}\left(D_{11}+D_{12} D_{n} D_{21}\right)-\left(C_{1}^{\top}+C_{2}^{\top} D_{n}^{\top} D_{12}^{\top}\right) \mathcal{X}, \\
& \bar{N}_{24}^{4}=\mathcal{Z}^{\top}, \\
& \bar{N}_{33}^{4}=-\mathcal{X}^{\top}\left(D_{11}+D_{12} D_{n} D_{21}\right)-\left(D_{11}^{\top}+D_{21}^{\top} D_{n}^{\top} D_{12}^{\top}\right) \mathcal{X}, \\
& \bar{N}_{34}^{4}=\mathcal{X}^{\top} .
\end{aligned}
$$

Hence, substitution of (7.48) and (7.57) into (7.46), split according to (7.47a) and (7.47b), results in the bilinear synthesis inequality

\[
\left[\begin{array}{cccc}
N_{11} & N_{12} & N_{13} & N_{14}  \tag{7.58}\\
* & N_{22} & N_{23} & N_{24} \\
* & * & N_{33} & N_{34} \\
* & * & * & N_{44}
\end{array}\right] \prec 0
\]

where

$$
\begin{aligned}
N_{11}= & A Y_{1}+B_{2} C_{n}+Y_{1} A^{\top}+C_{n}{ }^{\top} B_{2}^{\top}-\left(Y_{1} C_{1}^{\top}+C_{n}^{\top} D_{12}^{\top}\right) \mathcal{Y} \\
& -\mathcal{Y}^{\top}\left(C_{1} Y_{1}+D_{12} C_{n}\right) \\
N_{12}= & A+A_{n}^{\top}+B_{2} D_{n} C_{2}-\left(Y_{1} C_{1}^{\top}+C_{n}^{\top} D_{12}^{\top}\right) \mathcal{Z}-\mathcal{Y}^{\top}\left(C_{1}+D_{12} D_{n} C_{2}\right) \\
N_{13}= & B_{1}+B_{2} D_{n} D_{21}-\mathcal{Y}^{\top}\left(D_{11}+D_{12} D_{n} D_{21}\right)-\left(Y_{1} C_{1}^{\top}+C_{n}^{\top} D_{12}^{\top}\right) \mathcal{X} \\
N_{14}= & \mathcal{Y}^{\top} \\
N_{22}= & X_{1} A+B_{n} C_{2}+A^{\top} X_{1}+C_{2}^{\top} B_{n}^{\top}-\left(C_{1}^{\top}+C_{2}^{\top} D_{n}^{\top} D_{12}^{\top}\right) \mathcal{Z} \\
& -\mathcal{Z}^{\top}\left(C_{1}+D_{12} D_{n} C_{2}\right) \\
N_{23}= & X_{1} B_{1}+B_{n} D_{21}-\mathcal{Z}^{\top}\left(D_{11}+D_{12} D_{n} D_{21}\right)-\left(C_{1}^{\top}+C_{2}^{\top} D_{n}^{\top} D_{12}^{\top}\right) \mathcal{X} \\
N_{24}= & \mathcal{Z}^{\top} \\
N_{33}= & \nu^{2} I-\mathcal{X}^{\top}\left(D_{11}+D_{12} D_{n} D_{21}\right)-\left(D_{11}^{\top}+D_{21}^{\top} D_{n}^{\top} D_{12}^{\top}\right) \mathcal{X} \\
N_{34}= & \mathcal{X}^{\top} \\
N_{44}= & -I
\end{aligned}
$$

Considering a particular channel $T_{j}$ as introduced in Section 7.3.3 gives the maximum gain synthesis BMI (7.16) and the LMI (7.17).

\section*{Part IV}
\section*{New Perspectives on Identification for Fault Detection and Isolation }
\section*{Chapter 8}
\section*{Recursive Identification of Structured Systems: An Instrumental-Variable Approach applied to Mechanical Systems ${ }^{1}$ }
\begin{abstract}
Many systems, including mechanical systems, rely on parametric models for analysis, advanced control, and online monitoring, particularly as they experience temporal changes due to varying operating conditions, degradation, external disturbances, or dynamic interactions, necessitating the monitoring of their time-varying dynamics through online estimation. The objective of this chapter is to develop a real-time estimation algorithm aimed at estimating time-varying dynamics within an interpretable model structure. To achieve this, an additive parameterization is adopted, offering enhanced parsimony, particularly suitable for mechanical systems. The proposed approach integrates the recursive simplified refined instrumental variable method with block-coordinate descent to minimize an exponentially-weighted output error cost function. This novel recursive identification method yields parametric continuous-time additive models applicable in both open-loop and closed-loop controlled systems. Numerical simulations demonstrate the effectiveness of the proposed method, further validated using experimental data to detect time-varying resonance mechanics.
\end{abstract}

\footnotetext{${ }^{1}$ The authors Koen Classens and Rodrigo González have had an equal contribution to this research. The results in this chapter constitute Contribution VII in Section 1.6. The chapter is based on: [42] K. Classens, R. A. González, and T. Oomen, "Recursive Identification of Structured Systems: An Instrumental-Variable Approach Applied to Mechanical Systems," submitted for journal publication.
}\subsection*{8.1 Introduction}
System identification involves obtaining mathematical models of complex phenomena from measured data, with wide-ranging applications across diverse fields [161, 236]. The development of accurate models enables the simulation, analysis, and prediction of complex system behavior. In control engineering, the models are key in designing effective control strategies, while its predictive capabilities support decision-making and planning.

Online system identification plays a significant role in dynamic environments where systems are prone to change over time [159, 277]. Unlike offline identification, which relies on a batch of data, online system identification continuously updates models in real-time as new data becomes available. Online system identification allows for quick adjustments to evolving conditions, which can enhance model accuracy, improve performance, and facilitate effective maintenance planning of various industrial process [40]. Consequently, it is instrumental in maintaining optimal system performance, enabling timely fault detection and diagnosis, and supporting adaptive control in dynamic unpredictable settings.

Estimation of continuous-time (CT) dynamical models of industrial processes offers advantages over discrete-time (DT) models. Continuous-time models can provide a more accurate representation of the underlying dynamics of physical systems, since they allow for the direct incorporation of a priori knowledge such as the relative degree of the physical systems they model [89]. The incorporation of prior knowledge in the form of interpretable physics has gained increasing importance in the fields of system identification, control, monitoring, and machine learning. In the context of linear system identification, incorporating relative degree information is particularly useful when the time evolution of the signals is naturally continuous, as seen in mechanical systems [91], where impulse response discontinuities are typically absent due to the double integration relationship between force and position.

While a single non-additive transfer function is commonly employed in linear system identification, practical applications in flexible motion systems [184, 268], and vibration analysis are more effectively conceptualized as a sum of transfer functions with distinct denominators. For example, mechanical systems are more naturally interpreted in modal form, where each submodel represents a different resonant mode [91]. Additive model parametrizations bring benefits such as enhanced physical insight for fault diagnosis [39] and improved numerical conditioning for high-order systems [112]. These parametrizations are also explored in statistics [121] and econometrics [119, 185], offering increased model flexibility and the ability to decentralize analysis for optimization and control purposes. Despite contributions in nonlinear discrete-time finite-impulse response and generalized Hammerstein model estimation [10,11], the utilization of additive model structures in system identification has been limited.

The identification of additive models has recently gained increasing atten-\\[0pt]
tion. An algorithm for offline estimation of continuous-time additive models in single-input single-output (SISO) systems, applicable to both open and closedloop configurations, has been introduced [112]. This approach is based on the simplified refined instrumental variable method, SRIVC [281], which consists of iteratively computing instrumental variable estimates of the system parameters. Alternatively, a block coordinate algorithm is utilized for identifying these systems under this structure with a tailored block-coordinate descent version of the SRIVC method [108].

In contrast to these offline algorithms, many recursive algorithms have been developed by parameterizing the model as a single non-additive transfer function [159, 277]. These online methods are typically based on a forgetting factor, or in state estimation mechanisms analogous to the Kalman filter. Most methods are categorized as modifications of offline identification methods, (non)linear filtering methods, stochastic approximation methods, and pseudolinear regression-based methods [159]. Methods for time-varying systems, such as [189, 190] explore open and closed-loop approaches based on prefiltering the input and output derivatives with ad-hoc prefilters, similar to the state-variable filter and refined instrumental variable methods for linear and time-invariant systems [88, 281].

Although current recursive methods can effectively track time-varying systems, these yield non-additive transfer function models with limited interpretability, as the estimated parameters may not directly correspond to physical system quantities. In this chapter, additive linear time-varying systems are considered for which the parameter variation is slow compared to the system dynamics. To track these systems, instrumental variable iterations are performed for each submodel until convergence, aligning with the philosophy of the block coordinate descent approach for non-convex optimization [163]. A comprehensive identification method for modeling additive linear time-varying continuous-time systems is proposed for both open and closed-loop settings. The method is referred to as recursive SRIVC for additive systems, or abbreviated as additive RSRIVC.

In summary, the main contributions are summarized as follows.\\
C1: A condition is provided under which it is preferable to estimate an additive model structure instead of an unfactored transfer function.

C2: A recursive continuous-time system identification algorithm is derived for systems in additive form, for both open and closed-loop systems and its consistency is assessed.

C3: The proposed method is evaluated through simulations and is experimentally validated on a flexible beam setup with a time-varying resonance mode.

The remainder of this chapter is organized as follows. In Section 8.2 the\\
system setup is presented and the problem is formulated. Section 8.3 elaborates on the benefits of the proposed approach, showcasing motivational examples and demonstrating where an additive decomposition is preferable to an unfactored transfer function. Section 8.4 contains the main contribution of this work, namely the derivation of a recursive identification method for continuous-time systems in additive form. The estimator is numerically evaluated in Section 8.5, and implemented on a true mechanical system in Section 8.6. Lastly, Section 8.7 concludes this chapter.

\subsection*{8.2 System setup and problem formulation}
Initially, the system setup for additive systems in open-loop and closed-loop configuration is outlined, followed by the formulation of the estimation problem.

\subsection*{8.2.1 System setup}
Consider the following continuous time (CT), linear and time-variant (LTV), single-input single-output (SISO) continuous-time system


\begin{equation*}
x(t)=G^{*}(p, t) u(t), \tag{8.1}
\end{equation*}


where $G^{*}(p, t)=B^{*}(p, t) / A^{*}(p, t)$, with $p$ being the Heaviside operator (i.e., $\left.p x(t)=\frac{\mathrm{d} x(t)}{\mathrm{d} t}\right)$. The terms $A^{*}(p, t)$ and $B^{*}(p, t)$ are polynomials in $p$ with timevarying coefficients of the form


\begin{align*}
& A^{*}(p, t)=a_{n}^{*}(t) p^{n}+a_{n-1}^{*}(t) p^{n-1}+\cdots+a_{1}^{*}(t) p+1  \tag{8.2}\\
& B^{*}(p, t)=b_{m}^{*}(t) p^{m}+b_{m-1}^{*}(t) p^{m-1}+\cdots+b_{1}^{*}(t) p+b_{0}^{*}(t) \tag{8.3}
\end{align*}


with $a_{n}^{*} \neq 0$ and $n \geq m$. These polynomials are assumed to be coprime, i.e., for any fixed $t$ along the parameter trajectory, they do not share roots in $p$.

The system in (8.1) is represented in its unfactored function form. Any nonadditive transfer function that represents (8.1) can also be described as a sum of time-variant systems of lower order. Such decomposition is called an additive form of (8.1), which is described by


\begin{align*}
x_{i}(t) & =G_{i}^{*}(p, t) u(t),  \tag{8.4a}\\
x(t) & =\sum_{i=1}^{K} x_{i}(t), \tag{8.4b}
\end{align*}


where $G_{i}^{*}(p, t)=B_{i}^{*}(p, t) / A_{i}^{*}(p, t)$, and $A_{i}^{*}(p, t)$ and $B_{i}^{*}(p, t)$ have the same form as (8.2) and (8.3), but have orders $n_{i}$ and $m_{i}$ with $a_{i, n_{i}}^{*} \neq 0, n_{i} \geq m_{i}$, and

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-196}
\captionsetup{labelformat=empty}
\caption{Figure 8.1: Block diagram for the open-loop setting studied in this chapter.}
\end{center}
\end{figure}

$\sum_{i=1}^{K} n_{i}=n$. That is,


\begin{align*}
& A_{i}^{*}(p, t)=a_{i, n_{i}}^{*}(t) p^{n_{i}}+a_{i, n_{i}-1}^{*}(t) p^{n_{i}-1}+\cdots+a_{i, 1}^{*}(t) p+1  \tag{8.5}\\
& B_{i}^{*}(p, t)=b_{i, m_{i}}^{*}(t) p^{m_{i}}+b_{i, m_{i}-1}^{*}(t) p^{m_{i}-1}+\cdots+b_{i, 1}^{*}(t) p+b_{i, 0}^{*}(t) . \tag{8.6}
\end{align*}


Without loss of generality, it is assumed that the polynomials $A_{i}^{*}(p, t)$ are jointly coprime and anti-monic, i.e., their constant coefficient is fixed to one. In addition, for each separate submodel to be identifiable, the system is parameterized such that at most one subsystem $G_{i}^{*}(p, t)$ is biproper. An approach to determine an additive decomposition of (8.1) is by calculating the partial fraction decomposition of $B^{*}(p, t) / A^{*}(p, t)$.

The polynomials $A_{i}^{*}(p, t)$ and $B_{i}^{*}(p, t)$ are jointly described by the parameter vector

\[
\theta_{i}^{*}(t)=\left[\begin{array}{llllllll}
a_{i, 1}^{*}(t) & a_{i, 2}^{*}(t) & \ldots & a_{i, n_{i}}^{*}(t) & b_{i, 0}^{*}(t) & b_{i, 1}^{*}(t) & \ldots & b_{i, m_{i}}^{*}(t) \tag{8.7}
\end{array}\right]^{\top},
\]

and

\[
\beta^{*}(t):=\left[\begin{array}{llll}
\theta_{1}^{* \top}(t) & \theta_{2}^{* \top}(t) & \ldots & \theta_{K}^{* \top}(t) \tag{8.8}
\end{array}\right]^{\top} .
\]

A noisy output measurement is retrieved at equidistantly-spaced time instants $t=t_{k}$, see Figure 8.1, i.e.,


\begin{equation*}
y\left(t_{k}\right)=x\left(t_{k}\right)+v\left(t_{k}\right) \tag{8.9}
\end{equation*}


where $v$ is a stochastic process of zero mean and finite variance $\sigma^{2}$.\\
If $G^{*}(p, t)$ is in closed loop, see Figure 8.2, it is assumed that the system is driven by a reference signal $r\left(t_{k}\right)$ and the control input is determined through the control law


\begin{equation*}
u\left(t_{k}\right)=C_{\mathrm{d}}(q)\left(r\left(t_{k}\right)-y\left(t_{k}\right)\right), \tag{8.10}
\end{equation*}


where $C_{\mathrm{d}}(q)$ denotes the (discrete-time) controller transfer function, and $q$ denotes the forward shift operator. It is assumed that the input sequence $u\left(t_{k}\right)$ is interpolated via a zero-order hold prior to exciting the system. In closed loop, the input is described as


\begin{equation*}
u\left(t_{k}\right)=\tilde{r}\left(t_{k}\right)-\tilde{v}\left(t_{k}\right) \tag{8.11}
\end{equation*}


\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-197}
\captionsetup{labelformat=empty}
\caption{Figure 8.2: Block diagram for the closed-loop setting studied in this chapter.}
\end{center}
\end{figure}

where


\begin{equation*}
\tilde{r}\left(t_{k}\right):=S_{u o}^{*}\left(q, t_{k}\right) r\left(t_{k}\right), \quad \tilde{v}\left(t_{k}\right):=S_{u o}^{*}\left(q, t_{k}\right) v\left(t_{k}\right), \tag{8.12}
\end{equation*}


with $S_{u o}^{*}\left(q, t_{k}\right)$ being the sensitivity function $S_{u o}^{*}\left(q, t_{k}\right)=C_{\mathrm{d}}(q) /[1+ \left.G_{\mathrm{d}}^{*}\left(q, t_{k}\right) C_{\mathrm{d}}(q)\right]^{-1}$, and $G_{\mathrm{d}}^{*}\left(q, t_{k}\right)$ being the zero-order-hold equivalent discretetime system of $G^{*}(p, t)=\sum_{i=1}^{K} G_{i}^{*}(p, t)$. Explicit expressions for computing $G_{\mathrm{d}}^{*}\left(q, t_{k}\right)$ are obtained when changes in the parameters in $A^{*}(p, t)$ and $B^{*}(p, t)$ occur only at the sampling time instants $t_{k}$, see Tóth [250, Chap. 6.3.1] for more details.

\subsection*{8.2.2 Problem formulation}
Consider the CT LTV system with input $u(t)$ and output $y(t)$ in (8.4). The data-generating system is described by (8.9) in the open-loop setting. In case the system is closed-loop controlled, the data-generating system additionally includes (8.10) and setpoint $r(t)$.

It is assumed that the plant is represented by the model set


\begin{align*}
x_{i}(t) & =G_{i}\left(p, t, \theta_{i}\right) u(t),  \tag{8.13a}\\
x(t) & =\sum_{i=1}^{K} x_{i}(t),  \tag{8.13b}\\
y\left(t_{k}\right) & =x\left(t_{k}\right)+v\left(t_{k}\right) . \tag{8.13c}
\end{align*}


Here $G_{i}\left(p, t, \theta_{i}\right)=B_{i}\left(p, t, \theta_{i}\right) / A_{i}\left(p, t, \theta_{i}\right)$, where the polynomials $A_{i}\left(p, t, \theta_{i}\right)$ and $B_{i}\left(p, t, \theta_{i}\right)$ have the same structure as (8.5) and (8.6) with $\theta_{i}(t) \in \mathbb{R}^{n_{\theta_{i}}}$, and $n_{\theta_{i}}=n_{i}+m_{i}+1$. The estimate of the total additive form is described by $\beta(t) \in \mathbb{R}^{n_{\beta}}$, which has the same structure as (8.8) and where $n_{\beta}=\sum_{i=1}^{K}\left(n_{i}+m_{i}\right)+K$.

The considered problem is to recursively estimate $\beta(t)$ which characterizes the model structure given by (8.13), based on the input and output data $\left\{u\left(t_{k}\right), y\left(t_{k}\right)\right\}_{k=1}^{N}$. Here, $N$ is the number of samples available at time $t_{N}$ for the open-loop scenario, while $\left\{r\left(t_{k}\right)\right\}_{k=1}^{N}$ is also known in the closed-loop setting.

Next, the primary benefits of additive parameterizations are presented, supported by multiple motivating examples.

\subsection*{8.3 Parsimony and interpretability in continuous-time system identification}
The estimation in an additive form has several benefits including the following.

\begin{enumerate}
  \item Direct continuous-time identification methods may suffer from a lack of parsimony when the sum of transfer functions of particular relative degrees is estimated [108].
  \item The parameters in an additive structure may be more interpretable compared to an unfactored transfer function.
  \item Typically, derivatives of the input and output data are required for recursive estimation. Considering an additive structure can significantly reduce the order of the derivatives to be estimated from sampled data.
\end{enumerate}

First, the advantages regarding parsimony and interpretability are described. Later, in Section 8.4.2, Remark 8.6, the additional advantage regarding the derivative order is highlighted.

Direct continuous-time identification methods such as the SRIVC method may suffer from a lack of parsimony when identifying a system that consists of the sum of transfer functions. The following proposition quantifies the additional parameters being estimated when opting to estimate (8.1) with a model structure $G(p)=B(p) / A(p)$ of relative degree $r$ instead of separately estimating the parameters for each transfer function $B_{i}(p) / A_{i}(p)$.\\[0pt]
Proposition 1. [108] Consider the system in (8.1), and the model structure $G(p)=\sum_{i=1}^{K} B_{i}(p) / A_{i}(p)$. Opting for the model structure $G(p)=B(p) / A(p)$ during identification (with the minimal relative degree $r$ encompassing the true system) results in a lack of parsimony for the latter model structure if and only if


\begin{equation*}
\sum_{i=1}^{K} r_{i}-r>K-1, \tag{8.14}
\end{equation*}


where $r_{i}=n_{i}-m_{i}$ is the relative degree of $B_{i}(p) / A_{i}(p)$. The surplus in (8.14) is the number of additional parameters considered by the model structure $G(p)= B(p) / A(p)$.

An immediate implication derived from Proposition 1 is that when $r=0$ or $r=1$, the model structure $G(p)=B(p) / A(p)$ suffers from a lack of parsimony if a transfer function $G_{i}^{*}(p)$ exists in its partial fraction expansion (8.1) with a relative degree greater than one. Two examples of this property are given below.\\
Example 8.1. Consider the system

$$
G^{*}(p)=\frac{b_{1,0}^{*}}{a_{1,1}^{*} p+1}+\frac{b_{2,0}^{*}}{a_{2,2}^{*} p^{2}+a_{2,1}^{*} p+1}
$$

where the $a_{i, j}^{*}$ values are strictly positive, and the $b_{i, 0}^{*}$ are non-zero. Only 5 parameters must be estimated if the following model structure is used:


\begin{equation*}
G(p)=\frac{b_{1,0}}{a_{1,1} p+1}+\frac{b_{2,0}}{a_{2,2} p^{2}+a_{2,1} p+1} \tag{8.15}
\end{equation*}


Alternatively, if the consideration of this partial fraction decomposition is disregarded and the decision is made to employ the model structure

$$
G(p)=\frac{b_{2} p^{2}+b_{1} p+b_{0}}{a_{3} p^{3}+a_{2} p^{2}+a_{1} p+1}
$$

with no constraints on the parameter values, then 6 parameters must be estimated. This model structure leads to a lack of parsimony compared to the one in (8.15).

Example 8.2. Consider the system

$$
G^{*}(p)=\frac{b_{0}^{*}}{a_{2}^{*} p^{2}+a_{1}^{*} p+1}
$$

where $a_{1}^{*}$ and $a_{2}^{*}$ are strictly positive, and $a_{1}^{*}>2 \sqrt{a_{2}^{*}}$. Hence, $G^{*}(p)$ has two real-valued poles. If the following partial fraction decomposition of $B^{*}(p) / A^{*}(p)$ is considered

$$
G(p)=\frac{b_{1,0}}{a_{1,1} p+1}+\frac{b_{2,0}}{a_{2,1} p+1}
$$

then 4 parameters must be estimated instead of 3. This additive representation does not directly take into account the relationship between the numerator terms $b_{1,0}$ and $b_{2,0}$, leading to a lack of parsimony.

The following example shows a mechanical system in one of its canonical forms. In addition to parsimony for these type of systems, also the interpretability of the parameters is examined.

Example 8.3. Mechanical systems are typically described as a sum of modes, i.e.,


\begin{equation*}
G^{*}(p)=\sum_{i=1}^{K} \frac{b_{i, 0}^{*}}{p^{2} / \omega_{i}^{2}+2\left(\zeta_{i} / \omega_{i}\right) p+1}, \tag{8.16}
\end{equation*}


where $\omega_{i}>0$ and $\zeta_{i}>0$ denote the natural frequency and the damping coefficient of the ith subsystem [91]. These systems are described by $3 K$ parameters if the following model structure is used:


\begin{equation*}
G(p)=\sum_{i=1}^{K} \frac{b_{i, 0}}{a_{i, 2} p^{2}+a_{i, 1} p+1} . \tag{8.17}
\end{equation*}


On the contrary, if the partial fraction decomposition is overlooked and the decision is made to adopt the widely utilized model structure of order $n$ and relative degree two


\begin{equation*}
G(p)=\frac{b_{n-2} p^{n-2}+b_{n-3} p^{n-3}+\ldots+b_{0}}{a_{n} p^{n}+a_{n-1} p^{n-1} \ldots+a_{1} p+1}, \tag{8.18}
\end{equation*}


with $n=2 K$ and no constraints on the parameter values, then a total of $4 K-1$ parameters must be estimated. This model structure leads to a lack of parsimony compared to the one in (8.17) if $K>1$, with a further degradation of parsimony for larger values of $K$. Additionally, the parameters in (8.17) are physically interpretable as these directly relate to the natural frequencies $\omega_{i}$ and the damping coefficients $\zeta_{i}$. This interpretability is hidden and mostly lost in (8.18).

Interpretability of the parameters, as illustrated in the latter example, is highly favourable in many engineering applications. For instance, in the field of fault diagnosis, this interpretability greatly simplifies the task of fault isolation, as faults often manifest in a specific submodel. Estimating the system in an unfactored form leads to variations in most parameters, while estimating it in an additive form results in variations solely in the directly related parameters.

\subsection*{8.4 Recursive estimation of continuous-time systems}
This section provides a recursive solution to estimate additive models. A key optimization tool that is considered in the approach is the block-descent method, which is examined in the next subsection. Then, the recursive estimators are presented for both open and closed-loop settings. Additionally, a summary of the algorithm is presented, adaptations for marginally stable systems are given, and practical aspects are described.

\subsection*{8.4.1 Block coordinate descent}
Towards the goal of computing a recursive estimator for $\beta^{*}$, consider the minimization problem


\begin{equation*}
\min _{\substack{\theta_{i} \in \Omega_{i} \\ i=1, \ldots, K}} V_{N}(\beta), \tag{8.19}
\end{equation*}


where $\Omega_{i} \subset \mathbb{R}^{n_{\theta_{i}}}$ is a compact set where the true parameters of the $i$ th subsystem are assumed to be. The cost $V_{N}$, using the $N$ available samples at $t_{N}$, is the weighted least squares output error cost function


\begin{equation*}
V_{N}(\beta)=\sum_{k=1}^{N} \alpha_{N-k}\left[y\left(t_{k}\right)-\sum_{i=1}^{K} G_{i}\left(p, t, \theta_{i}\right) u\left(t_{k}\right)\right]^{2}, \tag{8.20}
\end{equation*}


where the notation $G_{i}\left(p, t, \theta_{i}\right) u\left(t_{k}\right)$ means that the input $u(t)$ is filtered through the continuous-time transfer function $G_{i}\left(p, t, \theta_{i}\right)$ and later evaluated at $t=t_{k}$. The function $\alpha$ is a non-negative weighting function of the form


\begin{equation*}
\alpha_{k}=\lambda^{k}, \quad \lambda \in(0,1] . \tag{8.21}
\end{equation*}


Note that $\alpha$ satisfies the multiplicative property $\alpha_{k+1}=\lambda \alpha_{k}$, with $\alpha_{0}=1$. The parameter $\lambda$ is often referred to as the forgetting factor [161].

The approach for solving (8.19) consists of iteratively minimizing the cost with respect to $\theta_{i}$ while leaving the other variables fixed. Algorithm 1 describes the general procedure that is solved, typically after every incoming sample, known as block coordinate descent [163].

\begin{verbatim}
Algorithm 1 Block coordinate descent
    Choose an initial parameter vector $\theta_{i}^{1}$ for each $i$
    for $l=1,2, \ldots$ do
        for $i=1, \ldots, K$ do
            $\theta_{i}^{l+1} \leftarrow \arg \min _{\theta_{i} \in \Omega_{i}} V_{N}\left(\theta_{1}^{l+1}, \ldots, \theta_{i-1}^{l+1}, \theta_{i}, \theta_{i+1}^{l}, \ldots, \theta_{K}^{l}\right)$
        end for
    end for
\end{verbatim}

Let $\beta^{l}:=\left[\left(\theta_{1}^{l}\right)^{\top}, \ldots,\left(\theta_{K}^{l}\right)^{\top}\right]^{\top} \in \Omega \subset \mathbb{R}^{n_{\beta}}$, with $\Omega=\prod_{i=1}^{K} \Omega_{i}$ being the parameter space. Each iteration of the algorithm (in $l$ ) can therefore be written as $\beta^{l+1}=\mathcal{A}\left(\beta^{l}\right)$, where $\mathcal{A}: \mathbb{R}^{n_{\beta}} \rightarrow \mathbb{R}^{n_{\beta}}$ is the composite mapping $\mathcal{A}=\mathcal{T} \circ \mathcal{C}^{K} \circ \mathcal{T} \circ \mathcal{C}^{K-1} \circ \cdots \circ \mathcal{T} \circ \mathcal{C}^{1}$. This notation is adopted from [163]. Here, the choice function is defined as $\mathcal{C}^{i}(\beta):=(\beta, i)$, and the optimization function $\mathcal{T}(\beta, i):=\left(\theta_{1}, \ldots, \theta_{i-1}, \bar{\theta}_{i}, \theta_{i+1}, \ldots, \theta_{K}\right)$, with $\bar{\theta}_{i}$ being defined as $\bar{\theta}_{i}=\arg \min _{\theta_{i} \in \Omega_{i}} V_{N}\left(\theta_{1}, \ldots, \theta_{i}, \ldots, \theta_{K}\right)$. The following result concerns the convergence of Algorithm 1 to a stationary point of the cost (8.19).

Theorem 8.4 (Global convergence of Algorithm 1). If the search along any coordinate direction $\theta_{i}$ yields a unique minimum point of $V_{N}$, then the limit of any convergent subsequence of $\left\{\beta^{l}\right\}$ obtained from $\beta^{l+1}=\mathcal{A}\left(\beta^{l}\right)$ belongs to the set of fixed points $\Gamma_{N}=\left\{\beta \in \Omega: \nabla V_{N}(\beta)=0\right\}$.

Proof. See, e.g., Luenberger and Ye [163, Section 8.9].\\
Remark 8.5. The result in Theorem 8.4 still holds if the unique minimum assumption is replaced with the requirement that $V_{N}$ decreases at each coordinate search. In this case it is sufficient to find a parameter vector $\bar{\theta}_{i}$ that reduces the cost $V_{N}$ instead of minimizing it.

The block coordinate descent algorithm described in Algorithm 1 requires a way to compute $\mathcal{T}(\beta, i)$ at each iteration, for each $i=1,2, \ldots, K$. That is, given\\
the data up until $t_{N}$, compute


\begin{align*}
\theta_{i}^{l+1}= & \underset{\theta_{i} \in \Omega_{i}}{\arg \min } \sum_{k=1}^{N} \alpha_{N-k}\left[y\left(t_{k}\right)-\sum_{j=1}^{i-1} G_{j}\left(p, t, \theta_{j}^{l+1}\right) u\left(t_{k}\right)\right.  \tag{8.22}\\
& \left.-\sum_{j=i+1}^{K} G_{j}\left(p, t, \theta_{j}^{l}\right) u\left(t_{k}\right)-G_{i}\left(p, t, \theta_{i}\right) u\left(t_{k}\right)\right]^{2}
\end{align*}


for $i=1,2, \ldots, K$. For fixed values of $\left\{\theta_{j}^{l+1}\right\}_{j=1}^{i-1}$ and $\left\{\theta_{j}^{l}\right\}_{j=i+1}^{K}$. For a sufficiently large compact parameter space $\Omega_{i}$, the optimization problem in (8.22) becomes a nonlinear least-squares problem that lends itself to iterative solution methods. To this end, define the residual output of each submodel as

$$
\tilde{y}_{i}\left(t_{k}\right):=y\left(t_{k}\right)-\sum_{j=1}^{i-1} G_{j}\left(p, t, \theta_{j}^{l+1}\right) u\left(t_{k}\right)-\sum_{j=i+1}^{K} G_{j}\left(p, t, \theta_{j}^{l}\right) u\left(t_{k}\right)
$$

Substitution of this residual output in (8.22) gives


\begin{equation*}
\theta_{i}^{l+1}=\underset{\theta_{i} \in \Omega_{i}}{\arg \min } \sum_{k=1}^{N} \alpha_{N-k}\left[\tilde{y}_{i}\left(t_{k}\right)-G_{i}\left(p, t, \theta_{i}\right) u\left(t_{k}\right)\right]^{2} . \tag{8.23}
\end{equation*}


Next is shown how this is solved through recursive SRIVC.

\subsection*{8.4.2 Recursive SRIVC for additive open-loop systems}
In the linear and time-invariant case, when solved iteratively, the optimization problem in (8.23) leads to a local minimum for the complete optimization problem (8.19) [108]. Each optimization problem in (8.23) takes the form of minimizing the weighted sum of squares of the residual $\varepsilon_{i}\left(t_{k}, \theta_{i}\right)= \tilde{y}_{i}\left(t_{k}\right)-G_{i}\left(p, t, \theta_{i}\right) u\left(t_{k}\right)$. This pseudolinear regression takes the general form


\begin{equation*}
\tilde{y}_{i, \mathrm{f}}\left(t_{k}, \theta_{i}\right)=\varphi_{i, \mathrm{f}}^{\top}\left(t_{k}, \theta_{i}\right) \theta_{i}\left(t_{k}\right)+\varepsilon_{i}\left(t_{k}, \theta_{i}\right) \tag{8.24}
\end{equation*}


This generic form admits the refined instrumental variable iterations [281]. Essentially, the refined instrumental variable iterations provide a sequence which, under mild conditions, converges to a point $\bar{\theta}_{i}$ that satisfies the first order optimality condition of the cost in (8.23), i.e.,

$$
\left.\sum_{k=1}^{N} \alpha_{N-k} \frac{\partial \varepsilon_{i}\left(t_{k}, \theta_{i}\right)}{\partial \theta_{i}}\right|_{\theta_{i}=\bar{\theta}_{i}} \varepsilon_{i}\left(t_{k}, \bar{\theta}_{i}\right)=0 .
$$

The instrumental variable refinements, from now denoted as SRIVC iterations, are denoted using the iterate counter $s$, as


\begin{align*}
\theta_{i, s+1}^{l+1}= & {\left[\sum_{k=1}^{N} \alpha_{N-k} \hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right) \varphi_{i, \mathrm{f}}^{\top}\left(t_{k}, \theta_{i, s}^{l+1}\right)\right]^{-1} }  \tag{8.25}\\
& \times\left[\sum_{k=1}^{N} \alpha_{N-k} \hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right) \tilde{y}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right)\right]
\end{align*}


The regressors are denoted by $\phi$ and the instruments are denoted by $\hat{\phi}$. Appendix 8.A shows how (8.23) fits the pseudolinear regression framework with (8.24) and how this leads to the RSRIVC iterations in (8.25). In (8.25), the subscripts $i$ and $l$ refer to the current block-coordinate descent iterate. The filtered residual output, regressor vector and instrument vector, indicated with subscript $f$, are respectively given by

\[
\left.\begin{array}{rl}
\tilde{y}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right)= & \frac{1}{A_{i}\left(p, \theta_{i, s}^{l+1}\right)} \tilde{y}_{i}\left(t_{k}\right) \\
\varphi_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right)= & {\left[\frac{-p}{A_{i}\left(p, \theta_{i, s}^{l+1}\right)} \tilde{y}_{i}\left(t_{k}\right) \cdots \frac{-p^{n_{i}}}{A_{i}\left(p, \theta_{i, s}^{l+1}\right)} \tilde{y}_{i}\left(t_{k}\right)\right.} \\
& \left.\frac{1}{A_{i}\left(p, \theta_{i, s}^{l+1}\right)} u\left(t_{k}\right) \cdots \frac{p^{m_{i}}}{A_{i}\left(p, \theta_{i, s}^{l+1}\right)} u\left(t_{k}\right)\right]^{\top} \\
\hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right)= & {\left[\begin{array}{llll}
\frac{-p}{A_{i}\left(p, \theta_{i, s}^{l+1}\right)} \hat{x}_{i}\left(t_{k}\right) & \cdots & \frac{-p^{n_{i}}}{A_{i}\left(p, \theta_{i, s}^{l+1}\right)} \hat{x}_{i}\left(t_{k}\right)
\end{array}\right.} \\
& \frac{1}{A_{i}\left(p, \theta_{i, s}^{l+1}\right)} u\left(t_{k}\right) \cdots  \tag{8.28}\\
& \cdots \\
A_{i}\left(p, \theta_{i, s}^{l+1}\right)
\end{array} u\left(t_{k}\right)\right]^{\top}, \text { }
\]

where $\hat{x}_{i}\left(t_{k}\right)$ is the estimate of $x_{i}\left(t_{k}\right)$. This estimate is based on the latest parameter estimate of the previous time-sample, denoted by $\bar{\theta}_{i}\left(t_{k-1}\right)$, i.e.,

$$
\hat{x}_{i}\left(t_{k}\right)=\frac{B_{i}\left(p, \bar{\theta}_{i}\left(t_{k-1}\right)\right)}{A_{i}\left(p, \bar{\theta}_{i}\left(t_{k-1}\right)\right)} u\left(t_{k}\right)
$$

To clarify, the refinement step refers to updating the denominator polynomial of the prefiltering step in (8.26) to (8.28). Hence, the filtered output, regressor vector, and instrument vector, are updated based on the estimates obtained from the previous iterate $s$. The instrument vector is similar to the regressor vector but uses $\hat{x}_{i}$ instead of $\tilde{y}_{i}$, i.e., an estimate of the output of the submodel instead of using the noisy measured variable.

Remark 8.6. The regressor (8.27) and instrument (8.28) require the computation of derivatives up to order $m_{i}$ for the input and $n_{i}$ for the residual output.

Note that the order of the required derivatives is significantly lower compared to the case where a single unfactored transfer function would have been used. In this scenario, given that $\sum_{i=1}^{K} n_{i}=n$, obtaining the $n$th order derivative of the output is necessary for standard recursive continuous-time identification methods, along with a substantially higher derivative of the input.\\
Remark 8.7. Note that the expressions in (8.25) to (8.28) give the standard SRIVC iterations when $\tilde{y}\left(t_{k}\right)=y\left(t_{k}\right)$ (i.e., when $K=1$ and all the modes of the composite transfer function are estimated jointly) and $\alpha_{N-k}=1$ for all integer $k$ less than $N$ (i.e., when there is no forgetting factor).\\
Remark 8.8. The expressions provided in (8.26), (8.27) and (8.28) require prefiltering the data, relying implicitly on the commutativity property given by


\begin{equation*}
c G(p) x(t)=G(p)[c x(t)] \tag{8.29}
\end{equation*}


where $c$ is a constant. It is not recommended to prefilter the data by transfer functions when the parameters exhibit rapid temporal variations, since the property (8.29) does not hold in general when $c$ is time-varying [250]. However, it is important to note that (8.29) is a useful approximation in cases where the parameters exhibit slow temporal variations.

Instead of computing (8.25), an equivalent recursion is derived which does not involve the summation operator over all available samples and allows to achieve an update of the estimate after every incoming time sample. This recursion involves computing


\begin{align*}
K_{i, s+1}^{l+1}\left(t_{N}\right)= & \bar{P}_{i}\left(t_{N-1}\right) \hat{\varphi}_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right) \times \\
& \left(\lambda+\varphi_{i, \mathrm{f}}^{\top}\left(t_{N}, \theta_{i, s}^{l+1}\right) \bar{P}_{i}\left(t_{N-1}\right) \hat{\varphi}_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right)\right)^{-1}  \tag{8.30}\\
\theta_{i, s+1}^{l+1}\left(t_{N}\right)= & \bar{\theta}_{i}\left(t_{N-1}\right)+K_{i, s+1}^{l+1}\left(t_{N}\right) \times \\
& \left(\tilde{y}_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right)-\varphi_{i, \mathrm{f}}^{\top}\left(t_{N}, \theta_{i, s}^{l+1}\right) \bar{\theta}_{i}\left(t_{N-1}\right)\right) \tag{8.31}
\end{align*}


and


\begin{equation*}
P_{i, s+1}^{l+1}\left(t_{N}\right)=\frac{1}{\lambda}\left(\bar{P}_{i}\left(t_{N-1}\right)-K_{i, s+1}^{l+1}\left(t_{N}\right) \varphi_{i, \mathrm{f}}^{\top}\left(t_{N}, \theta_{i, s}^{l+1}\right) \bar{P}_{i}\left(t_{N-1}\right)\right), \tag{8.32}
\end{equation*}


where $\bar{\theta}_{i}\left(t_{N-1}\right)$, and $\bar{P}_{i}\left(t_{N-1}\right)$ denote the last parameter estimate and inverse of the matrix being inverted in (8.25) at the previous time instance $t_{N-1}$ after the $\mathcal{A}$-iterations over $l$ and $i$, and after the SRIVC iterations using $s$. The derivation of this recursion is presented in Appendix 8.B. Note that $\bar{P}_{i}\left(t_{N-1}\right)$ is interpreted as an approximate covariance matrix of the resulting estimate, see [278] and [192] for more details.

Next it is analyzed under which conditions the additive RSRIVC method yields a consistent estimator of the continuous-time system. To this end, consider the following assumptions to facilitate the analysis, theorem, and related remarks.

Assumption 8.9 (BIBO stability). The true system and the estimated models are uniformly bounded-input, bounded-output (BIBO) stable, i.e., for any initial time instant $t_{0}$ and any bounded input signal, the output sequence $\left\{y\left(t_{k}\right)\right\}_{k \geq 0}$ is bounded [214].

Assumption 8.10 (Persistence of excitation). The input sequence is persistently exciting of order no less than $2 n$.

Assumption 8.11 (Coprimeness and sampling frequency). The additive subsystems pertaining to the true system and the models obtained through the descent iterations do not have zero-pole cancellations. Furthermore, the sampling frequency is more than twice the largest positive imaginary part of the zeros of $\prod_{i=1}^{K} A_{i}\left(p, \theta_{i, s}^{l}\right) A_{i}^{*}(p)$. The subscript $s$ and superscript $l$ relate to the SRIVC iterations and block-coordinate descent iterations, respectively, defined in Section 8.4.

Assumption 8.12 (Independence and stationarity). The input sequence $\left\{u\left(t_{k}\right)\right\}$ is quasi-stationary [161, p. 34], the disturbance $\left\{v\left(t_{l}\right)\right\}$ is stationary, and they are mutually independent for all integers $k$ and $l$.

Assumption 8.13 (Parameter trajectories). The changes in the parameters occur only at the sampling instants $t_{k}$, i.e., the parameter trajectories are piecewise constant.

Theorem 8.14. Consider the proposed estimator for the open-loop setting in Figure 8.1 with $\alpha_{k}=1$ for all $k$, and suppose Assumptions 8.9 to 8.13 hold. In addition, assume that the SRIVC iterations of the estimator for all $N$ sufficiently large converge, where the converged parameter is denoted by $\theta_{i, \infty}^{l+1}$. Then, if both the proposed model and system converge as $N \rightarrow \infty$ to linear and time-invariant transfer functions, then their limits are equal with probability 1 and generically consistent ${ }^{2}$ with respect to the limiting system and model parameters. In the remainder of this work, the following is assumed primarily to facilitate the analysis.

The proof is provided in Appendix 8.C.\\
Remark 8.15. Note that consistency is only considered for $\alpha_{k}=1$ for all $k$. This is a special case of (8.21). In case (8.21) is used with a different constant-rate-forgetting factor $\lambda<1$, the effect of noise does not disappear for $N \rightarrow \infty$. Therefore, considering more data does not reduce the influence of measurement noise making the estimator not consistent. Variable-rate-forgetting factors may be considered, ensuring consistent estimators (see [29] for details); however, these considerations are beyond the scope of this work.

\footnotetext{${ }^{2}$ A statement $s$, which depends on the elements $\rho$ of some open set $\Omega \subseteq \mathbb{R}^{n}$, is generically true with respect to $\Omega[234]$ if the set $\{\rho \in \Omega \mid s(\rho)$ is not true $\}$ has Lebesgue measure zero in $\Omega$.
}\subsection*{8.4.3 Summary iterative recursive estimation algorithm}
The proposed additive RSRIVC algorithm involves nested iterations in $l, i$, and $s$. To facilitate implementation, the nested iterations are discussed in this subsection.

First of all, the recursive nature allows to update the estimate after every incoming sample. Therefore, these iterations will be performed at the discrete times $t_{k}$, where $k$ denotes the count in the discrete-time domain. At each of these points in time, the $\mathcal{A}$-iterations are performed $M_{l}$ times, see Section 8.4.1, i.e., the block-coordinate method runs through each of $K$ additive models for $M_{l}$ times. The current additive model is denoted by $i$ and the $\mathcal{A}$-iteration is denoted by $l$. Finally, within each of these parameter updates, $M_{s}$ SRIVC iterations are performed, with the current SRIVC iterate being indicated by $s$.

To summarize, compute the recursive formulas in (8.30), (8.31) and (8.32) at each time step and for every additive model $G_{i}$ to solve the additive model identification problem in (8.23). This way, the proposed method solves (8.23) recursively. The algorithm is presented in Algorithm 2. Note that the regressor vector has an explicit dependence on the parameter vector of the other models being estimated; this dependence is implicit in the equations via the expression for the residual $\tilde{y}_{i}$. A block-diagram of the block-coordinate descent RSRIVC algorithm for additive systems is shown in Figure 8.3. Two distinct blocks are highlighted in this diagram, which refer to the SRIVC and the block-coordinate descent step. These are executed after every incoming sample. Optionally, a stability test is included which is further examined in Section 8.4.6.

Remark 8.16. Note that the SRIVC refinements in $s$ are optional. In case the refinements are omitted, the optimization problem (8.23) is not being solved until convergence at every time-step, but rather a quasi-Newton descent algorithm of the form (8.25) is being implemented [278]. Despite this, the iterations stem from the fact that solving (8.23) and iterating over the additive models is a viable way to derive a model that fully minimizes the cost in (8.19).

Remark 8.17. While not inherently restrictive to the method, it is practically advantageous if parameters change gradually. In general, slower parameter variation improves the effectiveness of the methods by reducing computational burden, improving parameter adaptation efficiency, and robustness.

\begin{verbatim}
Algorithm 2 Recursive estimator for open-loop continuous-time systems in
additive form
    Initialization: Set $M_{l}, M_{s}, \lambda$. Initialize $\bar{\theta}_{i}\left(t_{N_{1}}\right)$, and $\bar{P}_{i}\left(t_{N_{1}}\right)$ for each $i=$
    $1, \ldots, K$, using the first part of the input-output data $\left\{u\left(t_{k}\right), y\left(t_{k}\right)\right\}_{k=-N_{\text {init }}}^{0}$
    with any linear and time-invariant continuous-time system identification
    method. With $\bar{\theta}_{i}\left(t_{0}\right)$ and the input-output data, initialize $\varphi_{i, \mathrm{f}}, \hat{\varphi}_{i, \mathrm{f}}$, and
    $\tilde{y}_{i, f}$.
    for $k=1, \ldots$ do
        $\theta_{i}^{1}\left(t_{k}\right) \leftarrow \bar{\theta}_{i}\left(t_{k-1}\right)$ for all $i$
        for $l=1, \ldots, M_{l}$ do
            for $i=1, \ldots, K$ do
                $\theta_{i, 0}^{l+1}\left(t_{k}\right) \leftarrow \theta_{i}^{l}\left(t_{k}\right)$
                for $s=0,1, \ldots, M_{s}$ do
                    Parameter $\theta_{i, s+1}^{l+1}\left(t_{k}\right)$ update:
        $K_{i, s+1}^{l+1}\left(t_{k}\right) \leftarrow \frac{\bar{P}_{i}\left(t_{k-1}\right) \hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right)}{\lambda+\varphi_{i, \mathrm{f}}^{\top}\left(t_{k}, \theta_{i, s}^{l+1}\right) \bar{P}_{i}\left(t_{k-1}\right) \hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right)}$,
        $\theta_{i, s+1}^{l+1}\left(t_{k}\right) \leftarrow \bar{\theta}_{i}\left(t_{k-1}\right)+K_{i, s+1}^{l+1}\left(t_{k}\right)\left(\tilde{y}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right)-\varphi_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right) \bar{\theta}_{i}\left(t_{k-1}\right)\right)$,
        $P_{i, s+1}^{l+1}\left(t_{k}\right) \leftarrow \frac{1}{\lambda} \bar{P}_{i}\left(t_{k-1}\right)-\frac{1}{\lambda} K_{i, s+1}^{l+1}\left(t_{k}\right) \varphi_{i, \mathrm{f}}^{\top}\left(t_{k}, \theta_{i, s}^{l+1}\right) \bar{P}_{i}\left(t_{k-1}\right)$.
                end for
                $\theta_{i}^{l+1}\left(t_{k}\right) \leftarrow \theta_{i, M_{s}+1}^{l+1}\left(t_{k}\right)$
                    $P_{i}^{l+1}\left(t_{k}\right) \leftarrow P_{i, M_{s}+1}^{l+1}\left(t_{k}\right)$
                end for
            end for
            $\bar{\theta}_{i}\left(t_{k}\right)=\theta_{i}^{M_{l}+1}\left(t_{k}\right)$ for all $i$
            $\bar{P}_{i}\left(t_{k}\right)=P_{i}^{M_{l}+1}\left(t_{k}\right)$ for all $i$
    end for
    Output: the parameter estimates $\left\{\bar{\theta}_{i}\left(t_{k}\right)\right\}_{k \geq 1}$.
\end{verbatim}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-208}
\captionsetup{labelformat=empty}
\caption{Figure 8.3: Block diagram of the block-coordinate descent RSRIVC for openloop systems, summarized by Algorithm 2. The section featuring SRIVC iterations is highlighted (■), see (8.25) to (8.28), alongside the block containing the \$\textbackslash mathcal\{A}\$-iterations ( ), described in Section 8.4.1.\}\end{center}
\end{figure}

\subsection*{8.4.4 Closed-loop case}
The identical recursion as in (8.30), (8.31) and (8.32) applies to closed-loop scenario, albeit with a different instrument vector. Employing an output error cost for system identification is known to introduce asymptotic bias in the presence of output measurement noise [256]. Consequently, instrumental variable methods are used in the closed-loop setting to mitigate this bias [98, 106]. Instead of (8.23), the instrumental variable solution is employed which is given by

$$
\theta_{i}^{l+1} \in \operatorname{sol}_{\theta_{i} \in \Omega_{i}}\left\{\sum_{k=1}^{N} \alpha_{N-k} \hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i}\right)\left(\tilde{y}_{i, \mathrm{f}}\left(t_{k}, \theta_{i}\right)-\varphi_{i, \mathrm{f}}^{\top}\left(t_{k}, \theta_{i}\right) \theta_{i}\left(t_{k}\right)\right)=0\right\},
$$

with $\hat{\varphi}_{i, \mathrm{f}}$ being the filtered instrument vector related to additive model $i$. Users have the flexibility to choose from various instrument vectors [234]. In this context, the instrument is constructed as a noiseless version of the regressor, a choice recognized in similar identification contexts for yielding minimal asymptotic covariance among all instrumental variable estimators $[24,97,112,174]$. To this end, the instrument (8.28) is rewritten as


\begin{align*}
\hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right)= & {\left[\frac{-p}{A_{i}\left(p, \theta_{i, s}^{l+1}\right)} \frac{B_{i}\left(p, \bar{\theta}_{i}\left(t_{k-1}\right)\right)}{A_{i}\left(p, \bar{\theta}_{i}\left(t_{k-1}\right)\right)} \cdots \frac{-p^{n_{i}}}{A_{i}\left(p, \theta_{i, s}^{l+1}\right)} \frac{B_{i}\left(p, \bar{\theta}_{i}\left(t_{k-1}\right)\right)}{A_{i}\left(p, \bar{\theta}_{i}\left(t_{k-1}\right)\right)}\right.} \\
& \left.\frac{1}{A_{i}\left(p, \theta_{i, s}^{l+1}\right)} \cdots \frac{p^{m_{i}}}{A_{i}\left(p, \theta_{i, s}^{l+1}\right)}\right]^{\top} z\left(t_{k}\right) \tag{8.33}
\end{align*}


however, instead of $z\left(t_{k}\right)=u\left(t_{k}\right)$, the noiseless input is estimated as $z\left(t_{k}\right)=S_{\text {uo }}\left(q, t_{k}\right) r\left(t_{k}\right)$, where the sensitivity function $S_{\text {uo }}\left(q, t_{k}\right)=C_{\mathrm{d}}(q) /[1+ \left.C_{\mathrm{d}}(q) G_{\mathrm{d}}\left(q, t_{k}\right)\right]^{-1}$. Hence, the instrument now depends on an estimate of the control input, computed by means of a model of the sensitivity and the known setpoint $r\left(t_{k}\right)$.

\subsection*{8.4.5 Marginally stable systems}
Mechatronic position systems, frequently encountered in engineering applications, lend themselves to a decomposition comprising both rigid body and flexible modes. Notably, the rigid body modes are considered as double integrators, leading to eigenvalues precisely situated at the origin. The denominator polynomial of such marginally stable system cannot be represented in an anti-monic form as in (8.5) and (8.6).

The proposed approach is extended to the estimation of marginally stable systems as follows. Consider the system (8.4), but where the first submodel has $\ell$ poles at the origin. In other words, let $G_{1}^{*}(p, t)=B_{1}^{*}(p, t) /\left[p^{\ell} A_{1}^{*}(p, t)\right]$, where $A_{1}^{*}(p, t)$ and $B_{1}^{*}(p, t)$ are co-prime, and have the same form as in (8.2) and (8.3). For either open or closed-loop variants of the proposed approach, the computation of the gradient of each submodel with respect to their respective parameter vector is required. For $G_{1}(p)$, this computation leads to the following instrument vector.


\begin{align*}
\hat{\varphi}_{1, \mathrm{f}}\left(t_{k}, \theta_{1, s}^{l+1}\right)= & {\left[\frac{-p}{p^{\ell} A_{1}\left(p, \theta_{1, s}^{l+1}\right)} \frac{B_{1}\left(p, \bar{\theta}_{i}\left(t_{k-1}\right)\right)}{A_{1}\left(p, \bar{\theta}_{i}\left(t_{k-1}\right)\right)} \cdots \frac{-p^{n_{1}}}{p^{\ell} A_{1}\left(p, \theta_{1, s}^{l+1}\right)} \frac{B_{1}\left(p, \bar{\theta}_{i}\left(t_{k-1}\right)\right)}{A_{1}\left(p, \bar{\theta}_{i}\left(t_{k-1}\right)\right)}\right.} \\
& \left.\frac{1}{p^{\ell} A_{1}\left(p, \theta_{1, s}^{l+1}\right)} \cdots \frac{p^{m_{1}}}{p^{\ell} A_{1}\left(p, \theta_{1, s}^{l+1}\right)}\right]^{\top} z\left(t_{k}\right) \tag{8.34}
\end{align*}


where $z\left(t_{k}\right)=u\left(t_{k}\right)$ for the open-loop algorithm, and $z\left(t_{k}\right)=S_{u o}\left(q, t_{k}\right) r\left(t_{k}\right)$ for the closed-loop variant. On the other hand, the model residual retains the same\\
form as in (8.23), with the filtered residual output given by (8.24), but with the regressor vector now expressed as


\begin{align*}
\varphi_{1, \mathrm{f}}\left(t_{k}, \theta_{1, s}^{l+1}\right)= & {\left[\frac{-p}{A_{1}\left(p, \theta_{1, s}^{l+1}\right)} \tilde{y}_{1}\left(t_{k}\right) \cdots \frac{-p^{n_{1}}}{A_{1}\left(p, \theta_{1, s}^{l+1}\right)} \tilde{y}_{1}\left(t_{k}\right)\right.} \\
& \left.\frac{1}{p^{l} A_{1}\left(p, \theta_{1, s}^{l+1}\right)} u\left(t_{k}\right) \cdots \frac{p^{m_{1}}}{p^{l} A_{1}\left(p, \theta_{1, s}^{l+1}\right)} u\left(t_{k}\right)\right]^{\top} \tag{8.35}
\end{align*}


By computing the iterations in (8.25) with the first instrument and regressor vectors given by (8.34) and (8.35) respectively, a direct extension of the proposed method for identifying marginally stable systems in additive form is obtained.

Remark 8.18. Assumption 8.9 is exclusively employed in the generic consistency proof for uniformly stable systems, excluding marginally stable systems, yet it does not constrain the proposed approach.

\subsection*{8.4.6 Practical aspects for implementation}
Next, several practical aspects of the additive RSRIVC implementation are investigated. First, the robustness of the algorithm is addressed. Subsequently, to alleviate the computational load associated with the approach, output subsampling is examined.

In the offline SRIVC method, ensuring model stability is vital, especially in challenging conditions like low Signal-to-Noise Ratio (SNR) or mismatched model orders [116]. Typically, stability is monitored and enforced throughout iterations [90]. In RSRIVC, where both prefilter and auxiliary model depend on preceding estimates, stability verification is more intricate. For slowly-varying systems, checking characteristic equation poles and reflecting right-half plane poles to the left suffices for confirming bounded-input, bounded-output stability under specific conditions [189]. Notably, in time-varying parameter models, uniform exponential stability is assured by assuming slow parameter changes and keeping system matrix eigenvalues on the left half plane. RSRIVC incorporates a projection algorithm to guarantee the stability of the prefilter and auxiliary model, reflecting unstable poles into the left half plane while preserving the magnitude characteristics. This approach addresses potential stability issues that may arise, particularly at the beginning of the data record.

The algorithm presented is theoretically sound, however, the recursion in (8.32) may not be numerically reliable [159, Chapter 11]. Since $P\left(t_{k}\right)$ is essentially computed by successive subtractions, round-off errors can accumulate and make $P\left(t_{k}\right)$ ill-conditioned. Other continuous-time recursive methods based on instrumental variables [190, 193] incorporate a covariance matrix $Q\left(t_{k}\right)$ that is added to $P\left(t_{k}\right)$ at each iteration. The proposed algorithm does not include this matrix since it is not compatible with the coordinate descent interpretation of\\
the iterations in Algorithm 2. However, from a practical perspective, including this matrix can help with the convergence of the parameters.

Subsampling data may be valuable to managing computational complexity without compromising significant accuracy. This facilitates the implementation of recursive algorithms in resource-constrained environments. Subsampling should be done cautiously to ensure that the subsampled data still captures the smallest timescale for a reliable estimate. Therefore, it is primarily applicable when the timescale of the system dynamics is considerably shorter than the sampling time. When subsampling the output, it is advised to take all input samples into account for the estimation of the filtered input and filtered auxiliary variables. Hence, the block-coordinate descent algorithm and additive RSRIVC iterations may be performed at this lower sampling rate. Furthermore, as hinted in Remark 8.16, it is not always necessary to perform multiple iterations. In case the parameters of the system vary slowly with respect to the sampling period, typically no refinements are necessary and a single $\mathcal{A}$-iteration can achieve accurate estimates.

\subsection*{8.5 Numerical Simulation}
In this section, the proposed method is tested in a practically relevant simulation example. Consider the following additive system

$$
G^{*}(p, t)=\sum_{i=1}^{3} \frac{b_{i, 0}^{*}(t)}{a_{i, 2}^{*}(t) p^{2}+a_{i, 1}^{*}(t) p+1}
$$

with initial DC-gains $\left[\begin{array}{lll}b_{1,0}^{*}(0) & b_{2,0}^{*}(0) & b_{3,0}^{*}(0)\end{array}\right]=\left[\begin{array}{lll}3 & 0.5 & 1\end{array}\right]$, and associated denominator parameters $\left[\begin{array}{lll}a_{1,2}^{*}(0) & a_{2,2}^{*}(0) & a_{3,2}^{*}(0)\end{array}\right]=\left[\begin{array}{lll}0.25 & 0.04 & 0.0025\end{array}\right]$, and $\left[\begin{array}{lll}a_{1,1}^{*}(0) & a_{2,1}^{*}(0) & a_{3,1}^{*}(0)\end{array}\right]=\left[\begin{array}{lll}0.25 & 0.01 & 0.01\end{array}\right]$. The output $y\left(t_{k}\right)$ is measured at a sampling frequency of 20 Hz and is contaminated by a zero-mean Gaussian white noise with a variance of 0.01 . The system is operated in closedloop with feedback controller

$$
K(q)=\frac{0.02329 q^{2}-0.2058 q+0.00454}{q^{2}-q}
$$

and the setpoint equals $r(t)=3 \sin (0.005 t)+\sin (2 t)+\sin (5 t)+\sin (17.5 t)$. The algorithm is initialized with covariance matrices $\bar{P}_{i}\left(t_{N_{1}}\right)=\operatorname{diag}\left(10^{-3}, 10^{-3}, 10^{-5}\right)$, and forgetting factor $\lambda=0.999$. The estimator is initialized at a random system with parameters $\bar{\theta}_{i}\left(t_{N_{1}}\right)=\theta_{i}^{*}(0)(1+\mathcal{U}(-0.02,0.02))$, where $\mathcal{U}(a, b)$ denotes a uniform distribution with lower limit $a$ and upper limit $b$.

First, in the interval $t \in[100,300] \mathrm{s}$, the parameters $a_{1,1}, a_{1,2}$, and $a_{2,1}$, are varied in such a way that the corresponding natural frequency $\omega_{1}$ and damping

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-212}
\captionsetup{labelformat=empty}
\caption{Figure 8.4: Parameter estimates (-) and true parameters (---) related to the first submodel with decreasing denominator parameters.}
\end{center}
\end{figure}

coefficient $\zeta_{2}$ decrease, c.f., (8.16). Subsequently, in the interval $t \in[500,800] \mathrm{s}$, the denominator parameters of the third model, i.e., $a_{3,2}$ and $a_{3,1}$ are varied.

Algorithm 2 is deployed to estimate the time-varying parameters, where no SRIVC iterations are performed and only a single coordinate descent iteration is performed at every incoming sample, i.e., the two loops in the algorithm are omitted as $s=0$ and $M_{l}=1$. Since the system is operating in closed loop, the instrument is chosen as (8.33). The obtained estimates and the true parameters are depicted for each of the submodels in Figures 8.4 to 8.6. The estimated model at $t=50 \mathrm{~s}$ and $t=950 \mathrm{~s}$ are depicted in Figure 8.7. From the figures it is concluded that no further refinements and gradient descent steps are required to achieve a accurate tracking of the parameters.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-213(1)}
\captionsetup{labelformat=empty}
\caption{Figure 8.5: Parameter estimates (-) and true parameters (---) related to the second submodel with decreasing damping coefficient \$\textbackslash zeta\_\{2}\$, which corresponds to an increasing $a_{21}$.\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-213}
\captionsetup{labelformat=empty}
\caption{Figure 8.6: Parameter estimates (-) and true parameters (---) related to the third submodel with decreasing natural frequency \$\textbackslash omega\_\{3}\$, which corresponds to an increasing $a_{32}$ and decreasing $a_{31}$.\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-214}
\captionsetup{labelformat=empty}
\caption{Figure 8.7: Plant estimate at \$t=50 \textbackslash mathrm\{\~{}s}\$ (—) and at $t=950 \mathrm{~s}$ (—). Clearly, all resonances changed. Note that the natural frequency of the first mode decreased and the damping of the second mode decreased. A combination of the two occurred at the third mode.\}\end{center}
\end{figure}

\subsection*{8.6 Experimental validation on an overactuated and oversensed flexible beam setup}
In this section, the method is tested on an experimental setup. To this end, consider the setup shown in Figure 8.8. The system consists of a thin flexible steel beam of $500 \times 20 \times 2 \mathrm{~mm}$. It is equipped with five contactless fiberoptic sensors and three voice-coil actuators and is suspended by wire flexures, leaving one rotational and one translational direction unconstrained. The system is operating at a sampling frequency of 4096 Hz . The second and fourth sensor are not used for the conducted experiments.

The main aim is to estimate the SISO system $G^{*}$ between the input $u$, exciting the system at the outer ends of the beam equally, see Figures 8.8 and 8.9, and the output $y$ which is the average deflection of the outer ends. This average deflection is controlled by a feedback controller and the setpoint is a square wave with a frequency of 2 Hz . By means of a second control loop, the internal stiffness of the beam is artificially manipulated, see [39, 43] for details.

Lightly-damped systems such as the one under study are well-suited to be described in a modal representation, similar to Example 8.3, i.e.,

$$
G^{*}(p)=\sum_{i=1}^{K} \frac{b_{i, 0}^{*}}{p^{2} / \omega_{i}^{2}+2\left(\zeta_{i} / \omega_{i}\right) p+1},
$$

where $\zeta_{i}$ and $\omega_{i}$ represent the damping coefficients and natural frequencies of the flexible modes respectively, see [91]. The flexible beam fits the modal description, and the system is approximated with $K=2$ modes. The first mode is the\\
suspension

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-215(1)}
\captionsetup{labelformat=empty}
\caption{Figure 8.8: Prototype experimental flexible beam setup. The moving part is indicated by (a) and is suspended by wire flexures (b). The deflection is measured with five contactless fiber optic sensors, of which two are used (c) and the setup is actuated with three current-driven voice coils of which the outer two are used (d).}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-215}
\captionsetup{labelformat=empty}
\caption{Figure 8.9: Schematic illustration of the experimental setup. The three actuators are visible on the bottom part of the image and the five sensors at the top. During operation, the beam translates, rotates and exhibits internal flexible behavior. The inputs and outputs are transformed as \$u\_\{1}=u\_\{2\}=\textbackslash frac\{1\}\{2\} u\$ and $y=\frac{1}{2}\left(y_{1}+y_{2}\right)$, respectively.\}\end{center}
\end{figure}

mode of the system and the second mode is the first internal flexible mode which is artificially manipulated. Figure 8.12 shows the model of the system in the frequency domain, including the neglected higher order modes, and the model at $t_{0}$ after initializing the system. Two distinct forgetting factors are used tailored to the distinct modes, namely $\lambda_{1}=0.9999$ and $\lambda_{2}=0.999$, and the approximate covariance matrices are initialized at $\bar{P}_{1}\left(t_{0}\right)=\bar{P}_{2}\left(t_{0}\right)=\operatorname{diag}\left(10^{-11}, 10^{-6}, 10^{-1}\right)$.

An experiment is conducted where the internal flexible mode is actively manipulated from $t \in[40,100] \mathrm{s}$. Algorithm 2 is deployed to estimate the time-

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-216}
\captionsetup{labelformat=empty}
\caption{Figure 8.10: Parameter estimate related to the first submodel. This first mode relates to the suspension of the flexible beam setup which remains unaltered throughout the experiment explaining the graphs.}
\end{center}
\end{figure}

varying behavior, where no SRIVC iterations are performed and only a single coordinate descent iteration is performed at every incoming sample, i.e., $s=0$ and $M_{l}=1$. Since the system is operating in closed loop, the instrument is chosen as (8.33). The obtained estimates are depicted for each of the two submodels in Figures 8.10 and 8.11, and the estimated model at $t=0 \mathrm{~s}$ and $t=135$ s is depicted in Figure 8.12. From the figures it is concluded that the shifting resonance mode is effectively observed, and the first mode does not exhibit parameter variations. The small fluctuations in the estimate of the second mode are at the frequency of the setpoint.

\subsection*{8.7 Conclusions}
This chapter adresses a recursive method to identify additive systems for both open and closed-loop setups. The proposed algorithms, based on a blockcoordinate descent with refined instrumental variables, are capable to track time-varying parameters of continuous-time systems in real-time. This allows tracking of more parsimonious and physically-relevant model representations, particularly for mechanical systems. Practical aspects are provided and finally the effectiveness of the proposed approach is illustrated through numerical vali-

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-217(1)}
\captionsetup{labelformat=empty}
\caption{Figure 8.11: Parameter estimate related to the second submodel. This mode is artificially manipulated between \$t=40 \textbackslash mathrm\{\~{}s}\$ and $t=100 \mathrm{~s}$, which explains the changing parameters.\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-217}
\captionsetup{labelformat=empty}
\caption{Figure 8.12: Frequency response function of the nonmanipulated system (—), the plant estimate at \$t=t\_\{N\_\{1}\}=0 \textbackslash mathrm\{\~{}s\}(-)\$, and at the plant estimate at $t=135$ s (一).\}\end{center}
\end{figure}

dation and application on a real-time setup.

\section*{Appendices}
\section*{8.A Derivation of the refined instrumental variable iterations}
In this appendix, it is shown that a stationary point of the following optimization problem is obtained from the refined instrumental variable method. Recall the optimization problem


\begin{equation*}
\theta_{i}^{l+1}\left(t_{N}\right)=\underset{\theta_{i} \in \Omega_{i}}{\arg \min } \sum_{k=1}^{N} \alpha_{N-k}\left[\tilde{y}_{i}\left(t_{k}\right)-G_{i}\left(p, t, \theta_{i}\right) u\left(t_{k}\right)\right]^{2} \tag{8.36}
\end{equation*}


with

$$
\tilde{y}_{i}\left(t_{k}\right):=y\left(t_{k}\right)-\sum_{j=1}^{i-1} G_{j}\left(p, t, \theta_{j}^{l+1}\right) u\left(t_{k}\right)-\sum_{j=i+1}^{K} G_{j}\left(p, t, \theta_{j}^{l}\right) u\left(t_{k}\right)
$$

and define

$$
\varepsilon_{i}\left(t_{k}, \theta_{i}\right)=\tilde{y}_{i}\left(t_{k}\right)-G_{i}\left(p, t, \theta_{i}\right) u\left(t_{k}\right) .
$$

By taking Remark 8.17 into consideration, this residual is well approximated as

$$
\varepsilon_{i}\left(t_{k}, \theta_{i}\right)=\frac{1}{A_{i}\left(p, \theta_{i}\right)}\left(A_{i}\left(p, \theta_{i}\right) \tilde{y}_{i}\left(t_{k}\right)-B_{i}\left(p, \theta_{i}\right) u\left(t_{k}\right)\right)
$$

and subsequently $A_{i}\left(p, \theta_{i}\right)$ and $B_{i}\left(p, \theta_{i}\right)$ gives

$$
\begin{aligned}
\varepsilon_{i}\left(t_{k}, \theta_{i}\right)= & \frac{1}{A_{i}\left(p, \theta_{i}\right)}\left(a_{i, n_{i}}\left(t_{k}\right) p^{n_{i}} \tilde{y}_{i}\left(t_{k}\right)+\ldots\right. \\
& +a_{i, 1}\left(t_{k}\right) p \tilde{y}_{i}\left(t_{k}\right)+\tilde{y}_{i}\left(t_{k}\right)-b_{i, m_{i}}\left(t_{k}\right) p^{m_{i}} u\left(t_{k}\right)-\ldots \\
& \left.-b_{i, 1}\left(t_{k}\right) p u\left(t_{k}\right)-b_{i, 0}\left(t_{k}\right) u\left(t_{k}\right)\right)
\end{aligned}
$$

The filtered residual output, filtered regressor vector, and parameter vector are defined as

$$
\begin{aligned}
\tilde{y}_{i, \mathrm{f}}\left(t_{k}, \theta_{i}\right) & =\frac{1}{A_{i}\left(p, \theta_{i}\right)} \tilde{y}_{i}\left(t_{k}\right) \\
\varphi_{i, \mathrm{f}}\left(t_{k}\right) & =\frac{1}{A_{i}\left(p, \theta_{i}\right)}\left[\begin{array}{lllllll}
-p \tilde{y}_{i}\left(t_{k}\right) & \ldots & -p^{n_{i}} \tilde{y}_{i}\left(t_{k}\right) & u\left(t_{k}\right) & p u\left(t_{k}\right) & \ldots & p^{m_{i}} u\left(t_{k}\right)
\end{array}\right]^{\top} \\
\theta_{i}\left(t_{k}\right) & =\left[\begin{array}{llllll}
a_{i, 1}\left(t_{k}\right) & \ldots & a_{i, n_{i}}\left(t_{k}\right) & b_{i, 0}\left(t_{k}\right) & b_{i, 1}\left(t_{k}\right) & \ldots
\end{array} b_{i, m_{i}}\left(t_{k}\right)\right]^{\top}
\end{aligned}
$$

which allows to write $\varepsilon_{i}$ as


\begin{equation*}
\varepsilon_{i}\left(t_{k}, \theta_{i}\right)=\tilde{y}_{i, \mathrm{f}}\left(t_{k}, \theta_{i}\right)-\varphi_{i, \mathrm{f}}^{\top}\left(t_{k}, \theta_{i}\right) \theta_{i}\left(t_{k}\right) \tag{8.37}
\end{equation*}


which is a pseudolinear regression problem in $\theta_{i}$ that admits the refined instrumental variable [88, 277] iterations. To this end, the parameter estimate of the previous SRIVC iterate $s$ is taken in the denominator polynomial $A_{i}$ which is used as prefilter for $\tilde{y}_{i, \mathrm{f}}$ and $\varphi_{i, \mathrm{f}}$. Then, substitution in the first order optimality condition of (8.36), i.e.,

$$
\sum_{k=1}^{N} \alpha_{N-k} \frac{\partial \varepsilon_{i}\left(t_{k}, \theta_{i}\right)}{\partial \theta_{i}} \varepsilon_{i}\left(t_{k}, \theta_{i}\right)=0
$$

with the instrument vector $\hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i}\right)$ gives

$$
\sum_{k=1}^{N} \alpha_{N-k} \hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i}\right)\left(\tilde{y}_{i, \mathrm{f}}\left(t_{k}, \theta_{i}\right)-\varphi_{i, \mathrm{f}}^{\top}\left(t_{k}, \theta_{i}\right) \theta_{i}\left(t_{k}\right)\right)=0 .
$$

Hence, the iterations for a fixed block descent iteration $l+1$ are of the form


\begin{align*}
\theta_{i, s+1}^{l+1}\left(t_{N}\right)= & {\left[\sum_{k=1}^{N} \alpha_{N-k} \hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right) \varphi_{i, \mathrm{f}}^{\top}\left(t_{k}, \theta_{i, s}^{l+1}\right)\right]^{-1} } \\
& \times\left[\sum_{k=1}^{N} \alpha_{N-k} \hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right) \tilde{y}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right)\right] \tag{8.38}
\end{align*}


with $s \geq 1$.

\section*{8.B Recursive computation of the SRIVC iterations}
In order to reformulate (8.25) into its recursive form, define


\begin{equation*}
R_{i, s}^{l+1}\left(t_{N}\right)=\sum_{k=1}^{N} \alpha_{N-k} \hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right) \varphi_{i, \mathrm{f}}^{\top}\left(t_{k}, \theta_{i, s}^{l+1}\right) \tag{8.39}
\end{equation*}


and


\begin{equation*}
f_{i, s}^{l+1}\left(t_{N}\right)=\sum_{k=1}^{N} \alpha_{N-k} \hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right) \tilde{y}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right) \tag{8.40}
\end{equation*}


Using that $\alpha_{k}=\lambda^{k}$ and that $\alpha$ satisfies the multiplicative property $\alpha_{k+1}=\lambda \alpha_{k}$, with $\alpha_{0}=1$,


\begin{equation*}
R_{i, s}^{l+1}\left(t_{k}\right)=\lambda \bar{R}_{i}\left(t_{k-1}\right)+\hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right) \varphi_{i, \mathrm{f}}^{\top}\left(t_{k}, \theta_{i, s}^{l+1}\right) \tag{8.41}
\end{equation*}


and


\begin{equation*}
f_{i, s}^{l+1}\left(t_{k}\right)=\lambda \bar{f}_{i}\left(t_{k-1}\right)+\hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right) \tilde{y}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, s}^{l+1}\right) \tag{8.42}
\end{equation*}


where $\bar{R}_{i}\left(t_{k-1}\right)$ and $\bar{f}_{i}\left(t_{k-1}\right)$ denote the final value of the iterations at the previous time step $t_{k-1}$ (i.e., after iterating over the modes through $i$ and $l$, and after the iterations in $s$ ). Using that $\theta_{i, s+1}^{l+1}\left(t_{N}\right)=\left(R_{i, s}^{l+1}\left(t_{N}\right)\right)^{-1} f_{i, s}^{l+1}\left(t_{N}\right)$, and $\bar{f}_{i}\left(t_{k-1}\right)=\bar{R}_{i}\left(t_{k-1}\right) \bar{\theta}_{i}\left(t_{k-1}\right)$,

$$
\theta_{i, s+1}^{l+1}\left(t_{N}\right)=\left(R_{i, s}^{l+1}\left(t_{N}\right)\right)^{-1}\left(\lambda \bar{R}_{i}\left(t_{N-1}\right) \bar{\theta}_{i}\left(t_{N-1}\right)+\hat{\varphi}_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right) \tilde{y}_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right)\right)
$$

which after substitution of $\bar{R}_{i}\left(t_{k-1}\right)$ from (8.41) is written as

$$
\begin{aligned}
\theta_{i, s+1}^{l+1}\left(t_{N}\right)= & \left(R_{i, s}^{l+1}\left(t_{N}\right)\right)^{-1}\left(\hat{\varphi}_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right) \tilde{y}_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right)\right. \\
& \left.+\left(R_{i, s}^{l+1}\left(t_{N}\right)-\hat{\varphi}_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right) \varphi_{i, \mathrm{f}}^{\top}\left(t_{N}, \theta_{i, s}^{l+1}\right)\right) \bar{\theta}_{i}\left(t_{N-1}\right)\right)
\end{aligned}
$$

and thus


\begin{align*}
\theta_{i, s+1}^{l+1}\left(t_{N}\right)= & \bar{\theta}_{i}\left(t_{N-1}\right)+\left(R_{i, s}^{l+1}\left(t_{N}\right)\right)^{-1} \hat{\varphi}_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right)  \tag{8.43}\\
& \times\left(\tilde{y}_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right)-\varphi_{i, \mathrm{f}}^{\top}\left(t_{N}, \theta_{i, s}^{l+1}\right) \bar{\theta}_{i}\left(t_{N-1}\right)\right)
\end{align*}


In order to avoid inverting $R_{i, s}^{l+1}\left(t_{N}\right)$ at each time step, introduce $P_{i, s}^{l+1}\left(t_{N}\right)= \left(R_{i, s}^{l+1}\left(t_{N}\right)\right)^{-1}$. The Sherman-Morrison formula [133, Chap. 0.7.4], a generalization of the matrix inversion lemma, states that $\left(A+u v^{\top}\right)^{-1}=A^{-1}- \frac{A^{-1} u v^{\top} A^{-1}}{1+v^{\top} A^{-1} u}$, provided the inverses exist, with $A$ invertible and square, and column vectors $u$ and $v$. Take $A=\lambda \bar{R}_{i}\left(t_{N-1}\right), u=\hat{\varphi}_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right)$, and $v=\varphi_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right)$. Then,

$$
\begin{aligned}
\left(R_{i, s}^{l+1}\left(t_{N}\right)\right)^{-1}= & \frac{1}{\lambda}\left(\bar{R}_{i}\left(t_{N-1}\right)\right)^{-1} \\
& -\frac{\frac{1}{\lambda}\left(\bar{R}_{i}\left(t_{N-1}\right)\right)^{-1} \hat{\varphi}_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right) \varphi_{i, \mathrm{f}}^{\top}\left(t_{N}, \theta_{i, s}^{l+1}\right) \frac{1}{\lambda}\left(\bar{R}_{i}\left(t_{N-1}\right)\right)^{-1}}{1+\varphi_{i, \mathrm{f}}^{\top}\left(t_{N}, \theta_{i, s}^{l+1}\right) \frac{1}{\lambda}\left(\bar{R}_{i}\left(t_{N-1}\right)\right)^{-1} \hat{\varphi}_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right)},
\end{aligned}
$$

and thus with $\bar{P}_{i}\left(t_{N-1}\right)=\left(\bar{R}_{i}\left(t_{N-1}\right)\right)^{-1}$,


\begin{align*}
P_{i, s}^{l+1}\left(t_{N}\right)= & \frac{1}{\lambda} \bar{P}_{i}\left(t_{N-1}\right) \\
& -\frac{1}{\lambda} \frac{\bar{P}_{i}\left(t_{N-1}\right) \hat{\varphi}_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right) \varphi_{i, \mathrm{f}}^{\top}\left(t_{N}, \theta_{i, s}^{l+1}\right) \bar{P}_{i}\left(t_{N-1}\right)}{\lambda+\varphi_{i, \mathrm{f}}^{\top}\left(t_{N}, \theta_{i, s}^{l+1}\right) \bar{P}_{i}\left(t_{N-1}\right) \hat{\varphi}_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right)} \tag{8.44}
\end{align*}


Define $K_{i, s+1}^{l+1}\left(t_{N}\right)=P_{i, s}^{l+1}\left(t_{N}\right) \hat{\varphi}_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right)$, or equivalently,


\begin{align*}
K_{i, s+1}^{l+1}\left(t_{N}\right)= & \bar{P}_{i}\left(t_{N-1}\right) \hat{\varphi}_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right) \\
& \times\left(\lambda+\varphi_{i, \mathrm{f}}^{\top}\left(t_{N}, \theta_{i, s}^{l+1}\right) \bar{P}_{i}\left(t_{N-1}\right) \hat{\varphi}_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right)\right)^{-1} \tag{8.45}
\end{align*}


This expression allows us to write (8.44) as

$$
P_{i, s}^{l+1}\left(t_{N}\right)=\frac{1}{\lambda}\left(\bar{P}_{i}\left(t_{N-1}\right)-K_{i, s+1}^{l+1}\left(t_{N}\right) \varphi_{i, \mathrm{f}}^{\top}\left(t_{N}, \theta_{i, s}^{l+1}\right) \bar{P}_{i}\left(t_{N-1}\right)\right)
$$

and (8.43) as

$$
\theta_{i, s+1}^{l+1}\left(t_{N}\right)=\bar{\theta}_{i}\left(t_{N-1}\right)+K_{i, s+1}^{l+1}\left(t_{N}\right)\left(\tilde{y}_{i, \mathrm{f}}\left(t_{N}, \theta_{i, s}^{l+1}\right)-\varphi_{i, \mathrm{f}}^{\top}\left(t_{N}, \theta_{i, s}^{l+1}\right) \bar{\theta}_{i}\left(t_{N-1}\right)\right)
$$

Hence, (8.45), (8.44), and (8.43), form a recursive equivalent of (8.38).

\section*{8.C Proof of theorem 8.14}
In this appendix, the proof of Theorem 8.14 is provided.

Proof. As the number of SRIVC iterations $s$ tends to infinity,


\begin{align*}
\theta_{i, \infty}^{l+1}= & \frac{1}{N}\left[\sum_{k=1}^{N} \hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, \infty}^{l+1}\right) \varphi_{i, \mathrm{f}}^{\top}\left(t_{k}, \theta_{i, \infty}^{l+1}\right)\right]^{-1}  \tag{8.46}\\
& \times \frac{1}{N}\left[\sum_{k=1}^{N} \hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, \infty}^{l+1}\right) \tilde{y}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, \infty}^{l+1}\right)\right]
\end{align*}


That is,


\begin{equation*}
\frac{1}{N} \sum_{k=1}^{N} \hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, \infty}^{l+1}\right)\left[\tilde{y}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, \infty}^{l+1}\right)-\varphi_{i, \mathrm{f}}^{\top}\left(t_{k}, \theta_{i, \infty}^{l+1}\right) \theta_{i, \infty}^{l+1}\right]=0 \tag{8.47}
\end{equation*}


where


\begin{align*}
& \tilde{y}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, \infty}^{l+1}\right)-\varphi_{i, \mathrm{f}}^{\top}\left(t_{k}, \theta_{i, \infty}^{l+1}\right) \theta_{i, \infty}^{l+1} \\
& \quad=\frac{1}{A_{i}\left(t_{k}, \theta_{i, \infty}^{l+1}\right)} \tilde{y}_{i}\left(t_{k}\right)+\frac{p a_{i, \infty, 1}^{l+1}+\ldots+p^{n_{i}} a_{i, \infty, n_{i}}^{l+1} \tilde{y}_{i}\left(t_{k}\right)}{A_{i}\left(t_{k}, \theta_{i, \infty}^{l+1}\right)} \\
& \quad-\frac{b_{i, \infty, 0}^{l+1}+p b_{i, \infty, 1}^{l+1}+\ldots+p^{m_{i}} b_{i, \infty, m_{i}}^{l+1} u\left(t_{k}\right)}{A_{i}\left(t_{k}, \theta_{i, \infty}^{l+1}\right)}  \tag{8.48}\\
& \quad=\tilde{y}_{i}\left(t_{k}\right)-\frac{B_{i}\left(t_{k}, \theta_{i, \infty}^{l+1}\right)}{A_{i}\left(t_{k}, \theta_{i, \infty}^{l+1}\right)} u\left(t_{k}\right)  \tag{8.49}\\
& \quad=y\left(t_{k}\right)-\sum_{j=1}^{i-1} G_{j}\left(p, t_{k}, \theta_{j}^{l+1}\right) u\left(t_{k}\right) \\
& \quad-\sum_{j=i+1}^{K} G_{j}\left(p, t_{k}, \theta_{j}^{l}\right) u\left(t_{k}\right)-\frac{B_{i}\left(t_{k}, \theta_{i, \infty}^{l+1}\right)}{A_{i}\left(t_{k}, \theta_{i, \infty}^{l+1}\right)} u\left(t_{k}\right) \tag{8.50}
\end{align*}


To simplify the notation, in the subsequent analysis, the $t_{k}$ dependency on the transfer functions is omitted. Denoting the parameter vector $\theta_{i, \infty}^{\infty}$ as the limit of $\theta_{i, \infty}^{l}$ when the number of descent iterations tends to infinity, then, as $l \rightarrow \infty$,


\begin{equation*}
\tilde{y}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, \infty}^{\infty}\right)-\varphi_{i, \mathrm{f}}^{\top}\left(t_{k}, \theta_{i, \infty}^{\infty}\right) \theta_{i, \infty}^{\infty}=y\left(t_{k}\right)-\sum_{j=1}^{K} G_{j}\left(p, \theta_{j, \infty}^{\infty}\right) u\left(t_{k}\right) \tag{8.51}
\end{equation*}


On the other hand, note that


\begin{equation*}
\lim _{N \rightarrow \infty} \frac{1}{N} \sum_{k=1}^{N} \hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, \infty}^{\infty}\right) v\left(t_{k}\right)=\overline{\mathbb{E}}\left\{\hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, \infty}^{\infty}\right) v\left(t_{k}\right)\right\}=0 \tag{8.52}
\end{equation*}


where the first equality is due to the ergodicity property of the input and noise signals (see Theorem 2B. 1 of [31] for more details), and the second equality holds since $v$ and $\hat{\varphi}_{i, \mathrm{f}}$ are uncorrelated due to Assumption 8.12. So, for $i=1, \ldots, K$, combining (8.47), (8.51), (8.13) and (8.52) leads to


\begin{equation*}
\lim _{N \rightarrow \infty} \frac{1}{N} \sum_{j=1}^{K} \sum_{k=1}^{N} \hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, \infty}^{\infty}\right)\left[G_{j}^{*}(p)-G_{j}\left(p, \theta_{j, \infty}^{\infty}\right)\right] u\left(t_{k}\right)=0 \tag{8.53}
\end{equation*}


Now,


\begin{align*}
G_{j}^{*}(p)-G_{j}\left(p, \theta_{j, \infty}^{\infty}\right) & =\frac{B_{j}^{*}(p)}{A_{j}^{*}(p)}-\frac{B_{j}\left(p, \theta_{j, \infty}^{\infty}\right)}{A_{j}\left(p, \theta_{j, \infty}^{\infty}\right)} \\
& =\frac{B_{j}^{*}(p) A_{j}\left(p, \theta_{j, \infty}^{\infty}\right)-A_{j}^{*}(p) B_{j}\left(p, \theta_{j, \infty}^{\infty}\right)}{A_{j}^{*}(p) A_{j}\left(p, \theta_{j, \infty}^{\infty}\right)} \\
& =\frac{\left[\begin{array}{ccc}
1 & p & \cdots \\
A_{j}^{n_{j}+m_{j}}
\end{array}\right]}{A_{j}^{*}(p) A_{j}\left(p, \theta_{j, \infty}^{\infty}\right)} h_{j} \tag{8.54}
\end{align*}


where $h_{j} \in \mathbb{R}^{n_{j}+m_{j}+1}$ is the vector that contains the coefficients of $B_{j}^{*}(p) A_{j}\left(p, \theta_{j, \infty}^{\infty}\right)-A_{j}^{*}(p) B_{j}\left(p, \theta_{j, \infty}^{\infty}\right)$ in increasing order of degree. With regards to the instrument vector, the following decomposition holds (see, e.g., Eq. (16) of [191])


\begin{equation*}
\hat{\varphi}_{i, \mathrm{f}}\left(t_{k}, \theta_{i, \infty}^{\infty}\right)=S\left(-B_{i}, A_{i}\right) \frac{1}{A_{i}^{2}\left(p, \theta_{i, \infty}^{\infty}\right)} U_{i}\left(t_{k}\right) \tag{8.55}
\end{equation*}


where $U_{i}\left(t_{k}\right)=\left[\begin{array}{llll}1 & p & \ldots & p^{n_{i}+m_{i}}\end{array}\right]^{\top} u\left(t_{k}\right)$, and where $S\left(-B_{i}, A_{i}\right)$ is a Sylvester matrix formed by the coefficients of the polynomials - $B_{i}\left(p, \theta_{i, \infty}^{\infty}\right)$ and $A_{i}\left(p, \theta_{i, \infty}^{\infty}\right)$, similar to Eq. (13) of [191]. Thus, (8.53) is equivalent to


\begin{align*}
& \sum_{j=1}^{K} \lim _{N \rightarrow \infty} \frac{1}{N} \sum_{k=1}^{N} S\left(-B_{i}, A_{i}\right) \frac{1}{A_{i}^{2}\left(t_{k}, \theta_{i, \infty}^{\infty}\right)} U_{i}\left(t_{k}\right)  \tag{8.56}\\
& \quad \times \frac{1}{A_{j}^{*}(p) A_{j}\left(p, \theta_{j, \infty}^{\infty}\right)} U_{j}^{\top}\left(t_{k}\right) h_{j}=0
\end{align*}


for $i=1, \ldots, K$. After applying Lemma 8.19 in Section 8.C. 1 and the ergodicity result in Theorem 2B. 1 of [31], which hold since the system and model denominators describe uniformly stable transfer functions and the input is assumed quasi-stationary, the following is obtained.


\begin{equation*}
S\left(-\bar{B}_{i}, \bar{A}_{i}\right) \sum_{j=1}^{K} \overline{\mathbb{E}}\left\{\frac{1}{A_{i}^{2}\left(t_{k}, \theta_{i, \infty}^{\infty}\right)} U_{i}\left(t_{k}\right) \frac{1}{A_{j}^{*}(p) A_{j}\left(p, \theta_{j, \infty}^{\infty}\right)} U_{j}^{\top}\left(t_{k}\right)\right\} \bar{h}_{j}=0 \tag{8.57}
\end{equation*}


for $i=1, \ldots, K$, where $\left(\bar{A}_{i}, \bar{B}_{i}\right)=\lim _{N \rightarrow \infty}\left(A_{i}\left(p, \theta_{i, \infty}^{\infty}\right), B_{i}\left(p, \theta_{i, \infty}^{\infty}\right)\right)$, and $\bar{h}_{j}= \lim _{N \rightarrow \infty} h_{j}$. The $K$ conditions in (8.57) are jointly expressed as $\mathcal{S} \Phi_{u} \eta=0$, where $\mathcal{S}$ is a block-diagonal matrix whose block-diagonal is comprised by $\left\{S\left(-\bar{B}_{i}, \bar{A}_{i}\right)\right\}_{k=1}^{K}$, while $\eta=\left[\bar{h}_{1}^{\top}, \ldots, \bar{h}_{K}^{\top}\right]^{\top}$, and $\Phi_{u}$ has block entries given by

$$
\Phi_{u}^{i, j}=\overline{\mathbb{E}}\left\{\frac{1}{A_{i}^{2}\left(t_{k}, \theta_{j, \infty}^{\infty}\right)} U_{i}\left(t_{k}\right) \frac{1}{A_{j}^{*}(p) A_{j}\left(t_{k}, \theta_{j, \infty}^{\infty}\right)} U_{j}\left(t_{k}\right)\right\} .
$$

Assumption 8.11 regarding the coprimeness of $\bar{A}_{i}$ and $\bar{B}_{i}$ implies that $\mathcal{S}$ is nonsingular, as shown in Lemma A3.1 of [234]. Moreover, Lemma 8.20 in Section 8.C. 1\\
applied over each entry of $\Phi_{u}$ indicates that the non-singularity of $\Phi_{u}$ can be studied directly from the cross covariance of the inputs filtered by the LTI systems obtained at the limit as $N \rightarrow \infty$. Thus, following the same reasoning as in Lemma 3 of [106], Assumptions 8.9 to 8.12 imply the the generic nonsingularity of $\Phi_{u}$ with respect to the converging system and model denominator parameters. Consequently, $\eta=0$, i.e., $\lim _{N \rightarrow \infty} h_{i}=0$ for all $i=1, \ldots, K$, as desired.

\section*{8.C. 1 Technical Lemmas}
Lemma 8.19. If the matrix $\mathcal{O}_{k}$ has a bounded norm for all $k$ with probability 1, then, with probability 1,


\begin{equation*}
\lim _{N \rightarrow \infty} \frac{1}{N} \sum_{k=1}^{N} S_{k} \mathcal{O}_{k} h_{k}=\bar{S} \lim _{N \rightarrow \infty} \frac{1}{N} \sum_{k=1}^{N} \mathcal{O}_{k} \bar{h}, \tag{8.58}
\end{equation*}


where $S_{k}$ and $h_{k}$ are matrices and vectors of appropriate dimensions, such that $\lim _{k \rightarrow \infty} S_{k}=\bar{S}$ and $\lim _{k \rightarrow \infty} h_{k}=\bar{h}$.\\
Proof. Note that the following inequalities hold:


\begin{equation*}
\frac{1}{N}\left\|\sum_{k=1}^{N}\left(S_{k}-\bar{S}\right) \mathcal{O}_{k} h_{k}\right\| \leq \frac{1}{N} \sum_{k=1}^{N}\left\|S_{k}-\bar{S}\right\|\left\|\mathcal{O}_{k} h_{k}\right\| \leq \frac{M_{1}}{N} \sum_{k=1}^{N}\left\|S_{k}-\bar{S}\right\|, \tag{8.59}
\end{equation*}


where $M_{1}=\sup _{k}\left\|\mathcal{O}_{k} h_{k}\right\|<\infty$ with probability 1 , since $\mathcal{O}_{k}$ and $h_{k}$ are bounded. Since $\left\|S_{k}-\bar{S}\right\| \rightarrow 0$ as $k \rightarrow \infty$, then the Cesàro mean in (8.59) also goes to zero due to Theorem 4.2.3 of [51]. By the same reasoning, it follows that

$$
\frac{1}{N}\left\|\sum_{k=1}^{N} \bar{S} \mathcal{O}_{k}\left(h_{k}-\bar{h}\right)\right\| \leq \frac{1}{N} \sum_{k=1}^{N}\left\|\bar{S} \mathcal{O}_{k}\right\|\left\|h_{k}-\bar{h}\right\| \leq \frac{M_{2}}{N} \sum_{k=1}^{N}\left\|h_{k}-\bar{h}\right\| \xrightarrow{N \rightarrow \infty} 0
$$

Thus, the result in (8.58) follows from decomposing $S_{k} \mathcal{O}_{k} h_{k}$ as

$$
S_{k} \mathcal{O}_{k} h_{k}=\left(S_{k}-\bar{S}\right) \mathcal{O}_{k} h_{k}+\bar{S} \mathcal{O}_{k}\left(h_{k}-\bar{h}\right)+\bar{S} \mathcal{O}_{k} \bar{h}
$$

summing over $k$, dividing over $N$, and computing the limit as $N \rightarrow \infty$.\\
Lemma 8.20. Let


\begin{equation*}
w(t)=\sum_{k=0}^{\infty} \alpha_{t}(k) e(t-k), \quad v(t)=\sum_{k=0}^{\infty} \beta_{t}(k) e(t-k) \tag{8.60}
\end{equation*}


where the families of filters $\alpha_{t}(k), \beta_{t}(k), t=1,2, \ldots$ are uniformly stable, and $\{e(t)\}$ is a quasi-stationary signal. In addition, let $\bar{w}(t)$ and $\bar{v}(t)$ be defined as in (8.60) but for the LTI filters $\bar{\alpha}(k)$ and $\bar{\beta}(k)$ respectively, where $\alpha_{t}(k) \rightarrow \bar{\alpha}(k)$ and $\beta_{t}(k) \rightarrow \bar{\beta}(k)$ for every $k$ as $t \rightarrow \infty$. Then,


\begin{equation*}
|\overline{\mathbb{E}}\{w(t) v(t)\}-\overline{\mathbb{E}}\{\bar{w}(t) \bar{v}(t)\}| \rightarrow 0 \quad \text { as } N \rightarrow \infty . \tag{8.61}
\end{equation*}


Proof. The following inequalities hold:


\begin{align*}
& |\overline{\mathbb{E}}\{w(t) v(t)\}-\overline{\mathbb{E}}\{\bar{w}(t) \bar{v}(t)\}| \\
& \quad=\left|\frac{1}{N} \sum_{t=1}^{N} \sum_{k=0}^{\infty} \sum_{\ell=0}^{\infty} \mathbb{E}\{e(t-k) e(t-\ell)\}\left(\alpha_{t}(k) \beta_{t}(\ell)-\bar{\alpha}(k) \bar{\beta}(\ell)\right)\right|  \tag{8.62}\\
& \left.\quad \leq \frac{C}{N} \sum_{t=1}^{N} \sum_{k=0}^{\infty} \sum_{\ell=0}^{\infty} \right\rvert\,\left(\alpha_{t}(k)\left(\beta_{t}(\ell)-\bar{\beta}(\ell)\right)+\bar{\beta}(\ell)\left(\alpha_{t}(k)-\bar{\alpha}(k)\right) \mid\right.  \tag{8.63}\\
& \quad \leq \frac{C}{N} \sup _{t \in \mathbb{N}} \sum_{k=0}^{\infty}\left|\alpha_{t}(k)\right| \sum_{t=1}^{N} \sum_{\ell=0}^{\infty}\left|\beta_{t}(\ell)-\bar{\beta}(\ell)\right| \\
& \quad+\frac{C}{N} \sum_{\ell=0}^{\infty}|\bar{\beta}(\ell)| \sum_{t=1}^{N} \sum_{k=0}^{\infty}\left|\alpha_{t}(k)-\bar{\alpha}(k)\right| \tag{8.64}
\end{align*}


where (8.62) holds by definition, (8.63) uses the quasi-stationary property $|\mathbb{E}\{e(t-k) e(t-\ell)\}| \leq C$ for any indices $t, k$ and $\ell$, and (8.64) is due to the triangle inequality. Since $\alpha_{t}(k)$ and $\beta_{t}(\ell)$ describe uniformly stable families of filters, the series $\sup _{t \in \mathbb{N}} \sum_{k=0}^{\infty}\left|\alpha_{t}(k)\right|$ and $\sum_{k=0}^{\infty}|\bar{\beta}(k)|$ converge. Thus, to prove (8.61) it is required to show that


\begin{equation*}
\frac{1}{N} \sum_{t=1}^{N} \sum_{k=0}^{\infty}\left|\alpha_{t}(k)-\bar{\alpha}(k)\right| \rightarrow 0 \quad \text { as } N \rightarrow \infty \tag{8.65}
\end{equation*}


and similarly with $\beta_{t}(k)$. Due to the pointwise convergence, $\lim _{t \rightarrow \infty} \mid \alpha_{t}(k)- \bar{\alpha}(k) \mid=0$, and due to the uniform stability of $\alpha_{t}(k)$,

$$
\sum_{k=0}^{\infty}\left|\alpha_{t}(k)-\bar{\alpha}(k)\right| \leq 2 \sum_{k=0}^{\infty} \sup _{t \in \mathbb{N}}\left|\alpha_{t}(k)\right|<\infty
$$

Thus, by the Weierstrass M-test [20, Theorem A28],

$$
\lim _{t \rightarrow \infty} \sum_{k=0}^{\infty}\left|\alpha_{t}(k)-\bar{\alpha}(k)\right|=\sum_{k=0}^{\infty} \lim _{t \rightarrow \infty}\left|\alpha_{t}(k)-\bar{\alpha}(k)\right|=0
$$

Hence, (8.65) can be interpreted as a Cesàro mean with sequence in $t$ given by $\sum_{k=0}^{\infty}\left|\alpha_{t}(k)-\bar{\alpha}(k)\right|$, which goes to zero as $t$ tends to infinity. Thus, by Theorem 4.2.3 of [51], (8.65) holds. The same derivation holds for $\beta_{t}(\ell)$, which implies that the right hand side of (8.64) tends to zero as $N$ tends to infinity, thus proving (8.61).

\section*{Chapter 9}
\section*{Identification of Additive Continuous-time Systems in Open and Closed Loop ${ }^{1}$ }
\begin{abstract}
When identifying electrical, mechanical, or biological systems, parametric continuous-time identification methods can lead to interpretable and parsimonious models when the model structure aligns with the physical properties of the system. Traditional linear system identification may not consider the most parsimonious model when relying solely on unfactored transfer functions, which typically result from standard direct approaches. This chapter presents a novel identification method that delivers additive models for both open and closed-loop setups. The estimators that are derived are shown to be generically consistent, and can admit the identification of marginally stable additive systems. Numerical simulations show the efficacy of the proposed approach, and its performance in identifying a modal representation of a flexible beam is verified using experimental data.
\end{abstract}

\footnotetext{${ }^{1}$ The authors Rodrigo González and Koen Classens have had an equal contribution to this chapter, which is inherently related to Chapter 8. The results in this chapter constitute Contribution VIII in Section 1.6. The chapter is based on: [112] R. A. González, K. Classens, C. R. Rojas, J. S. Welsh, and T. Oomen, "Identification of Additive Continuous-time Systems in Open and Closed Loop," submitted for journal publication.
}\subsection*{9.1 Introduction}
The goal in system identification is to obtain mathematical descriptions of systems using input and output data. These models can play a critical role for prediction, analysis, and design of control laws for electrical, mechanical, biological, or environmental systems, among others [161, 236]. A distinction can be made between discrete-time and continuous-time system identification methods using sampled data. Identification algorithms for discrete-time systems provide difference equation models that give meaningful information only at the sampling instants. In contrast, continuous-time system identification methods use sampled data to estimate differential equation models that represent the system at any point in time.

Linear continuous-time system identification methods [208] are widely used and successful in a range of practical applications [86, 89, 277], offering several advantages over the standard discrete-time algorithms. One advantage of continuous-time methods is their ability to directly incorporate a priori knowledge of the relative degree of the physical systems they model. This feature is particularly useful in estimating, e.g., mechanical systems, as they often exhibit no impulse response discontinuities due to the double integration relationship between force and position [91], leading to models with relative degree equal to 2 . In contrast, discrete-time methods must also account for sampling zeros that are not present in the continuous-time system representation [9], requiring an additional optimization step if the resulting discrete-time model is later converted to continuous time [111]. Recent research has proposed tailored variants of instrumental variables to address this particular challenge [107].

When addressing the identification of physical systems, increasingly stringent performance requirements necessitate the use of parsimonious models, i.e., models with the fewest number of parameters that can adequately describe the phenomenon under investigation. From a statistical standpoint, it is well known that parsimonious model structures that contain the true system within the model set result in reduced variance compared to over-parameterized model sets [239]. Most linear continuous-time identification methods parameterize the model structure as an unfactored transfer function, that is, a quotient of polynomials in the Laplace transform variable $s$ with numerator and denominator polynomial degrees indicating the number of zeros and poles of the model, respectively. Unfactored transfer function parametrizations may not always offer the most parsimonious model structure, but due to their simplicity they are widely adopted in the Simplified Refined Instrumental Variable method for Continuoustime systems (SRIVC, [281]), the Poisson moment functional approach [218], the Least-Squares State-Variable Filter method (LSSVF, [279]), and other linear filter and integral methods [87]. The closed-loop variants of these estimators are also limited to estimating unfactored transfer functions [96, 97, 280]. Although more general model structures can be incorporated using the prediction error\\[0pt]
method and maximum likelihood paradigm [8], typically one unfactored transfer function is estimated in the linear and time-invariant continuous-time identification problem [160].

While unfactored transfer functions are standard choices for the model structure in linear system identification, many practical applications related to, e.g., flexible motion systems [184] and vibration analysis [91], often involve systems that are more easily interpreted as a sum of transfer functions with distinct denominators, typically corresponding to different resonant modes. In addition, modal parameter estimation for structures subject to vibrations is essential for design and model validation, to guarrantee safety and quality control [211, 268]. Additive model parametrizations have advantages such as leading to physically more insightful models for fault diagnosis [39, 40] and improving the numerical conditioning of parameter estimation for high-order or highly-resonant systems [100]. These model parametrizations, which have been considered in statistics [121] and econometrics [119], offer increased model flexibility and the ability to decentralize the analysis of each additive component for optimization and control purposes. Despite some contributions in nonlinear discrete-time finiteimpulse response and generalized Hammerstein model estimation [10,11], the application of additive model structures in the realm of system identification has been limited. Recently, [108] introduced a block coordinate algorithm to identify continuous-time systems under an additive model structure. However, this approach is confined to open-loop setups and can be computationally demanding due to the need for estimating each submodel with a tailored version of the SRIVC method at each iteration of the descent algorithm [105].

Another difficulty when estimating linear systems with an additive model decomposition is that some systems are known to have integral action, i.e., their transfer function descriptions contain integrators. Applications featuring such systems can be found in platooning [203], hydraulics, mechanical systems [91, 204], among others. As an illustrative example, mechatronic positioning systems can often be represented as a combination of rigid-body and flexible modes, with the rigid-body modes modeled as double integrators [184]. Many system identification methods are only suitable for asymptotically stable systems, since the predictors become ill-conditioned when the models are unstable. Although this problem has been tackled for Box-Jenkins model structures in discrete-time and continuous-time settings [78, 109], these works do not consider additive model parametrizations.

In this chapter, a comprehensive identification method is proposed for modeling additive linear continuous-time systems in both open and closed-loop settings. The contributions can be summarized as follows:

C1 Optimality conditions are derived that the proposed estimators for additive continuous-time system identification must satisfy in both open and closed-loop scenarios in a unified manner. To achieve this, a connection between the first-order optimality condition of the open-loop estimator in an\\[0pt]
output error setup and the instrumental variable approach in the closedloop setting is established. In the closed-loop case, explicit expressions for an instrument vector are derived that ensures a consistent estimator while also yielding a minimum asymptotic covariance matrix in a positive definite sense. The closed-loop results extend the ones found in [97] to a general class of additive continuous-time models.

C2 Open and closed-loop estimators are developed based on the derived optimality conditions, extending the SRIVC and CLSRIVC estimators for additive continuous-time models. Also, the identification of marginally stable additive systems is considered, and a thorough consistency analysis is provided for the proposed estimators, demonstrating their generic consistency under mild conditions.

C3 The proposed method is evaluated through extensive Monte Carlo simulations, and its efficacy is shown using data from an experimental flexible beam setup.

The remainder of this chapter is structured as follows. Section 9.2 introduces the problem setup for both open and closed-loop settings. Section 9.3 describes the optimality conditions for the additive model structures of both settings. Section 9.4 contains the proposed unified iterative procedure for additive continuous-time system identification with its asymptotic analysis. Extensive simulations can be found in Section 9.5, while Section 9.6 contains experimental setup results. Concluding remarks are presented in Section 9.7. A technical lemma used for proving the main theoretical results can be found in the Appendix.

\subsection*{9.2 System and model setup}
Consider the single-input single-output, linear and time-invariant (LTI), continuous-time system in additive form


\begin{equation*}
x(t)=\sum_{i=1}^{K} G_{i}^{*}(p) u(t) \tag{9.1}
\end{equation*}


where $p$ is the Heaviside or derivative operator (i.e., $p u(t)=\frac{\mathrm{d}}{\mathrm{d} t} u(t)$ ) and $u(t)$ is the input signal. Each subsystem $G_{i}^{*}(p)$ has $n_{i}$ poles and $m_{i}$ zeros, and can be expressed as $B_{i}^{*}(p) / A_{i}^{*}(p)$, where the numerator and denominator polynomials are assumed coprime, i.e., they do not share roots. These polynomials are given by


\begin{align*}
& A_{i}^{*}(p)=a_{i, n_{i}}^{*} p^{n_{i}}+a_{i, n_{i}-1}^{*} p^{n_{i}-1}+\cdots+a_{i, 1}^{*} p+1  \tag{9.2}\\
& B_{i}^{*}(p)=b_{i, m_{i}}^{*} p^{m_{i}}+b_{i, m+i-1}^{*} p^{m_{i}-1}+\cdots+b_{i, 1}^{*} p+b_{i, 0}^{*}
\end{align*}


\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-232(1)}
\captionsetup{labelformat=empty}
\caption{Figure 9.1: Block diagrams for the open-loop setting studied in this chapter.}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-232}
\captionsetup{labelformat=empty}
\caption{Figure 9.2: Block diagram for the closed-loop setting studied in this chapter.}
\end{center}
\end{figure}

with $a_{n_{i}}^{*} \neq 0$ and $m_{i} \leq n_{i}$. Without loss of generality, assume that the $A_{i}^{*}(p)$ polynomials do not share roots and that they are anti-monic, i.e., their constant coefficient is fixed to 1 . In addition, to obtain a unique characterization of $\left\{G_{i}^{*}(p)\right\}_{i=1}^{K}$, assume that at most one subsystem $G_{i}^{*}(p)$ has the same number of poles and zeros. The polynomials $A_{i}^{*}(p)$ and $B_{i}^{*}(p)$ are jointly described by the parameter vector

\[
\theta_{i}^{*}=\left[\begin{array}{llllllll}
a_{i, 1}^{*} & a_{i, 2}^{*} & \ldots & a_{i, n_{i}}^{*} & b_{i, 0}^{*} & b_{i, 1}^{*} & \ldots & b_{i, m_{i}}^{*} \tag{9.3}
\end{array}\right]^{\top} .
\]

A noisy measurement of the output is retrieved at every time instant $t=t_{k}$, $k=1, \ldots, N$, where $\left\{t_{k}\right\}_{k=1}^{N}$ are evenly spaced in time ${ }^{2}$. That is,


\begin{equation*}
y\left(t_{k}\right)=x\left(t_{k}\right)+v\left(t_{k}\right), \tag{9.4}
\end{equation*}


where $v\left(t_{k}\right)$ is assumed to be a zero-mean stationary random process of finite variance $\sigma^{2}$. Two frameworks are considered in this work: open and closed-loop identification. Block diagrams that describe these setups are presented in Figure 9.1 and 9.2. The output noise $v\left(t_{k}\right)$ is assumed to be uncorrelated with the input samples $u\left(t_{k}\right)$ for the open-loop case, and uncorrelated with the reference signal $r\left(t_{k}\right)$ for the closed-loop case. For both settings, for simplicity, it is assumed that the input signal $u(t)$ is known to be constant between samples, i.e., it has a zero-order hold ( ZOH ) behavior. Other known intersample behaviors, such as first-order hold or band-limited, can also be addressed within the proposed

\footnotetext{${ }^{2}$ Extensions of the proposed identification methods for irregularly sampled signals are feasible by following similar steps to the ones found in [135].
}
framework at the expense of a more involved notation and ad-hoc prefiltering techniques [110]. The closed-loop setting considers an input signal generated from a known discrete-time LTI controller $C_{\mathrm{d}}(q)$, with $q$ being the forward-shift operator. Formally, the input $u\left(t_{k}\right)$ is described in terms of the reference and output noise as follows:

\begin{equation*}
u\left(t_{k}\right)=\tilde{r}\left(t_{k}\right)-\tilde{v}\left(t_{k}\right) \tag{9.5}
\end{equation*}

where

\begin{equation*}
\tilde{r}\left(t_{k}\right):=S_{u o}^{*}(q) r\left(t_{k}\right), \quad \tilde{v}\left(t_{k}\right):=S_{u o}^{*}(q) v\left(t_{k}\right), \tag{9.6}
\end{equation*}

with $S_{u o}^{*}(q)$ being the sensitivity function $S_{u o}^{*}(q)=C_{\mathrm{d}}(q) /\left[1+G_{\mathrm{d}}^{*}(q) C_{\mathrm{d}}(q)\right]^{-1}$, and $G_{\mathrm{d}}^{*}(q)$ being the ZOH-equivalent discrete-time system of $G^{*}(p)= \sum_{i=1}^{K} G_{i}^{*}(p)$.

The main aim is to develop a data-driven method to obtain estimates of the parameter vector

$$
\beta^{*}:=\left[\begin{array}{llll}
\theta_{1}^{* \top} & \theta_{2}^{* \top} & \ldots & \theta_{K}^{* \top}
\end{array}\right]^{\top} .
$$

To this end, the input and output data $\left\{u\left(t_{k}\right), y\left(t_{k}\right)\right\}_{k=1}^{N}$ are considered as given for the open-loop scenario, while $\left\{r\left(t_{k}\right)\right\}_{k=1}^{N}$ is also known in the closed-loop setting.

Remark 9.1. In Section 9.4 the identification is considered of additive systems that are marginally stable. Since the denominator polynomials related to these systems cannot be represented in an anti-monic form as in (9.2), the required system and model assumptions for this case are provided separately.

Remark 9.2. Throughout this chapter it is assumed that the input (for the openloop case) and reference signals (for the closed-loop case) are quasi-stationary. Consequently, the standard definition of expectation for quasi-stationary signals is considered [161, p. 34]

$$
\overline{\mathbb{E}}\left\{g\left(t_{k}\right)\right\}:=\lim _{N \rightarrow \infty} \frac{1}{N} \sum_{k=1}^{N} \mathbb{E}\left\{g\left(t_{k}\right)\right\}
$$

\subsection*{9.3 Stationary points for additive continuous-time system identification}
In this section, the optimality conditions are presented that will be exploited for designing the proposed additive system identification method in Section 9.4.

\subsection*{9.3.1 Open-loop setting}
In the open-loop case, the aim is to find an estimator that minimizes the outputerror cost


\begin{equation*}
\hat{\beta}=\underset{\beta \in \Omega}{\arg \min } \frac{1}{2 N} \sum_{k=1}^{N}\left(y\left(t_{k}\right)-\sum_{i=1}^{K} G_{i}\left(p, \theta_{i}\right) u\left(t_{k}\right)\right)^{2}, \tag{9.7}
\end{equation*}


where each $G_{i}\left(p, \theta_{i}\right)$ is a transfer function parameterized as in (9.2). Note that the notation $G(p) u\left(t_{k}\right)$ means that the sampled input is interpolated with a zeroorder hold, filtered through the continuous-time system $G(p)$, at later evaluated at $t=t_{k}$. The optimizer of the cost function in (9.7) must satisfy the first-order optimality condition


\begin{equation*}
\frac{1}{N} \sum_{k=1}^{N} \hat{\varphi}\left(t_{k}, \hat{\beta}\right)\left(y\left(t_{k}\right)-\sum_{i=1}^{K} G_{i}\left(p, \hat{\theta}_{i}\right) u\left(t_{k}\right)\right)=0 \tag{9.8}
\end{equation*}


where $\hat{\beta}$ is given by (9.7), and the gradient of the residual is written as

\[
\hat{\varphi}\left(t_{k}, \hat{\beta}\right)=\left[\begin{array}{lll}
\hat{\varphi}_{1}^{\top}\left(t_{k}, \hat{\beta}\right) & \ldots & \hat{\varphi}_{K}^{\top}\left(t_{k}, \hat{\beta}\right) \tag{9.9}
\end{array}\right]^{\top}
\]

with each vector $\hat{\varphi}_{i}\left(t_{k}, \hat{\beta}\right)$ being given by


\begin{gather*}
\hat{\varphi}_{i}\left(t_{k}, \hat{\beta}\right)=\left[\frac{-p \hat{B}_{i}(p)}{\left[\hat{A}_{i}(p)\right]^{2}} u\left(t_{k}\right) \ldots \frac{-p^{n_{i}} \hat{B}_{i}(p)}{\left[\hat{A}_{i}(p)\right]^{2}} u\left(t_{k}\right)\right.  \tag{9.10}\\
\left.\frac{1}{\hat{A}_{i}(p)} u\left(t_{k}\right) \ldots \frac{p^{m_{i}}}{\hat{A}_{i}(p)} u\left(t_{k}\right)\right]^{\top}
\end{gather*}


\subsection*{9.3.2 Closed-loop setting}
In contrast to the open-loop analysis, an output-error loss function of the form in (9.7) applied to closed-loop system identification can result in an asymptotically biased estimator. This phenomenon is due to the possible correlation of the output noise with the input $u(t)$ through the closed-loop interconnection [106, 256]. The bias in closed loop can be mitigated or reduced completely as $N$ tends to infinity if an instrumental variable approach is considered, as in [98] for the unfactored transfer function case. Thus, instead of (9.7) for the closed-loop setting, the aim is to compute the instrumental variable solution


\begin{equation*}
\hat{\beta} \in \operatorname{sol}_{\beta \in \Omega}\left\{\frac{1}{N} \sum_{k=1}^{N} \zeta\left(t_{k}\right)\left(y\left(t_{k}\right)-\sum_{i=1}^{K} G_{i}\left(p, \theta_{i}\right) u\left(t_{k}\right)\right)=0\right\} \tag{9.11}
\end{equation*}


where $\zeta\left(t_{k}\right) \in \mathbb{R}^{K+\sum_{i=1}^{K}\left(n_{i}+m_{i}\right)}$ is an instrument vector that is assumed to be uncorrelated with the output noise $v\left(t_{k}\right)$. Such assumption is not restrictive in\\
practice, since the instrument vector is typically formed by filtered versions of the reference signal, which are designed by the user.

A crucial aspect of the estimator in (9.11) is how the instrument vector is chosen for yielding consistent estimates of minimum covariance. Conditions on the instrument vector under which the estimator is generically consistent are provided in Lemma 9.3.\\
Lemma 9.3. Assume that the instrument vector $\zeta\left(t_{k}\right)$ is uncorrelated with the output noise $v\left(t_{k}\right)$, and that $r\left(t_{k}\right)$ and $v\left(t_{l}\right)$ are quasi-stationary and mutually independent for all integers $k$ and $l$. Then, the estimator (9.11) is generically consistent if $\overline{\mathbb{E}}\left\{\zeta\left(t_{k}\right) \tilde{\varphi}^{r \top}\left(t_{k}\right)\right\}$ is generically nonsingular ${ }^{3}$ with respect to the system and model denominator parameters, where $\tilde{\varphi}^{r}\left(t_{k}\right)$ is formed by stacking the vectors $\left\{\tilde{\varphi}_{i}^{r}\left(t_{k}\right)\right\}_{i=1}^{K}$, where


\begin{gather*}
\tilde{\varphi}_{i}^{r}\left(t_{k}\right)=\left[\frac{-p B_{i}^{*}(p)}{A_{i}(p) A_{i}^{*}(p)} \tilde{r}\left(t_{k}\right) \cdots \frac{-p^{n_{i}} B_{i}^{*}(p)}{A_{i}(p) A_{i}^{*}(p)} \tilde{r}\left(t_{k}\right)\right. \\
\left.\frac{1}{A_{i}(p)} \tilde{r}\left(t_{k}\right) \cdots \frac{p^{m_{i}}}{A_{i}(p)} \tilde{r}\left(t_{k}\right)\right]^{\top} \tag{9.12}
\end{gather*}


Proof. As the sample size tends to infinity, the sum in $k$ in (9.11) converges to an expected value due to the quasi-stationarity assumptions on the reference and noise signals [235], which leads to $\bar{\theta}_{i}=\lim _{N \rightarrow \infty} \hat{\theta}_{i}$ satisfying


\begin{align*}
& \overline{\mathbb{E}}\left\{\zeta\left(t_{k}\right)\left(y\left(t_{k}\right)-\sum_{i=1}^{K} G_{i}\left(p, \bar{\theta}_{i}\right) u\left(t_{k}\right)\right)\right\}=0 \\
\Longleftrightarrow & \sum_{i=1}^{K} \overline{\mathbb{E}}\left\{\zeta\left(t_{k}\right)\left[G_{i}^{*}(p)-G_{i}\left(p, \bar{\theta}_{i}\right)\right] u\left(t_{k}\right)\right\}=0 \\
\Longleftrightarrow & \sum_{i=1}^{K} \overline{\mathbb{E}}\left\{\zeta\left(t_{k}\right)\left[G_{i}^{*}(p)-G_{i}\left(p, \bar{\theta}_{i}\right)\right] \tilde{r}\left(t_{k}\right)\right\}=0, \tag{9.13}
\end{align*}


where it is used that the instrument vector is uncorrelated with the output noise $v\left(t_{k}\right)$ in both steps (recall that $\tilde{r}\left(t_{k}\right)$ is given by (9.6)). Furthermore, consider

$$
\left[G_{i}^{*}(p)-G_{i}\left(p, \bar{\theta}_{i}\right)\right] \tilde{r}\left(t_{k}\right)=\frac{1}{A_{i}^{*}(p) \bar{A}_{i}(p)}\left[\begin{array}{llll}
1 & p & \ldots & p^{n_{i}+m_{i}}
\end{array}\right] \eta_{i}
$$

where $\eta_{i} \in \mathbb{R}^{n_{i}+m_{i}+1}$ is the vector that contains the coefficients of $A_{i}^{*}(p) \bar{B}_{i}(p)- \bar{A}_{i}(p) B_{i}^{*}(p)$ in descending order of degree. By following the derivation of (15) of [191], is obtained that

$$
\frac{1}{A_{i}^{*}(p) \bar{A}_{i}(p)} S\left(-B_{i}^{*}, A_{i}^{*}\right)\left[\begin{array}{llll}
1 & p & \ldots & p^{n_{i}+m_{i}}
\end{array}\right]^{\top} \tilde{r}\left(t_{k}\right)=\tilde{\varphi}_{i}^{r}\left(t_{k}\right),
$$

\footnotetext{${ }^{3}$ In this context, a statement $s$, which depends on the elements $\rho$ of some open set $\Omega \subseteq \mathbb{R}^{n}$, is generically true with respect to $\Omega[234]$ if the set $M=\{\rho \in \Omega \mid s(\rho)$ is not true $\}$ has Lebesgue measure zero in $\Omega$.
}
where $\tilde{\varphi}_{i}^{r}\left(t_{k}\right)$ is given in (9.12), and $S\left(-B_{i}^{*}, A_{i}^{*}\right)$ is a Sylvester matrix that is nonsingular for $i=1, \ldots, K$ due to the coprimeness of the polynomials $A_{i}^{*}(p)$ and $B_{i}^{*}(p)$ [234, Lemma A3.1]. If the vectors $\tilde{\varphi}^{r}\left(t_{k}\right)$ and $\eta$ are formed by stacking the vectors $\tilde{\varphi}_{i}^{r}\left(t_{k}\right)$ and $\eta_{i}$ respectively, then (9.13) reduces to the condition
$$
\overline{\mathbb{E}}\left\{\zeta\left(t_{k}\right) \tilde{\varphi}^{r \top}\left(t_{k}\right)\right\}\left[\begin{array}{ccc}
S\left(-B_{1}^{*}, A_{1}^{*}\right) & & 0 \\
0 & \ddots & \\
0 & & S\left(-B_{K}^{*}, A_{K}^{*}\right)
\end{array}\right]^{-\top} \eta=0
$$
which implies that $\eta$ is generically the zero vector (i.e., $G_{i}\left(p, \bar{\theta}_{i}\right)=G_{i}^{*}(p)$ for all $i=1, \ldots, K$ ) if the matrix $\overline{\mathbb{E}}\left\{\zeta\left(t_{k}\right) \tilde{\varphi}^{r \top}\left(t_{k}\right)\right\}$ is generically nonsingular.\\[0pt]
Remark 9.4. The nonsingularity condition in Lemma 9.3 suggests that the instrument vector should be correlated with filtered versions of the derivatives of the noiseless model output and input. Such interpretation is ubiquitous in instrumental variable estimation, and has been thoroughly studied for the unfactored transfer function case [24, 88, 174, 192]. Lemma 9.3 presents the first extension of these analyses to the additive system identification framework.

In accordance to Lemma 9.3, solely instrument vectors that meet the nonsingularity requirement are considered, which defines a family of generically consistent estimators. Among these, the aim is to characterize the one that obtains the least asymptotic covariance. In cases where the transfer function is unfactored, it is established in both discrete-time [237, 240] and continuous-time [99] formulations that the instrument vector achieving the lower bound of the asymptotic covariance matrix for the parameter estimator is given by the noise-free component of the regressor vector. However, for the additive model description, the computation of $\hat{\beta}$ cannot be carried out in a straightforward analytical manner, and the model output cannot be expressed using just one regressor vector. Despite these difficulties, Theorem 9.5 confirms that the asymptotic distribution of $\hat{\beta}$ in (9.11) follows a normal distribution, and the asymptotic covariance can be explicitly computed.\\
Theorem 9.5. Consider the estimator (9.11). Assume that $r\left(t_{k}\right)$ and $v\left(t_{l}\right)$ are quasi-stationary and mutually independent for all integers $k$ and $l$, and that the instrument vector $\zeta\left(t_{k}\right)$ is constructed such that $\hat{\beta}$ is a consistent estimator of $\beta^{*}$. Then, $\hat{\beta}$ is asymptotically Gaussian distributed, i.e.,


\begin{equation*}
\sqrt{N}\left(\hat{\beta}-\beta^{*}\right) \xrightarrow{\text { dist. }} \mathcal{N}\left(0, P_{\mathrm{IV}}\right), \tag{9.14}
\end{equation*}


where the asymptotic covariance matrix $P_{\mathrm{IV}}$ is given by

$$
P_{\mathrm{IV}}=\sigma^{2} \overline{\mathbb{E}}\left\{\zeta\left(t_{k}\right) \psi^{\top}\left(t_{k}\right)\right\}^{-1} \overline{\mathbb{E}}\left\{\zeta\left(t_{k}\right) \zeta^{\top}\left(t_{k}\right)\right\} \overline{\mathbb{E}}\left\{\psi\left(t_{k}\right) \zeta^{\top}\left(t_{k}\right)\right\}^{-1}
$$

where

\[
\psi\left(t_{k}\right)=\left.\left[\begin{array}{lll}
\frac{\partial G_{1}\left(p, \theta_{1}\right)}{\partial \theta_{1}^{\top}} & \ldots & \frac{\partial G_{K}\left(p, \theta_{K}\right)}{\partial \theta_{K}^{\top}} \tag{9.15}
\end{array}\right]^{\top}\right|_{\beta=\beta^{*}} \tilde{r}\left(t_{k}\right)
\]

Proof. A first-order Taylor expansion of the left hand side of (9.11) gives

$$
\frac{1}{\sqrt{N}} \sum_{k=1}^{N} \zeta\left(t_{k}\right)\left(y\left(t_{k}\right)-\sum_{i=1}^{K} G_{i}^{*}(p) u\left(t_{k}\right)\right)-\frac{1}{\sqrt{N}} \sum_{k=1}^{N} \zeta\left(t_{k}\right) \psi^{\top}\left(t_{k}\right)\left(\hat{\beta}-\beta^{*}\right) \# 0,
$$

where the \# symbol means that the left-hand side and the right-hand side differ by a quantity converging to zero in probability, and where $\psi\left(t_{k}\right)$ is defined in (9.15). Hence,


\begin{gather*}
\sqrt{N}\left(\hat{\beta}-\beta^{*}\right) \#\left[\frac{1}{N} \sum_{k=1}^{N} \zeta\left(t_{k}\right) \psi^{\top}\left(t_{k}\right)\right]^{-1} \frac{1}{\sqrt{N}} \sum_{k=1}^{N} \zeta\left(t_{k}\right) v\left(t_{k}\right) \\
\# \overline{\mathbb{E}}\left\{\zeta\left(t_{k}\right) \psi^{\top}\left(t_{k}\right)\right\}^{-1} \frac{1}{\sqrt{N}} \sum_{k=1}^{N} \zeta\left(t_{k}\right) v\left(t_{k}\right) \tag{9.16}
\end{gather*}


It follows from Lemma A4.1 of [234] that


\begin{equation*}
\frac{1}{\sqrt{N}} \sum_{k=1}^{N} \zeta\left(t_{k}\right) v\left(t_{k}\right) \xrightarrow{\text { dist. }} \mathcal{N}(0, P) \tag{9.17}
\end{equation*}


where, following (22) of [192] and exploiting the fact that $\zeta\left(t_{k}\right)$ and $v\left(t_{k}\right)$ are uncorrelated, it is obtained that

$$
\begin{aligned}
P & =\lim _{N \rightarrow \infty} \frac{1}{N} \sum_{k=1}^{N} \sum_{s=1}^{N} \mathbb{E}\left\{\left[\zeta\left(t_{k}\right) v\left(t_{k}\right)\right]\left[\zeta\left(t_{s}\right) v\left(t_{s}\right)\right]^{\top}\right\} \\
& =\sigma^{2} \overline{\mathbb{E}}\left\{\zeta\left(t_{k}\right) \zeta^{\top}\left(t_{k}\right)\right\}
\end{aligned}
$$

The desired result then follows from Lemma A4.2 of [234] applied to (9.16).\\
Corollary 9.6. The asymptotic covariance matrix of $\hat{\beta}$ in (9.11) is minimized in a positive definite sense when $\zeta\left(t_{k}\right)=\psi\left(t_{k}\right)$.

Proof. Since any covariance matrix is positive semi-definite,

\[
\overline{\mathbb{E}}\left\{\left[\begin{array}{l}
\zeta\left(t_{k}\right)  \tag{9.18}\\
\psi\left(t_{k}\right)
\end{array}\right]\left[\begin{array}{l}
\zeta\left(t_{k}\right) \\
\psi\left(t_{k}\right)
\end{array}\right]^{\top}\right\} \succeq 0
\]

The nonsingularity of $\overline{\mathbb{E}}\left\{\zeta\left(t_{k}\right) \psi^{\top}\left(t_{k}\right)\right\}$ implies that the matrix $\overline{\mathbb{E}}\left\{\zeta\left(t_{k}\right) \zeta^{\top}\left(t_{k}\right)\right\}$ is positive definite. Thus, (9.18) is equivalent to the Schur complement

$$
\overline{\mathbb{E}}\left\{\psi\left(t_{k}\right) \psi^{\top}\left(t_{k}\right)\right\}-\overline{\mathbb{E}}\left\{\psi\left(t_{k}\right) \zeta^{\top}\left(t_{k}\right)\right\} \overline{\mathbb{E}}\left\{\zeta\left(t_{k}\right) \zeta^{\top}\left(t_{k}\right)\right\}^{-1} \overline{\mathbb{E}}\left\{\zeta\left(t_{k}\right) \psi^{\top}\left(t_{k}\right)\right\} \succeq 0 .
$$

Provided that the matrices above are nonsingular, this is equivalent to

$$
\overline{\mathbb{E}}\left\{\psi\left(t_{k}\right) \psi^{\top}\left(t_{k}\right)\right\}^{-1} \preceq \overline{\mathbb{E}}\left\{\zeta\left(t_{k}\right) \psi^{\top}\left(t_{k}\right)\right\}^{-1} \overline{\mathbb{E}}\left\{\zeta\left(t_{k}\right) \zeta^{\top}\left(t_{k}\right)\right\} \overline{\mathbb{E}}\left\{\psi\left(t_{k}\right) \zeta^{\top}\left(t_{k}\right)\right\}^{-1}
$$

Equality holds when $\zeta\left(t_{k}\right)=M \psi\left(t_{k}\right)$, with $M$ being a nonsingular constant matrix. Since $M$ can be canceled out from (9.11), setting $M$ equal to the identity matrix without loss of generality leads to the desired result.

Corollary 9.6 indicates that the optimal instrument vector is given by a filtered version of the reference signal that depends on the true system parameters, as seen in (9.15). Since the true parameters are not known, the solution for the unfactored transfer function case adopted in, e.g., the closed-loop SRIVC estimator (CLSRIVC, [97]), is to let $\zeta\left(t_{k}\right)$ depend on $\beta$ as well. In the additive identification case, the instrument vector takes the form (9.9), but where, for $i=1, \ldots, K$,


\begin{align*}
& \zeta_{i}\left(t_{k}, \beta\right)=\left[\begin{array}{lll}
\frac{-p B_{i}(p)}{\left[A_{i}(p)\right]^{2}} & \ldots & \frac{-p^{n_{i}} B_{i}(p)}{\left[A_{i}(p)\right]^{2}}
\end{array}\right. \\
& \left.\frac{1}{A_{i}(p)} \cdots \frac{p^{m_{i}}}{A_{i}(p)}\right]^{\top} S_{u o}(q) r\left(t_{k}\right), \tag{9.19}
\end{align*}


where $S_{\text {uo }}(q)=C_{\mathrm{d}}(q) /\left[1+G_{\mathrm{d}}(q) C_{\mathrm{d}}(q)\right]^{-1}$.\\
In summary, the open-loop and closed-loop identification problems can be solved using the optimality condition (9.8) and the (refined) instrumental variable expression (9.11), respectively. In the closed-loop scenario, the instrument $\zeta\left(t_{k}\right)$ depends on the model, as indicated in (9.19).

Remark 9.7. To unify the notations pertaining to the open and closed loop cases, from now on the instrument vector composed by stacking the vectors (9.19) for $i=1, \ldots, K$ is denoted as $\hat{\varphi}\left(t_{k}, \beta\right)$. The difference between this vector and the one described by (9.10) will be clear from the context.

\subsection*{9.4 Additive system identification: An instrumental variable solution}
This section presents the proposed instrumental variable-based method for identifying additive continuous-time systems in open or closed loop settings, and its consistency analysis.

\subsection*{9.4.1 Derivation of the instrumental variable solution}
When computing $\hat{\beta}$ that satisfies (9.8) or (9.11), the difference between the measured and predicted outputs cannot be written as a unique pseudolinear regression when $K>1$ due to the additive structure of the model. Then, to estimate additive models, the residual is written in $K$ different ways:


\begin{equation*}
y\left(t_{k}\right)-\sum_{i=1}^{K} G_{i}\left(p, \hat{\theta}_{i}\right) u\left(t_{k}\right)=y_{f, i}\left(t_{k}, \hat{\theta}_{i}\right)-\varphi_{i}^{\top}\left(t_{k}, \hat{\theta}_{i}\right) \hat{\theta}_{i} \tag{9.20}
\end{equation*}


where $i=1, \ldots, K$. Here, $y_{f, i}\left(t_{k}, \hat{\beta}\right)$ represents a filtered residual output of the form


\begin{equation*}
y_{f, i}\left(t_{k}, \hat{\beta}\right)=\frac{1}{\hat{A}_{i}(p)} \tilde{y}_{i}\left(t_{k}\right) \tag{9.21}
\end{equation*}


where the residual output pertaining to the $i$ th additive submodel is given by


\begin{equation*}
\tilde{y}_{i}\left(t_{k}\right)=y\left(t_{k}\right)-\sum_{\substack{l=1, \ldots, K, l \neq i}} G_{l}\left(p, \hat{\theta}_{l}\right) u\left(t_{k}\right) \tag{9.22}
\end{equation*}


and the regressor vector associated with the $i$ th additive submodel, denoted by $\varphi_{i}\left(t_{k}, \hat{\theta}_{i}\right)$, is expressed as

\[
\left.\begin{array}{rl}
\varphi_{i}\left(t_{k}, \hat{\theta}_{i}\right)= & {\left[\begin{array}{llll}
\frac{-p}{\hat{A}_{i}(p)} & \tilde{y}_{i}\left(t_{k}\right) & \ldots & \frac{-p^{n_{i}}}{\hat{A}_{i}(p)} \\
& \tilde{y}_{i}\left(t_{k}\right)
\end{array}\right.} \\
& \frac{1}{\hat{A}_{i}(p)} u\left(t_{k}\right) \tag{9.23}
\end{array} \cdots \frac{p^{m_{i}}}{\hat{A}_{i}(p)} u\left(t_{k}\right)\right]^{\top} .
\]

Thus, by defining the following signals and matrix


\begin{align*}
\varphi\left(t_{k}, \hat{\beta}\right) & =\left[\varphi_{1}^{\top}\left(t_{k}, \hat{\theta}_{1}\right), \ldots, \varphi_{K}^{\top}\left(t_{k}, \hat{\theta}_{K}\right)\right]^{\top}  \tag{9.24}\\
\Upsilon\left(t_{k}, \hat{\beta}\right) & =\left[y_{f, 1}\left(t_{k}, \hat{\beta}\right), \ldots, y_{f, K}\left(t_{k}, \hat{\beta}\right)\right]^{\top}  \tag{9.25}\\
\hat{\mathcal{B}} & =\left[\begin{array}{lll}
\hat{\theta}_{1} & & 0 \\
& \ddots & \\
0 & & \hat{\theta}_{K}
\end{array}\right] \tag{9.26}
\end{align*}


the conditions in (9.8) and (9.11) can be written in $K$ different ways using (9.20), which leads to the following equivalent equation that the optimal estimate must satisfy:


\begin{equation*}
\frac{1}{N} \sum_{k=1}^{N} \hat{\varphi}\left(t_{k}, \hat{\beta}\right)\left(\Upsilon^{\top}\left(t_{k}, \hat{\beta}\right)-\varphi^{\top}\left(t_{k}, \hat{\beta}\right) \hat{\mathcal{B}}\right)=0 \tag{9.27}
\end{equation*}


Thus, by fixing $\hat{\beta}=\hat{\beta}^{j}$ in $\hat{\varphi}\left(t_{k}, \hat{\beta}\right), \Upsilon^{\top}\left(t_{k}, \hat{\beta}\right)$ and $\varphi^{\top}\left(t_{k}, \hat{\beta}\right)$, the $j$ th iteration of the proposed estimator is obtained for both open and closed loop settings by solving for $\hat{\mathcal{B}}$ as follows:


\begin{equation*}
\mathcal{B}^{j+1}=\left[\frac{1}{N} \sum_{k=1}^{N} \hat{\varphi}\left(t_{k}, \beta^{j}\right) \varphi^{\top}\left(t_{k}, \beta^{j}\right)\right]^{-1}\left[\frac{1}{N} \sum_{k=1}^{N} \hat{\varphi}\left(t_{k}, \beta^{j}\right) \Upsilon^{\top}\left(t_{k}, \beta^{j}\right)\right] \tag{9.28}
\end{equation*}


where the next iteration $\beta^{j+1}$ is extracted from the block diagonal coefficients of $\mathcal{B}^{j+1}$ as in (9.26), and the instrument vector is described by (9.10) or (9.19) for the open or closed loop setups, respectively. The iterations in (9.28) are computed until convergence for (9.8) or (9.11) to be satisfied.

Remark 9.8. The iterations in (9.28) can be viewed as an extension of the open-loop and closed-loop refined instrumental variable estimators to the additive model case. When $K=1$ is considered, i.e., the unfactored transfer function case, the iterations in (9.28) reduce to


\begin{equation*}
\theta_{1}^{j+1}=\left[\frac{1}{N} \sum_{k=1}^{N} \hat{\varphi}_{1}\left(t_{k}, \theta_{1}^{j}\right) \varphi_{1}^{\top}\left(t_{k}, \theta_{1}^{j}\right)\right]^{-1}\left[\frac{1}{N} \sum_{k=1}^{N} \hat{\varphi}_{1}\left(t_{k}, \theta_{1}^{j}\right) y_{f, 1}\left(t_{k}, \theta_{1}^{j}\right)\right], \tag{9.29}
\end{equation*}


where the instrument vector $\hat{\varphi}_{1}\left(t_{k}, \theta_{1}^{j}\right)$ is given by (9.10) for the open-loop case, and (9.19) for the closed-loop setting. These iterations correspond to the standard SRIVC and CLSRIVC methods, respectively [97, 281]. The SRIVC and CLSRIVC estimators have recently been proven to be generically consistent under mild conditions [106, 191], and the asymptotic efficiency of the SRIVC estimator has been proven in [192].

Remark 9.9. The proposed approach can be extended to the estimation of marginally stable systems as follows. Consider the system (9.1), but where the first submodel has $\ell$ poles at the origin. In other words, let $G_{1}^{*}(p)= B_{1}^{*}(p) /\left[p^{\ell} A_{1}^{*}(p)\right]$, where $A_{1}^{*}(p)$ and $B_{1}^{*}(p)$ are coprime polynomials, and have the same form as in (9.2). For either open or closed-loop variants of the proposed approach, the computation of the gradient of each submodel is required with respect to their respective parameter vector. For $G_{1}(p)$, this computation leads to the following instrument vector:


\begin{align*}
\hat{\varphi}_{1}\left(t_{k}, \beta\right)= & {\left[\begin{array}{lll}
\frac{-p B_{1}(p)}{p^{\ell}\left[A_{1}(p)\right]^{2}} & \cdots & \frac{-p^{n_{1}} B_{1}(p)}{p^{\ell}\left[A_{1}(p)\right]^{2}}
\end{array}\right.} \\
& \frac{1}{p^{\ell} A_{1}(p)} \cdots  \tag{9.30}\\
\cdots & \left.\frac{p^{m_{1}}}{p^{\ell} A_{1}(p)}\right]^{\top} z\left(t_{k}\right)
\end{align*}


where $z\left(t_{k}\right)=u\left(t_{k}\right)$ for the open-loop algorithm, and $z\left(t_{k}\right)=S_{u o}(q) r\left(t_{k}\right)$ for the closed-loop variant. On the other hand, the model residual retains the same form as in (9.20), with the filtered residual output given by (9.21), but with the regressor vector now expressed as


\begin{align*}
\varphi_{1}\left(t_{k}, \hat{\theta}_{1}\right)= & {\left[\begin{array}{lll}
\frac{-p}{\hat{A}_{1}(p)} \tilde{y}_{1}\left(t_{k}\right) & \ldots & \frac{-p^{n_{1}}}{\hat{A}_{1}(p)} \tilde{y}_{1}\left(t_{k}\right)
\end{array}\right.} \\
& \left.\frac{1}{p^{\ell} \hat{A}_{1}(p)} u\left(t_{k}\right) \ldots \frac{p^{m_{1}}}{p^{\ell} \hat{A}_{1}(p)} u\left(t_{k}\right)\right]^{\top} \tag{9.31}
\end{align*}


By computing the iterations in (9.28) with the first block of the instrument and regressor vectors given by (9.30) and (9.31) respectively, a direct extension of the proposed method is obtained for identifying marginally stable systems in additive form. This solution extends the one proposed in Solution 6.2 of [109], which\\[0pt]
is only applicable to unfactored transfer functions in closed loop. The choice to include an integral action term in the model can be motivated by physical intuition, or by model order selection methods [161].

Note that a block diagonal structure for $\mathcal{B}^{j+1}$ is not achieved in general at each iteration (9.28), but it is guaranteed if the iterations converge to a stationary point that satisfies the pseudolinear regression equations in (9.27). This result is presented in Lemma 9.10.

Lemma 9.10. Consider the iterative procedure in (9.28) for finite $N$ with $\varphi\left(t_{k}, \beta^{j}\right)$ and $\Upsilon\left(t_{k}, \beta^{j}\right)$ given by (9.24) and (9.25) respectively, while $\hat{\varphi}\left(t_{k}, \beta^{j}\right)$ is formed by (9.10) or (9.19) in the open and closed-loop setups, respectively. Denote $\bar{\beta}$ as the converging point of the iterative procedure, with $\overline{\mathcal{B}}=\lim _{j \rightarrow \infty} \mathcal{B}^{j}$, and assume that the matrix


\begin{equation*}
\left[\frac{1}{N} \sum_{k=1}^{N} \hat{\varphi}\left(t_{k}, \bar{\beta}\right) \varphi^{\top}\left(t_{k}, \bar{\beta}\right)\right] \tag{9.32}
\end{equation*}


is nonsingular. Then, the converging point of the iterative procedure satisfies (9.8) or (9.11) if and only if the matrix $\overline{\mathcal{B}}$ is block-diagonal.

Proof. The straight implication part of the result is direct from the derivation leading to (9.27) and (9.28). For the converse, note that the converging point of the iterative procedure in (9.28) must satisfy

$$
\overline{\mathcal{B}}=\left[\sum_{k=1}^{N} \hat{\varphi}\left(t_{k}, \bar{\beta}\right) \varphi^{\top}\left(t_{k}, \bar{\beta}\right)\right]^{-1}\left[\sum_{k=1}^{N} \hat{\varphi}\left(t_{k}, \bar{\beta}\right) \Upsilon^{\top}\left(t_{k}, \bar{\beta}\right)\right] .
$$

Equivalently,

$$
\sum_{k=1}^{N} \hat{\varphi}\left(t_{k}, \bar{\beta}\right)\left[\Upsilon^{\top}\left(t_{k}, \bar{\beta}\right)-\varphi^{\top}\left(t_{k}, \bar{\beta}\right) \overline{\mathcal{B}}\right]=0 .
$$

Define $\tilde{\mathcal{B}}=\operatorname{diag}\left(\bar{\theta}_{1}, \ldots, \bar{\theta}_{K}\right)$. This matrix satisfies

$$
\Upsilon^{\top}\left(t_{k}, \bar{\beta}\right)-\varphi^{\top}\left(t_{k}, \bar{\beta}\right) \tilde{\mathcal{B}}=\left(y\left(t_{k}\right)-\sum_{i=1}^{K} G_{i}\left(p, \bar{\theta}_{i}\right) u\left(t_{k}\right)\right)\left[\begin{array}{lll}
1 & \ldots & 1
\end{array}\right]
$$

which shows that the matrix formed by the off-diagonal elements of $\overline{\mathcal{B}}$, denoted as $E=\overline{\mathcal{B}}-\tilde{\mathcal{B}}$, must satisfy

$$
\begin{aligned}
\sum_{k=1}^{N} \hat{\varphi}\left(t_{k}, \bar{\beta}\right) \varphi^{\top}\left(t_{k}, \bar{\beta}\right) E & =\sum_{k=1}^{N} \hat{\varphi}\left(t_{k}, \bar{\beta}\right)\left(y\left(t_{k}\right)-\sum_{i=1}^{K} G_{i}\left(p, \bar{\theta}_{i}\right) u\left(t_{k}\right)\right)[1 \quad[1 \quad \ldots \quad 1] \\
& =0,
\end{aligned}
$$

where the last equality is due to the assumption that the limiting point $\bar{\beta}$ satisfies the optimality condition (9.8) or (9.11) in the open or closed-loop setting, respectively. Since the modified normal matrix (9.32) is assumed to be nonsingular, $E=0$, which concludes the proof.

As seen in Lemma 9.10, the nonsingularity of the matrix (9.28) is essential for the method to be well posed. This aspect, as well as the convergence of the iterations, will be studied extensively in the next subsection.

\subsection*{9.4.2 Consistency Analysis}
Before presenting the main theorem on the generic consistency of the proposed method for open and closed loop, several signals of interest are introduced. By substituting (9.4) into (9.23), the regressor vector $\varphi\left(t_{k}, \beta\right)$ is expressed as the block vector formed by

$$
\varphi_{i}\left(t_{k}, \beta\right)=\tilde{\varphi}_{i}^{u}\left(t_{k}, \beta\right)+\Delta_{i}^{u}\left(t_{k}, \beta\right)-v_{i}^{u}\left(t_{k}, \beta\right)
$$

where $\tilde{\varphi}_{i}^{u}\left(t_{k}, \beta\right)$ has the same form as $\tilde{\varphi}_{i}^{r}\left(t_{k}, \beta\right)$ in (9.12) but with $\tilde{r}\left(t_{k}\right)$ replaced by $u\left(t_{k}\right)$. That is,


\begin{align*}
\tilde{\varphi}_{i}^{u}\left(t_{k}, \beta\right)= & {\left[\frac{-p B_{i}^{*}(p)}{A_{i}(p) A_{i}^{*}(p)} u\left(t_{k}\right) \cdots \frac{-p^{n_{i}} B_{i}^{*}(p)}{A_{i}(p) A_{i}^{*}(p)} u\left(t_{k}\right)\right.} \\
& \left.\frac{1}{A_{i}(p)} u\left(t_{k}\right) \cdots \frac{p^{m_{i}}}{A_{i}(p)} u\left(t_{k}\right)\right]^{\top} \tag{9.33}
\end{align*}


The vector $\Delta^{u}\left(t_{k}, \beta\right)$ contains the interpolation errors that arise from constructing the filtered disturbance-free derivatives of the output, as well as the residual model bias. The entries of $\Delta_{i}^{u}\left(t_{k}, \beta\right)$, denoted $\Delta_{i, j}^{u}\left(t_{k}, \beta\right)$, are zero for $j>n_{i}$ and


\begin{align*}
\Delta_{i, j}^{u}\left(t_{k}, \beta\right)= & \frac{p^{j} B_{i}^{*}(p)}{A_{i}(p) A_{i}^{*}(p)} u\left(t_{k}\right)-\frac{p^{j}}{A_{i}(p)}\left\{\frac{B_{i}^{*}(p)}{A_{i}^{*}(p)} u(t)\right\}_{t=t_{k}} \\
& -\sum_{\substack{l=1, \ldots, K, l \neq i}} \frac{p^{j}}{A_{i}(p)}\left\{\left(G_{l}^{*}(p)-G_{l}(p)\right) u(t)\right\}_{t=t_{k}} \tag{9.34}
\end{align*}


for $j=1, \ldots, n_{i}$. The direct contribution of the noise to the regressor is given by

$$
v_{i}^{u}\left(t_{k}, \beta\right)=\left[\begin{array}{lllll}
\frac{p}{A_{i}(p)} v\left(t_{k}\right) & \ldots & \frac{p^{n_{i}}}{A_{i}(p)} v\left(t_{k}\right) & 0 & \ldots
\end{array}\right]^{\top} .
$$

In the closed-loop case, the input also contains filtered output noise that must be rewritten for the analysis. By exploiting (9.5), the regressor vector in the closed-loop setting can also be expressed as

$$
\varphi_{i}\left(t_{k}, \beta\right)=\tilde{\varphi}_{i}^{r}\left(t_{k}, \beta\right)+\Delta_{i}^{r}\left(t_{k}, \beta\right)-v_{i}^{r}\left(t_{k}, \beta\right)
$$

where $\tilde{\varphi}_{i}^{r}\left(t_{k}, \beta\right)$ is given by (9.12), and $\Delta_{i}^{r}\left(t_{k}, \beta\right)$ has the same form as (9.34) but with $\tilde{r}\left(t_{k}\right)$ instead of $u\left(t_{k}\right)$, and $v_{i}^{r}\left(t_{k}, \beta\right)$ solely contains filtered versions of the output noise $v\left(t_{k}\right)$.

For the main result of this section, the following assumptions are required, some of which are divided into open or closed-loop assumptions depending on the considered setting.

Assumption 9.11 (Persistency of excitation). Open loop: The input $u\left(t_{k}\right)$ is persistently exciting of order no less than $2 n$. Closed loop: The input $r\left(t_{k}\right)$ is persistently exciting of order no less than $2 n+n_{c}$, where $n_{c}$ is the order of the discrete-time LTI controller $C_{\mathrm{d}}(q)$.

Assumption 9.12 (Stationarity and independence). Open loop: The input $u\left(t_{k}\right)$ and disturbance $v\left(t_{s}\right)$ are quasi-stationary and mutually independent for all $k$ and $s$. Closed loop: The reference $r\left(t_{k}\right)$ and disturbance $v\left(t_{s}\right)$ are quasistationary and mutually independent for all $k$ and $s$.

Assumption 9.13 (Stability and coprimeness). All the zeros of the $j$ th iteration of the model denominator estimate $A_{i, j}(p)$ have strictly negative real parts, with $A_{i, j}(p)$ and $B_{i, j}(p)$ being coprime. Closed loop: Additionally, the discrete-time equivalent model estimates at each iteration are asymptotically stabilized by the controller $C_{\mathrm{d}}(q)$.

Assumption 9.14 (Sampling rate). The sampling frequency is more than twice that of the largest imaginary part of the zeros of $\prod_{j=1}^{K} A_{j}(p) A_{j}^{*}(p)$.

Assumptions 9.11 to 9.14 are the same ones that have been considered in recent consistency analyses of open-loop and closed-loop refined instrumental variable estimators [106, 191]. Assumptions 9.11 and 9.12 can be readily satisfied if the input is white noise or a periodic signal of sufficient number of frequency lines. The stability condition in Assumption 9.13 is not restrictive, since the poles of the model at each iteration are typically reflected to the left half-plane if they are unstable. Cancelling the unstable poles using all-pass filters also circumvents this problem [109]. Finally, Assumption 9.14 avoids aliasing issues associated with the discrete-time implementation of the prefiltering steps, and is satisfied in practice if the sampling period is chosen small compared to the rise time of the system.

Theorem 9.15 shows that the proposed method in generically consistent for both open and closed-loop settings.

Theorem 9.15. Consider the proposed estimator (9.28) for the open and closedloop settings in Figure 9.1 and 9.2, and suppose Assumptions 9.11 to 9.14 hold. Then, the following statements are true:

\begin{enumerate}
  \item The modified normal matrix $\overline{\mathbb{E}}\left\{\hat{\varphi}\left(t_{k}, \beta\right) \varphi^{\top}\left(t_{k}, \beta\right)\right\}$ is generically nonsingular with respect to the system and model denominator polynomials, provided that the following condition holds for each respective setting:\\
(a) Open loop:
\end{enumerate}


\begin{equation*}
\left\|\overline{\mathbb{E}}\left\{\hat{\varphi}\left(t_{k}, \beta\right) \Delta^{u \top}\left(t_{k}, \beta\right)\right\}\right\|_{2}<\sigma_{\min }\left(\overline{\mathbb{E}}\left\{\hat{\varphi}\left(t_{k}, \beta\right) \tilde{\varphi}^{u \top}\left(t_{k}, \beta\right)\right\}\right), \tag{9.35}
\end{equation*}


where $\sigma_{\min }(\cdot)$ represents the minimum singular value of a matrix, and $\hat{\varphi}\left(t_{k}, \beta\right), \tilde{\varphi}^{u}\left(t_{k}, \beta\right)$ and $\Delta^{u}\left(t_{k}, \beta\right)$ are formed by stacking the vectors described by (9.10), (9.33) and (9.34), respectively.\\
(b) Closed loop:


\begin{equation*}
\left\|\overline{\mathbb{E}}\left\{\hat{\varphi}\left(t_{k}, \beta\right) \Delta^{r \top}\left(t_{k}, \beta\right)\right\}\right\|_{2}<\sigma_{\min }\left(\overline{\mathbb{E}}\left\{\hat{\varphi}\left(t_{k}, \beta\right) \tilde{\varphi}^{r \top}\left(t_{k}, \beta\right)\right\}\right), \tag{9.36}
\end{equation*}


where $\hat{\varphi}\left(t_{k}, \beta\right), \tilde{\varphi}^{r}\left(t_{k}, \beta\right)$ are formed by stacking the vectors described by (9.19) and (9.12), respectively, and $\Delta^{r}\left(t_{k}, \beta\right)$ has the same form as (9.34) but with $\tilde{r}\left(t_{k}\right)$ instead of $u\left(t_{k}\right)$.\\
2. Assume that (9.35) for open loop or (9.36) for closed loop is satisfied, and the iterations of the estimator converge for all $N$ sufficiently large to say, $\overline{\mathcal{B}}$, with $\overline{\mathcal{B}}$ being a block-diagonal matrix formed by $\bar{\beta}_{i}$. Then, the true parameter vector $\beta^{*}$ is the unique converging point of $\bar{\beta}$ as the sample size tends to infinity.

Remark 9.16. The expressions in (36) and (37) give sufficient but not necessary conditions for the generic nonsingularity of $\overline{\mathbb{E}}\left\{\hat{\varphi}\left(t_{k}, \beta\right) \varphi^{\top}\left(t_{k}, \beta\right)\right\}$. They are satisfied in practice when the respective interpolation error $\Delta^{u}\left(t_{k}, \beta\right)$ or $\Delta^{r}\left(t_{k}, \beta\right)$ arising from constructing the filtered output derivatives is not significant, which typically occurs when the sampling period is chosen appropriately.

Proof of Statement 1, Part (a): Since the input is uncorrelated with the output noise, the expectation of interest is given by

$$
\overline{\mathbb{E}}\left\{\hat{\varphi}\left(t_{k}, \beta\right) \varphi^{\top}\left(t_{k}, \beta\right)\right\}=\overline{\mathbb{E}}\left\{\hat{\varphi}\left(t_{k}, \beta\right)\left[\tilde{\varphi}^{u}\left(t_{k}, \beta\right)+\Delta^{u}\left(t_{k}, \beta\right)\right]^{\top}\right\} .
$$

Provided the condition (9.35) holds, due to Theorem 5.1 of [54], the perturbation matrix $\overline{\mathbb{E}}\left\{\hat{\varphi}\left(t_{k}, \beta\right) \Delta^{u \top}\left(t_{k}, \beta\right)\right\}$ is small enough (in 2-norm) to not affect the nonsingularity of the matrix $\overline{\mathbb{E}}\left\{\hat{\varphi}\left(t_{k}, \beta\right) \varphi^{\top}\left(t_{k}, \beta\right)\right\}$. Thus, the generic nonsingularity of the matrix $\overline{\mathbb{E}}\left\{\hat{\varphi}\left(t_{k}, \beta\right) \tilde{\varphi}^{u \top}\left(t_{k}, \beta\right)\right\}$ is studied instead.

By following standard operations (see, e.g., [106]), the noise-free, interpolation-error-free regressor vector and the instrument vector can be rewritten using the identities


\begin{align*}
\tilde{\varphi}_{i}^{u}\left(t_{k}, \beta\right) & =S\left(-B_{i}^{*}, A_{i}^{*}\right) \frac{1}{A_{i}(p) A_{i}^{*}(p)} u_{i}\left(t_{k}\right)  \tag{9.37}\\
\hat{\varphi}_{i}\left(t_{k}, \beta\right) & =S\left(-B_{i}, A_{i}\right) \frac{1}{A_{i}^{2}(p)} u_{i}\left(t_{k}\right) \tag{9.38}
\end{align*}


where $S\left(-B_{i}^{*}, A_{i}^{*}\right)$ and $S\left(-B_{i}, A_{i}\right)$ are Sylvester matrices that are nonsingular due to the coprimeness of the polynomials of the true system $G_{i}^{*}(p)$ and the coprimeness assumption on the model $G_{i}(p)$ [234, Lemma A3.1]. The vector $u_{i}\left(t_{k}\right)$ is formed by input derivatives, i.e.,

\[
u_{i}\left(t_{k}\right)=\left[\begin{array}{llll}
p^{n_{i}+m_{i}} & , & p^{n_{i}+m_{i}-1}, & \ldots, \tag{9.39}
\end{array}\right]^{\top} u\left(t_{k}\right)
\]

Hence,


\begin{equation*}
\overline{\mathbb{E}}\left\{\hat{\varphi}\left(t_{k}, \beta\right) \tilde{\varphi}^{u \top}\left(t_{k}, \beta\right)\right\}=\mathcal{S} \Phi_{u} \mathcal{S}_{*}^{\top} \tag{9.40}
\end{equation*}


where

\[
\Phi_{u}=\overline{\mathbb{E}}\left\{\begin{array}{c}
\left.\left[\begin{array}{c}
\frac{1}{A_{1}^{2}(p)} \\
\frac{1}{A_{2}^{2}(p)} \\
\vdots \\
\frac{1}{A_{K}^{2}(p)} \\
u_{K}\left(t_{k}\right) \\
u_{K}\left(t_{k}\right)
\end{array}\right]\left[\begin{array}{c}
\frac{1}{A_{1}^{*}(p) A_{1}(p)} u_{1}\left(t_{k}\right) \\
\frac{1}{A_{2}^{*}(p) A_{2}(p)} u_{2}\left(t_{k}\right) \\
\vdots \\
\frac{1}{A_{K}^{*}(p) A_{K}(p)} u_{K}\left(t_{k}\right)
\end{array}\right]^{\top}\right\}, \tag{9.41}
\end{array}\right.
\]

and where $\mathcal{S}$ and $\mathcal{S}_{*}$ are block diagonal matrices formed by $S\left(-B_{i}, A_{i}\right)$ and $S\left(-B_{i}^{*}, A_{i}^{*}\right)$, respectively. Since these block Sylvester matrices are nonsingular, in order for (9.40) to be generically nonsingular, it must be shown that $\Phi$ is generically nonsingular. Provided the Assumptions 9.11, 9.12 and 9.14 hold, the generic nonsingularity of such matrix follows from taking $S(q)=1$ and $z_{i}\left(t_{k}\right)=u_{i}\left(t_{k}\right)$ in Lemma 9.17 in the Appendix, which concludes the proof.

Proof of Statement 1, Part (b): The proof follows the same lines as the proof of Part (a) of Statement 1 with regards to the perturbation analysis, with $u\left(t_{k}\right)$ replaced by $\tilde{r}\left(t_{k}\right)$ in the expressions. In this case, the noise-free regressor and instrument vectors can be rewritten by considering


\begin{align*}
\tilde{\varphi}_{i}^{u}\left(t_{k}, \beta\right) & =S\left(-B_{i}^{*}, A_{i}^{*}\right) S_{u o}^{*}(q) \frac{1}{A_{i}(p) A_{i}^{*}(p)} r_{i}\left(t_{k}\right)  \tag{9.42}\\
\hat{\varphi}_{i}\left(t_{k}, \beta\right) & =S\left(-B_{i}, A_{i}\right) S_{u o}(q) \frac{1}{A_{i}^{2}(p)} r_{i}\left(t_{k}\right) \tag{9.43}
\end{align*}


where $r_{i}\left(t_{k}\right), i=1, \ldots, K$, have the same structure as (9.39). Hence, it is obtained that

$$
\overline{\mathbb{E}}\left\{\hat{\varphi}\left(t_{k}, \beta\right) \tilde{\varphi}^{r \top}\left(t_{k}, \beta\right)\right\}=\mathcal{S} \Phi_{r} \mathcal{S}_{*}^{\top}
$$

where $\Phi_{r}$ has the same form as (9.46) in the Appendix, but with $S(q)$ and $z_{i}\left(t_{k}\right)$ replaced by $S_{u o}(q)$ and $r_{i}\left(t_{k}\right)$, respectively. The generic nonsingularity of this matrix follows from Lemma 9.17.

Proof of Statement 2: As $N \rightarrow \infty$, (9.28) implies that


\begin{equation*}
\overline{\mathbb{E}}\left\{\hat{\varphi}\left(t_{k}, \bar{\beta}\right) \varphi^{\top}\left(t_{k}, \bar{\beta}\right)\right\}^{-1} \overline{\mathbb{E}}\left\{\hat{\varphi}\left(t_{k}, \bar{\beta}\right)\left(\Upsilon^{\top}\left(t_{k}, \bar{\beta}\right)-\varphi^{\top}\left(t_{k}, \bar{\beta}\right) \overline{\mathcal{B}}\right)\right\}=0 . \tag{9.44}
\end{equation*}


Since the matrix inverse above is generically nonsingular by Statement 1, only the second expectation requires to be analyzed. By following the derivation of\\
the proposed estimator in (9.28), it is found that

$$
\Upsilon^{\top}\left(t_{k}, \bar{\beta}\right)-\varphi^{\top}\left(t_{k}, \bar{\beta}\right) \overline{\mathcal{B}}=\left(y\left(t_{k}\right)-\sum_{i=1}^{K} G_{i}\left(p, \bar{\theta}_{i}\right) u\left(t_{k}\right)\right)[1, \ldots, 1]
$$

which, when inserted in (9.44), leads to the condition

$$
\overline{\mathbb{E}}\left\{\hat{\varphi}\left(t_{k}, \bar{\beta}\right)\left(\sum_{i=1}^{K}\left[G_{i}\left(p, \theta_{i}^{*}\right)-G_{i}\left(p, \bar{\theta}_{i}\right)\right] u\left(t_{k}\right)\right)\right\}=0 .
$$

The rest of the proof follows from the derivations made in the proof in Lemma 9.3 and is therefore omitted.

\subsection*{9.5 Simulations}
In this section, Monte Carlo experiments are performed to test the asymptotic properties of the proposed estimator, and to compare them with other direct continuous-time identification methods.

\subsection*{9.5.1 Open-loop experiment}
Consider the following 8th order system

$$
G^{*}(p)=\sum_{i=1}^{4} \frac{b_{i, 0}^{*}}{a_{i, 2}^{*} p^{2}+a_{i, 1}^{*} p+1}
$$

where the DC-gains are given by $\left[\begin{array}{llll}b_{1,0}^{*} & b_{2,0}^{*} & b_{3,0}^{*} & b_{4,0}^{*}\end{array}\right]=\left[\begin{array}{llll}3 & 0.4 & 0.2 & 0.05\end{array}\right]$, and their associated poles are, respectively, $-0.25 \pm 1.39 \mathrm{i},-0.15 \pm 3.16 \mathrm{i},-0.17 \pm 5.77 \mathrm{i}$, and $-0.5 \pm 9.99 \mathrm{i}$. The frequency response of this system is shown in Figure 9.3. The input is chosen to be a zero-mean Gaussian noise with unitary variance, and the output is being sampled every $h=0.05 \mathrm{~s}$. The noise $v\left(t_{k}\right)$ in (9.4) is given by

$$
v\left(t_{k}\right)=\frac{q+0.5}{q-0.85} e\left(t_{k}\right)
$$

where $e\left(t_{k}\right)$ is a zero-mean Gaussian white noise of variance 0.02 , yielding a signal-to-noise ratio of approximately 9 dB . Thirty different sample sizes are considered, ranging logarithmically from $N=2 \cdot 10^{3}$ to $N=10^{5}$. For each sample size, 300 Monte Carlo runs are conducted with varying input and noise realizations. The goal is to compare the performance of the proposed estimator with the SRIVC estimator, which is the algorithm behind the tfest command in the MATLAB System Identification Toolbox [158]. To this end, the mean-square error of the DC gains $b_{i, 0}$ related to each submodel is considered. Both estimators

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-247}
\captionsetup{labelformat=empty}
\caption{Figure 9.3: Bode plot of the open-loop continuous-time system \$G\^{}\{*}\$.\}\end{center}
\end{figure}

initialize at a random system that satisfies $\beta_{i}=\beta_{i}^{*}(1+\mathcal{U}(-0.05,0.05))$, where $\mathcal{U}(a, b)$ denotes the uniform distribution with lower and upper limits $a$ and $b$, respectively. The maximum number of iterations of the algorithms is set to 100 , and the relative error bound that is used as a termination rule for both iterative procedures is set to $10^{-10}$.

Figure 9.4 shows the MSE of each DC gain estimate as a function of the sample size. Both methods exhibit a linear decay in terms of the MSE of each parameter, which indicates that they are consistent in this scenario. This aligns with Theorem 9.15. Furthermore, a substantial decrease in the MSE for both $b_{2,0}$ and $b_{3,0}$ is observed when comparing the proposed method to the SRIVC method, while remaining competitive with respect to the other parameters. The proposed estimator requires 12 parameters to describe the system, whereas the standard SRIVC estimator considers 8 poles and 6 zeros, leading to a total of 15 parameters to be estimated. The parsimony of the additive structure results in more accurate estimates.

\subsection*{9.5.2 Closed-loop experiment}
Consider the following 4th order system


\begin{equation*}
G^{*}(p)=\frac{3}{0.25 p^{2}+0.25 p+1}+\frac{1}{0.025 p^{2}+0.01 p+1} \tag{9.45}
\end{equation*}


\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-248}
\captionsetup{labelformat=empty}
\caption{Figure 9.4: Mean square error of the DC gain estimates with respect to the sample size $N$, using the open-loop approach. The proposed method (-) delivers parameter estimates with less MSE for \$b\_\{2,0}\$ and $b_{3,0}$, while having similar performance to the SRIVC estimator (-) in the other parameters.\}\end{center}
\end{figure}

which is controlled by a feedback PID controller of the form

$$
C_{\mathrm{d}}(q)=0.0115+\frac{0.00725 q}{q-1}+\frac{0.00454(q-1)}{q},
$$

with sampling period $h=0.05 \mathrm{~s}$. The parameters of interest are $\beta^{*}= \left[\begin{array}{llllll}0.25 & 0.25 & 3 & 0.01 & 0.025 & 1\end{array}\right]^{\top}$. The reference is chosen to be a zero-mean Gaussian white noise signal of unitary variance, and the noise $v\left(t_{k}\right)$ is a zeromean white noise Gaussian of variance 0.01. Fifty different sample sizes are considered, ranging logarithmically from $N=500$ to $N=123036$, each with 300 Monte Carlo runs with varying reference and noise realizations. Three estimators are tested: the SRIVC estimator (i.e., the direct closed-loop approach, [77]), the CLSRIVC estimator [97], and the proposed estimator for closed-loop identification, i.e., (9.28) with instrument vector described by (9.19). The estimators are all initialized by the same mechanism detailed in Section 9.5.1, and the relative error bound for the termination rule of each algorithm is set to $10^{-7}$. The SRIVC and CLSRIVC estimators consider a model structure consisting of 4 poles and 2 zeros, and the proposed method exploits the parametrization in (9.45), which gives 6 parameters to be computed in total.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-249}
\captionsetup{labelformat=empty}
\caption{Figure 9.5: Mean square error of the system parameter estimates with respect to the sample size $N$, using the closed-loop approach. All estimators give consistent estimates, and the proposed method (-) gives the least mean-square error for every parameter, compared to the SRIVC estimator (-) and the CLSRIVC estimator (---).}
\end{center}
\end{figure}

The goal is to compare the parameter vectors of the resulting additive form of each estimator. For this, after computing the estimates using the SRIVC and CLSRIVC methods, the resulting transfer function is factored to obtain the associated additive parameter vector estimate. In Figure 9.5, the mean square error of each parameter is shown for all the sample sizes considered in this study. All three estimators are known to be generically consistent in this setup due to Theorems 10 and 14 of [106], and Theorem 9.15 of this work. However, the proposed method attains lower MSEs compared to the SRIVC and CLSRIVC methods. Similar to Section 9.5.1, this improvement is due to a more parsimonious model structure that is imposed: the proposed additive estimator only requires 6 parameters to exactly describe (9.45), as opposed to\\
the unfactored

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-250(1)}
\captionsetup{labelformat=empty}
\caption{Figure 9.6: Prototype experimental flexible beam setup. The moving part is indicated by (a) and is suspended by wire flexures (b). The deflection is measured with five contactless fiber optic sensors, of which middle sensor is used (C) and the setup is actuated with three current-driven voice coils of which the middle actuator used (d).}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-250}
\captionsetup{labelformat=empty}
\caption{Figure 9.7: Schematic illustration of the experimental setup. The three actuators are visible on the bottom part of the image and the five sensors at the top. During operation, the beam translates, rotates and exhibits internal flexible behavior. The inputs and outputs are indicated by $u$ and $y$, respectively.}
\end{center}
\end{figure}

transfer function model descriptions provided by the standard SRIVC and CLSRIVC estimators, which require 7.

\subsection*{9.6 Experimental validation}
In this section, the proposed identification method is validated using experimental data. Consider the setup depicted in Figure 9.6. The system consists of a slender and flexible steel beam of $500 \times 20 \times 2 \mathrm{~mm}$. It is equipped with five

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-251}
\captionsetup{labelformat=empty}
\caption{Figure 9.8: Estimation of the frequency response function of the flexible beam system. A nonparametric estimate (-) and the parametric modal estimate with a 4-sample delay (-) are indicated as well as the initial condition (---) of the algorithm. The proposed method converges to a parametric model closely aligned with the first four modes of the system.}
\end{center}
\end{figure}

contactless fiberoptic sensors and three voice-coil actuators and is suspended by wire flexures, leaving one rotational and one translational direction unconstrained. The system is operating at a sampling frequency of 4096 Hz , and the middle actuator and sensor are used for conducting the experiments, see Figure 9.7. Lightly-damped systems such as the one under study can be described in a modal representation of the form

$$
G^{*}(p)=\sum_{i=1}^{K} \frac{b_{i, 0}^{*}}{p^{2} / \omega_{i}^{2}+2\left(\xi_{i} / \omega_{i}\right) p+1},
$$

where $\xi_{i}$ and $\omega_{i}$ represent the relative damping and natural frequencies of the flexible modes, see [91] for more details.

An open-loop experiment is conducted with a random-phase multisine input of 2 s with a flat spectrum between 0.5 Hz to 500 Hz . The performance of the proposed method is tested for the open-loop scenario, i.e., (9.28) with instrument vector given by (9.10). Considering the input and output data $\left\{u\left(t_{k}\right), y\left(t_{k}\right)\right\}_{k=1}^{8192}$, the first four modes of the system are estimated (i.e., four second-order systems without zeros), and the result is depicted in Figure 9.8. The iterations in (9.28)\\
are initialized with the submodel estimates

$$
\begin{aligned}
\theta_{1}^{1}=\left[\begin{array}{lll}
0.001 & 0.025 & 10
\end{array}\right]^{\top}, & \theta_{2}^{1}=\left[\begin{array}{lll}
5 \mathrm{e}-5 & 1.5 \mathrm{e}-4 & 0.2
\end{array}\right]^{\top} \\
\theta_{3}^{1}=\left[\begin{array}{lll}
8.5 \mathrm{e}-7 & 2 \mathrm{e}-5 & 0.001
\end{array}\right]^{\top}, & \theta_{4}^{1}=\left[\begin{array}{lll}
7.5 \mathrm{e}-7 & 5 \mathrm{e}-6 & 0.01
\end{array}\right]^{\top}
\end{aligned}
$$

These parameters describe a model which deviates on purpose from the expected outcome to illustrate the converging nature of the algorithm and its robustness against poor initialization. The delay is estimated to be four samples and the converged estimate of the parametric model has parameters

$$
\begin{aligned}
& \hat{\theta}_{1}=\left[\begin{array}{lll}
0.0024 & 0.023 & 11.81
\end{array}\right]^{\top}, \\
& \hat{\theta}_{2}=\left[\begin{array}{lll}
2.33 \mathrm{e}-5 & 1.65 \mathrm{e}-4 & 0.159
\end{array}\right]^{\top}, \\
& \hat{\theta}_{3}=\left[\begin{array}{lll}
8.59 \mathrm{e}-7 & 1.03 \mathrm{e}-5 & 3.19 \mathrm{e}-4
\end{array}\right]^{\top}, \\
& \hat{\theta}_{4}=\left[\begin{array}{lll}
7.55 \mathrm{e}-7 & 3.55 \mathrm{e}-6 & 1.27 \mathrm{e}-4
\end{array}\right]^{\top} .
\end{aligned}
$$

These parameters describe a continuous-time model that is close to the nonparametric estimate obtained using the same data sequence, verifying the validity of the model obtained.

\subsection*{9.7 Conclusions}
In this chapter, a unified method for identifying continuous-time models in an additive form is presented, applicable to both open and closed-loop settings. This method is derived from the optimality conditions established for both scenarios and extends the properties of well-known refined instrumental variable algorithms to address the identification of systems requiring more flexible model parameterizations. The open and closed-loop estimators have been rigorously demonstrated to be generically consistent, and extensive simulations were conducted to validate this property. Moreover, the proposed additive identification method has been successfully tested using experimental data. The approach in this chapter not only provides accurate models but also offers insights beyond standard unfactored transfer function estimation, as it is capable of directly retrieving the system parameters in its additive form.

\section*{Appendices}
\section*{9.A Technical lemma}
Lemma 9.17 (Generic Nonsingularity). Consider the $n_{i}$ th order, asymptotically stable continuous-time transfer functions $1 / A_{i}(p), i=1, \ldots, K$, which depend on some parameters $\nu_{i}$, and define $A_{i}^{*}(p)$ as the polynomial $A_{i}(p)$ evaluated at $\nu_{i}=\nu_{i}^{*}$. The polynomials $A_{i}^{*}(p), i=1, \ldots, K$, are assumed to be coprime. Furthermore, consider the asymptotically stable discrete-time transfer function $S(q)$, which depends on some parameters $\xi$, and define $S^{*}(q)$ as the transfer function $S(q)$ evaluated at $\xi=\xi^{*}$. Assume that $S(q)$ has at most $m_{s}$ nonminimum phase zeros, and that Assumption 9.14 holds. If the signal $z\left(t_{k}\right)$ is quasi-stationary and persistently exciting of order no less than $2 \sum_{i=1}^{K} n_{i}+m_{s}$, then the following matrix is generically nonsingular with respect to $\left\{\nu_{i}\right\}_{i=1}^{K}$ and $\xi$ :

\[
\Phi=\overline{\mathbb{E}}\left\{\left[\begin{array}{c}
S(q) \frac{1}{A_{2}^{2}(p)} z_{1}\left(t_{k}\right)  \tag{9.46}\\
S(q) \frac{1}{A_{2}^{2}(p)} z_{2}\left(t_{k}\right) \\
\vdots \\
S(q) \frac{1}{A_{K}^{2}(p)} z_{K}\left(t_{k}\right)
\end{array}\right]\left[\begin{array}{c}
S^{*}(q) \frac{1}{A_{1}^{*}(p) A_{1}(p)} z_{1}\left(t_{k}\right) \\
S^{*}(q) \frac{1}{A_{2}^{*}(p) A_{2}(p)} z_{2}\left(t_{k}\right) \\
\vdots \\
S^{*}(q) \frac{1}{A_{K}^{*}(p) A_{K}(p)} z_{K}\left(t_{k}\right)
\end{array}\right]^{\top}\right\},
\]

where $z_{i}\left(t_{k}\right)=\left[p^{n_{i}+m_{i}}, p^{n_{i}+m_{i}-1}, \ldots, 1\right]^{\top} z\left(t_{k}\right)$, with $m_{i}<n_{i}$ for all but at most one $i$, at which $m_{i}=n_{i}$.

Proof: It is required to show that the two statements needed for applying the genericity result in Lemma A2.3 of [234] are true, which are that

\begin{enumerate}
  \item all the entries of $\Phi$ are real analytic functions of the coefficients of $A_{i}(p)$ and $S(q)$, and
  \item there exist $\left\{\nu_{i}\right\}_{i=1}^{K}$ and $\xi$ vectors that lead to $\Phi$ being nonsingular.
\end{enumerate}

The first statement follows from applying the same logic as in Lemma 9 of [191] and is therefore omitted. As for the second statement, it is shown that setting $S(q)=S^{*}(q)$ and $T_{i}(p)=T_{i}^{*}(p)$ for all $i=1,2, \ldots, K$, leads to a nonsingular\\
matrix $\Phi$. To this end, define

$$
\Phi^{*}=\overline{\mathbb{E}}\left\{\left[\begin{array}{c}
S^{*}(q) \frac{1}{\left[A_{1}^{*}(p)\right]^{2}} z_{1}\left(t_{k}\right) \\
S^{*}(q) \frac{1}{\left[A_{2}^{*}(p)\right]^{2}} z_{2}\left(t_{k}\right) \\
\vdots \\
S^{*}(q) \frac{1}{\left[A_{K}^{*}(p)\right]^{2}} z_{K}\left(t_{k}\right)
\end{array}\right]\left[\begin{array}{c}
S^{*}(q) \frac{1}{\left[A_{1}^{*}(p)\right]^{2}} z_{1}\left(t_{k}\right) \\
S^{*}(q) \frac{1}{\left[A_{2}^{*}(p)\right]^{2}} z_{2}\left(t_{k}\right) \\
\vdots \\
S^{*}(q) \frac{1}{\left[A_{K}^{*}(p)\right]^{2}} z_{K}\left(t_{k}\right)
\end{array}\right]^{\top}\right\}
$$

Take $x \in \mathbb{R}^{K+\sum_{i=1}^{K}\left(n_{i}+m_{i}\right)}$. The following inequality is direct:


\begin{equation*}
x^{\top} \Phi^{*} x=\overline{\mathbb{E}}\left\{\left(\frac{B_{x}(p)}{\prod_{i=1}^{K}\left[A_{i}^{*}(p)\right]^{2}} \tilde{z}\left(t_{k}\right)\right)^{2}\right\} \geq 0 \tag{9.47}
\end{equation*}


where $\tilde{z}\left(t_{k}\right)=S^{*}(q) z\left(t_{k}\right)$ is a persistently exciting signal of order no less than $2 \sum_{i=1}^{K} n_{i}$, and $B_{x}(p)$ is a polynomial of degree at most $2 \sum_{i=1}^{K} n_{i}-\max _{k}\left(n_{k}-\right. \left.m_{k}\right)$ of the form

$$
B_{x}(p)=\sum_{i=1}^{K} Q_{i}(p) \prod_{\substack{j=1, \ldots, K, j \neq i}}\left[A_{j}^{*}(p)\right]^{2},
$$

with $Q_{i}(i=1, \ldots, K)$ being an arbitrary polynomial of degree $n_{i}+m_{i}$, described by the entries of the vector $x$. By following the same arguments as in Eqs. (3840) of [191] (which require the persistence of excitation and sampling frequency assumptions), it is found that $x^{\top} \Phi^{*} x=0$ implies that $B_{x}(p) \equiv 0$.

Now the goal is to show that $B_{x}(p) \equiv 0$ implies that $Q_{i}(p) \equiv 0$ for all $i=1, \ldots, K$, which means that $x=0$. First, it is clear that at least two $Q$ polynomials must be nonzero in case the statement were to be disproven. To tackle this case, consider the following contradiction. Suppose that there are at least two polynomials $Q_{k}, Q_{l}$ that are nonzero, and without loss of generality ${ }^{4}$ assume that $\operatorname{deg}\left(Q_{k}\right)<2 n_{k}$. Since $B_{x}(p) \equiv 0$, the following relations between polynomials hold true:


\begin{equation*}
Q_{k}(p) \prod_{\substack{j=1, \ldots, K, j \neq k}}\left[A_{j}^{*}(p)\right]^{2}=-\left[A_{k}^{*}(p)\right]^{2} \sum_{\substack{i=1, \ldots, K, i \neq k}} Q_{i}(p) \prod_{\substack{j=1, \ldots, K, j \neq i, k}}\left[A_{j}^{*}(p)\right]^{2} \neq 0 \tag{9.48}
\end{equation*}


Thus, the polynomial in (9.48) must have the same zeros as $\left[A_{k}^{*}(p)\right]^{2}$, counting their multiplicity. These zeros cannot be accounted for in $\prod_{\substack{j=1, \ldots, K \\ j \neq k}}\left[A_{j}^{*}(p)\right]^{2}$ since the $A_{j}^{*}$ polynomials are jointly coprime, and neither in $Q_{k}(p)$, since $\operatorname{deg}\left(Q_{k}\right)<\operatorname{deg}\left(\left[A_{k}^{*}(p)\right]^{2}\right)=2 n_{k}$. It is concluded that such $Q_{k}, Q_{l}$ nonzero polynomials that satisfy the equality in (9.48) cannot exist. Hence, by contradiction,

\footnotetext{${ }^{4}$ At most one polynomial $Q_{i}$ will have degree $2 n_{i}$, due to $n_{i}=m_{i}$ occurring for at most one $i \in\{1, \ldots, K\}$.
}
it is found that all $Q$ polynomials must be zero for $B_{z}(p) \equiv 0$ to be true, and therefore, $\Phi^{*}$ is nonsingular.

Since both statements 1) and 2) are true, the result in Lemma 9.17 follows from Lemma A2.3 of [234].

\section*{Chapter}
\section*{Locating Nonlinearities in Mechanical Systems: A Frequency-Domain Dynamic Network Perspective ${ }^{1}$ }
\begin{abstract}
Accurately modeling nonlinearities is becoming increasingly important for mechanical systems, particularly in the context of system design, model-based control and monitoring systems for fault diagnosis. In the nonlinear modeling process, a pivotal phase involves pinpointing the physical locations and quantifying the magnitude of nonlinearities. The present chapter introduces a data-driven approach for nonlinearity location and quantification by analyzing nonparametric frequency response functions. To achieve this objective, measurement locations in mechanical systems are interpreted as nodes arranged in a dynamic network, and linearization techniques are employed on the frequency response functions formed from node to node. The efficacy of the proposed approach and the concept of nonlinearity localization and quantification are illustrated by numerical simulations and experiments on a flexible beam setup.
\end{abstract}

\footnotetext{${ }^{1}$ The results in this chapter constitute Contribution IX in Section 1.6. The chapter is based on: [48] K. Classens, M. Schoukens, T. Oomen, and J. P. Noël, "Locating Nonlinearities in Mechanical Systems: A Frequency-Domain Dynamic Network Perspective," submitted for journal publication.
}\subsection*{10.1 Introduction}
The necessity for nonlinear models is increasingly acknowledged as a fundamental requirement in engineering disciplines. This acknowledgment is driven by the complex behavior of real-world systems, which often exceeds the capabilities of simplistic linear representations. A comprehensive understanding of nonlinear behavior is essential for various reasons. This understanding allows engineers to predict and mitigate potential problems during the design phase, such as structural instability or excessive vibrations [146]. On the other hand, nonlinearities can be strategically leveraged to overcome the performance limitations of linear systems, for instance, by novel mechanical design [4, 23, 114, 206, 244, 253] or with nonlinear control [141, 180]. However, many engineering systems, such as precision mechanics, are designed to exhibit dominant linear behavior. In these systems, the emergence of nonlinear behavior often serves as a symptomatic indicator of underlying faults. These faults can manifest in various forms, such as wear and tear, material degradation, or structural damage.

Detecting nonlinearities has broad applications across diverse mechanical domains, for instance serving as critical tool in structural engineering to detect faults or damage and for the purpose of predictive maintenance [38, 40]. To achieve these goals, it is essential not only to detect the presence of nonlinearities within the system, but also to quantify their magnitude and precisely pinpoint their location [59, 75]. In the field of mechanics, complex nonlinearities such as friction [245], hysteresis [213], or contact nonlinearities [181] are frequently encountered. In addition to mechanics, locating nonlinearities is relevant for many other field such as understand physiological processes in biological systems [282], medical diagnosis and treatment, as well as enhancement of acoustical engineering. This interdisciplinary application underscores the significance of nonlinearity location across various fields and practical applications.

Many systems can be conceptualized as an interconnection of subsystems. Adopting the perspective of interconnected subsystems is known as the so-called dynamic networks setting. Most of the research on identification in dynamic networks has focused on acquiring reliable estimates of linear subsystems under various noise assumptions [ $56,57,73,257,258,272,273$ ], structure detection [36, 104, 171], and input selection [58]. Beyond identification of linear systems, substantial progress has been made in the identification of nonlinearities within mechanical systems in the last decade [183]. State-of-the-art methods in this domain allow to address strong nonlinearities in complex structures across diverse domains [168, 182, 210]. Only recently, frequency-domain identification and the dynamic network perspective were combined [229], allowing for detecting, locating, and quantifying nonlinearities in networks using linear approximation methods.

Although many advancements have been made in the field of nonlinearity identification, many of these approaches require prior knowledge of the locations\\[0pt]
of the nonlinearities across the structure of interest. Another common aspect is the definition of an optimality criterion, which is only valid for a specified class of systems and under specific excitation signal conditions [225, 228]. The existing literature on nonlinearity location is relatively limited [50,144,282] and, besides a nonlinear feedback perspective [151], lacks a solution for non-parametrically addressing this task. Given that many systems, including complex multi-input multi-output (MIMO) motion systems, primarily rely on non-parametric frequency response functions for modeling, there is a particular need for detecting, locating, and quantifying nonlinearities within this framework. Moreover, such approach offers considerable advantages, as experimental data is cheap to obtain and since it can be applied to a broad range of nonlinear systems with minimal reliance on prior assumptions and user input.

This chapter explores the location and quantification of nonlinearities, building on the approach presented in [229]. It further leverages the equivalences between mechanical systems and dynamic networks, supported by experimental validation. Additionally, a more detailed step-by-step procedure is outlined to guide the process. In this context, the Best Linear Approximation (BLA) [73, 200, 202, 222, 223, 224], previously employed for nonlinearity detection [ 252 , 275], forms the foundation of the proposed method, however, is now utilized for pinpointing and quantifying nonlinearities. Specifically, the MIMO BLA framework [66, 67] is integrated with the closed-loop [199, 201] and process noise [227] framework. This establishes a comprehensive framework for networked systems.

The main contributions are summarized as follows:\\
C1: A procedure is described to cast mechanical systems as dynamic networks, which is relevant for the identification of local modules and for locating and quantifying the magnitude of nonlinear elements.

C2: A detailed step-by-step procedure is described to locate nonlinearities in MIMO systems and to quantify its magnitude. The procedure is based on frequency domain data and builds upon the BLA framework for MIMO systems.

C3: The proposed method is evaluated through simulations and is experimentally validated on a flexible beam setup with a nonlinearity of varying magnitude.

The remainder of this chapter is organized as follows. In Section 10.2 dynamic networks are introduced, and the analogy between dynamic networks and mechanical systems is presented. Subsequently, the main problem is formulated. In Section 10.3 the concept of the BLA is revisited for open-loop single-input single-output (SISO) systems. Next,the focus is shifted to MIMO systems and a detailed step-by-step description of the proposed method for dynamic networks, based on the BLA concept, is given in Section 10.4. The proposed approach is\\
numerically evaluated in Section 10.5, and applied to an experimental mechanical system in 10.6. Lastly, Section 10.7 concludes this chapter.

\subsection*{10.2 System setup and problem formulation}
In this section, the general framework of dynamic network identification tailored towards mechanical systems is presented, leading to a formulation of the primary objective of this chapter.

\subsection*{10.2.1 Dynamic networks}
Consider a dynamic network consisting of $n_{y}$ nodes, as illustrated by the example in Figure 10.1. These nodes represent the internal variables of the network. Within this network, the signal associated with each node, represented by $y_{i}(t)$, is computed as the sum of all incoming signals


\begin{equation*}
y_{i}(t)=\sum_{j=1, j \neq i}^{n_{y}} y_{i j}(t)+y_{u_{i}}(t)+v_{i}(t), \tag{10.1}
\end{equation*}


where $y_{i j}(t)$ is the output of subsystem $G_{i j}$, and $y_{u_{i}}(t)$ is the output of the subsystem $G_{u_{i}}$, both pointing towards node $i$. The input of subsystem $G_{i j}$ is the node signal $y_{j}(t)$ and the input of subsystem $G_{u_{i}}$ is the external input $u_{i}(t)$. Each of the SISO subsystems may be linear or nonlinear. The remaining signal $v_{i}(t)$ denotes an additive process noise contribution that is assumed to be independent of external inputs $u_{j}(t), j \in 1, \ldots, n_{y}$ and the other noise signals $v_{k}(t), k \in 1, \ldots, n_{y}$, where $k \neq i$. The noise signal $v_{i}(t)$ is assumed to be zeromean and has a finite variance $\sigma_{v_{i}}$. This representation follows the definitions and visualization of [56, 257]. Typically, only the node signals $y_{i}(t)$ and the input signals $u_{i}(t)$ are known.

Remark 10.1. Note that in this dynamic network framework, process noise is considered at the network nodes. Hence, this does not correspond with pure measurement noise as the noise in this framework propagates through the submodels.

\subsection*{10.2.2 From mechanical system to dynamic network}
An exact analogy can be made between mechanical systems with a discrete number of masses and a dynamic network. As an illustration, the underlying linear physical system of Figure 10.2 is translated into a network representation. The networked representation is based on the equations of motion,

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-260}
\captionsetup{labelformat=empty}
\caption{Figure 10.1: A dynamic network consisting of three nodes, indicted with \$y\_\{i}\$, which are interconnected by dynamic subsystems $G_{i j}$ and $G_{u_{i}}$. The node signals are calculated by summing all incoming signals. The exogenous incoming signals consist of the external inputs $u_{i}$ and process noise contributions $v_{i}$.\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-260(1)}
\captionsetup{labelformat=empty}
\caption{Figure 10.2: A mechanical system with three masses \$m\_\{i}\$ connected by four linear springs $k_{j}$ and dampers $c_{j}$. The positions of the masses are indicated by $x_{i}$ and a force $u_{i}$ is applied to each mass.\}\end{center}
\end{figure}


\begin{align*}
m_{1} \ddot{x}_{1}(t)= & -k_{1} x_{1}(t)-k_{2}\left(x_{1}(t)-x_{2}(t)\right)  \tag{10.2a}\\
& -c_{1} \dot{x}_{1}(t)-c_{2}\left(\dot{x}_{1}(t)-\dot{x}_{2}(t)\right)+u_{1}(t) \\
m_{2} \ddot{x}_{2}(t)= & -k_{2}\left(x_{2}(t)-x_{1}(t)\right)-k_{3}\left(x_{2}(t)-x_{3}(t)\right)  \tag{10.2b}\\
& -c_{2}\left(\dot{x}_{2}(t)-\dot{x}_{1}(t)\right)-c_{3}\left(\dot{x}_{2}(t)-\dot{x}_{3}(t)\right)+u_{2}(t) \\
m_{3} \ddot{x}_{3}(t)= & -k_{3}\left(x_{3}(t)-x_{2}(t)\right)-k_{4} x_{3}(t)  \tag{10.2c}\\
& -c_{3}\left(\dot{x}_{3}(t)-\dot{x}_{2}(t)\right)-c_{4} \dot{x}_{3}(t)+u_{3}(t)
\end{align*}


where $\ddot{x}(t)=\frac{\mathrm{d}^{2} x(t)}{\mathrm{d} t^{2}}$ and $\dot{x}(t)=\frac{\mathrm{d} x(t)}{\mathrm{d} t}$. Taking $p=\frac{\mathrm{d}}{\mathrm{d} t}$, and isolating $x_{1}(t), x_{2}(t)$, and $x_{3}(t)$, allows to write the three equations of motion as


\begin{align*}
x_{1}(t)= & \frac{k_{2}+c_{2} p}{m_{1} p^{2}+\left(c_{1}+c_{2}\right) p+k_{1}+k_{2}} x_{2}(t)  \tag{10.3a}\\
& +\frac{1}{m_{1} p^{2}+\left(c_{1}+c_{2}\right) p+k_{1}+k_{2}} u_{1}(t) \\
x_{2}(t)= & \frac{k_{2}+c_{2} p}{m_{2} p^{2}+\left(c_{2}+c_{3}\right) p+k_{2}+k_{3}} x_{1}(t) \\
& +\frac{k_{3}+c_{3} p}{m_{2} p^{2}+\left(c_{2}+c_{3}\right) p+k_{2}+k_{3}} x_{3}(t)  \tag{10.3b}\\
& +\frac{1}{m_{2} p^{2}+\left(c_{2}+c_{3}\right) p+k_{2}+k_{3}} u_{2}(t) \\
x_{3}(t)= & \frac{k_{3}+c_{3} p}{m_{3} p^{2}+\left(c_{3}+c_{4}\right) p+k_{3}+k_{4}} x_{2}(t)  \tag{10.3c}\\
& +\frac{1}{m_{3} p^{2}+\left(c_{3}+c_{4}\right) p+k_{3}+k_{4}} u_{3}(t)
\end{align*}


This description of transfer functions admits the dynamic network representation in Figure 10.1, where each position measurement $x_{i}(t)$ forms a node signal $y_{i}(t)$ which is perturbed by the node noise $v_{i}(t)$. The linear subsystems $G_{i j}(p)$ are formed by the relations between $x_{i}(t)$ and $x_{j}(t)$ and the linear subsystems $G_{u_{i}}(p)$ are formed by the relations between $x_{i}(t)$ and $u_{i}(t)$. Hence, after including the node noise, the node signals are formed by


\begin{align*}
& y_{1}(t)=G_{12}(p) y_{2}(t)+G_{u_{1}}(p) u_{1}(t)+v_{1}(t)  \tag{10.4a}\\
& y_{2}(t)=G_{21}(p) y_{1}(t)+G_{23}(p) y_{3}(t)+G_{u_{2}}(p) u_{2}(t)+v_{2}(t)  \tag{10.4b}\\
& y_{3}(t)=G_{32}(p) y_{2}(t)+G_{u_{3}}(p) u_{3}(t)+v_{3}(t) \tag{10.4c}
\end{align*}


which indeed admits the visualisation of Figure 10.1.

In case nonlinear elements are present, see Figure 10.3, the equations of motion will be nonlinear. As a result, the differential equations cannot be casted in the transfer function form which is a reasoning that applies to linear systems. Still, the system can be represented as dynamic network where $y_{i j}(t)$ is the output of a (possibly) nonlinear subsystem $G_{i j}$ with input $y_{j}$ and $y_{u_{i}}(t)$ is the output of a (possibly) nonlinear subsystem $G_{u_{i}}$ with input $u_{i}$. In this particular case, the node corresponding to $y_{3}$ will be connected to a nonlinear subsystems whereas the nodes $y_{1}$ and $y_{2}$ are only connected to linear elements. Hence, this particular system still admits the representation in Figure 10.1.

Remark 10.2. The main focus in this work lies on mechanical systems with mass-spring-damper systems, however, many systems admit the proposed framework including but not limited to electrical circuits, biological systems, hydraulic systems, and industrial process plants [150].

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-262}
\captionsetup{labelformat=empty}
\caption{Figure 10.3: A mechanical system with three masses connected by springs and dampers. In this example, a single nonlinear spring is present between the third mass and the fixed world. The positions of the masses are measured and a force is applied to each mass.}
\end{center}
\end{figure}

Remark 10.3. Consider the equations of motion of the system in Figure 10.3. These are described by (10.3), which can be written in a matrix representation as

$$
\begin{aligned}
{\left[\begin{array}{l}
x_{1}(t) \\
x_{2}(t) \\
x_{3}(t)
\end{array}\right]=} & \underbrace{\left[\begin{array}{ccc}
0 & G_{12}(p) & G_{13}(p) \\
G_{21}(p) & 0 & G_{23}(p) \\
G_{31}(p) & G_{32}(p) & 0
\end{array}\right]}_{:=G_{x}(p)}\left[\begin{array}{l}
x_{1}(t) \\
x_{2}(t) \\
x_{3}(t)
\end{array}\right] \\
& +\underbrace{\left[\begin{array}{ccc}
G_{u_{1}}(p) & 0 & 0 \\
0 & G_{u_{2}}(p) & 0 \\
0 & 0 & G_{u_{3}}(p)
\end{array}\right]}_{:=G_{u}(p)}\left[\begin{array}{l}
u_{1}(t) \\
u_{2}(t) \\
u_{3}(t)
\end{array}\right],
\end{aligned}
$$

where $G_{13}(p)=G_{31}(p)=0$ since the first and third mass are not directly connected. The traditional MIMO system from inputs to outputs is represented as

$$
\left[\begin{array}{l}
x_{1}(t) \\
x_{2}(t) \\
x_{3}(t)
\end{array}\right]=\underbrace{\left[\begin{array}{lll}
S_{11}(p) & S_{12}(p) & S_{13}(p) \\
S_{21}(p) & S_{22}(p) & S_{23}(p) \\
S_{31}(p) & S_{32}(p) & S_{33}(p)
\end{array}\right]}_{:=S(p)}\left[\begin{array}{l}
u_{1}(t) \\
u_{2}(t) \\
u_{3}(t)
\end{array}\right],
$$

and relates to the networked representation via $S(p)=\left(I-G_{x}(p)\right)^{-1} G_{u}(p)$. The dynamics of the subsystems, i.e., the entries of $G_{x}(p)$ and $G_{u}(p)$, are all second order systems. The entries of $S(p)$ are all of sixth order. Hence, the dynamics of the entries in the networked representation are much simpler compared to the dynamics of the entries of $S(p)$, where the different second order entries are mixed together as a result of the inverse operator.

\subsection*{10.2.3 Problem formulation}
Consider a dynamic network with $n_{y}$ nodes, following the definitions in Section 10.2.1. The primary objective is to develop a data-driven method to determine the presence of nonlinearities and pinpoint their locations within a dynamic\\
network. To this end, the node signals $y_{i}(t), i \in 1, \ldots n_{y}$, are given, as well as the corresponding external inputs $u_{i}(t)$. The secondary objective is to quantify the extent or magnitude of these nonlinear effects.

The proposed method relies on the following assumptions:\\[0pt]
Assumption 10.4. The system is a fading memory system and belongs to the PISPO (periodic in, same period out) system class [25] from the external excitation $u_{i}$ to all the node signals $y_{i}$. For the PISPO system class, the steady-state response to a periodic excitation with period $T$ is periodic with the same period $T$. Systems that produce sub-harmonics, operate with autonomous oscillations, undergo bifurcation, or exhibit chaos are not considered in this context. However, hard nonlinearities like clipping, dead zones, relays, and quantizers are permitted [227].

Assumption 10.5. Every node $i$ is directly excited by an independent external input $u_{i}$.

Remark 10.6. Despite these assumptions, nonlinearity location is possible for systems which do not satisfy Assumption 10.4 such as the duffing oscillator which behaves like a fading memory system within limited amplitude range or hysteretic systems, however, in that case the proposed approach for quantification does not directly apply.

\subsection*{10.3 Best linear approximation}
Given the requirement for fast and reliable detection with limited userintervention, a non-parametric approach is pursued. This section briefly reviews the robust BLA framework for open-loop SISO systems and how it is used for nonlinearity detection and quantification. In the next section, this framework is extended to networked systems.

\subsection*{10.3.1 System and signal class}
As described in Section 10.2.3, the subsystems $G_{i j}, G_{u_{i}}$, have fading memory and belong to the PISPO system class [25]. Amongst others, this ensures that a periodic input applied to the system results, after a transient period, into a periodic output with the same periodicity as the input. A detailed mathematical treatment of these aspects is provided in [202].

The measured output signals of these (possibly nonlinear) systems, denoted by $y(t)$, are obtained as


\begin{equation*}
y(t)=y_{0}(t)+v(t) \tag{10.5}
\end{equation*}


where $y_{0}(t)$ is the noiseless system output of the nonlinear dynamic system with input $u(t)$, and $v(t)$ is a, possibly colored, zero-mean, finite-variance noise signal, see Figure 10.4.

The zero-mean random phase multisine input signal class is considered in the next sections. Random phase multisines give the user full control over the amplitude spectrum, but have random phases. Their periodic nature is key in term of noise and nonlinearity characterization. Random phase multisines belong to the Riemann-equivalence class of excitation signals. This class contains all signals that are (asymptotically) Gaussian distributed, and have the same power on each finite frequency interval (for the number of samples and excited frequencies growing to infinity) [224]. A random multisine signal is defined as


\begin{equation*}
u(t)=\frac{1}{\sqrt{N}} \sum_{k=1}^{N / 2-1} U_{k} \cos \left(2 \pi k f_{0} t+\phi_{k}\right) \tag{10.6}
\end{equation*}


where the phase $\phi_{k}$ is uniformly independently random distributed such that $E\left\{e^{j \phi_{k}}\right\}=0, N$ is the total number of points per period, and $f_{0}$ is the frequency resolution. For $f_{s}$ being the sampling frequency and $T$ being the period length, $f_{0}=f_{s} / N=1 / T$. A common choice is to select the phases such that they are uniformly distributed between $[0,2 \pi)$. The amplitudes $U_{k}$ can be chosen to follow a desired amplitude spectrum.

\subsection*{10.3.2 Best linear approximation definition}
The BLA framework approximates a nonlinear system with zero-mean (colored) additive output noise, i.e., following (10.5), using a linear time-invariant (LTI) model. The BLA is defined in [73, 200, 202, 222, 223] as the LTI model that minimizes the expected least-squares difference between the measured and the modeled output, hence


\begin{equation*}
G_{\mathrm{bla}}(q)=\underset{G(q)}{\arg \min } \mathbb{E}_{u, v}\left\{|\tilde{y}(t)-G(q) \tilde{u}(t)|^{2}\right\} \tag{10.7}
\end{equation*}


where


\begin{align*}
& \tilde{u}(t)=u(t)-\mathbb{E}_{u}\{u(t)\}  \tag{10.8}\\
& \tilde{y}(t)=y(t)-\mathbb{E}_{u, v}\{y(t)\} \tag{10.9}
\end{align*}


Here $\mathbb{E}_{u, v}\{\cdot\}$ denotes the expected value operator with respect to the random variations due to the input $u(t)$ and the output noise $v(t)$, and $G(q)$ belongs to the set of all possible LTI systems.

The approximation of a nonlinear system with $G_{\text {bla }}(q)$ is visualized in Figure 10.4. The approximation is the sum of three elements, namely the output of $G_{\text {bla }}$, and 2 distinct disturbance contributions. These disturbances are the

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-265}
\captionsetup{labelformat=empty}
\caption{Figure 10.4: The BLA represents a nonlinear system $G$ using a LTI contribution \$G\_\{\textbackslash text \{bla }\}\$, a stochastic nonlinear contribution $y_{s}$, and a noise source $v$.\}\end{center}
\end{figure}

noise distortion $v(t)$, which is commonly present in the LTI literature, and the stochastic nonlinear distortion $y_{s}(t)$, also referred to as the noncoherent output [224], hence,


\begin{equation*}
y(t)=G_{\mathrm{bla}}(q) u(t)+y_{s}(t)+v(t) . \tag{10.10}
\end{equation*}


Equivalently, using a frequency-domain notation the BLA is obtained from


\begin{equation*}
G_{\mathrm{bla}}\left(\omega_{k}\right)=\underset{G\left(\omega_{k}\right)}{\arg \min } \mathbb{E}_{u, v}\left\{\left|\tilde{y}\left(\omega_{k}\right)-G\left(\omega_{k}\right) \tilde{u}\left(\omega_{k}\right)\right|^{2}\right\}, \tag{10.11}
\end{equation*}


where the approximation consists of the same three contributions, i.e.,


\begin{equation*}
y\left(\omega_{k}\right)=G_{\mathrm{bla}}\left(\omega_{k}\right) u\left(\omega_{k}\right)+y_{s}\left(\omega_{k}\right)+v\left(\omega_{k}\right), \tag{10.12}
\end{equation*}


with $\omega_{k}=2 \pi k f_{0}$.\\
The total distortion consisting of the stochastic nonlinear distortion, representing the unmodeled nonlinear contributions, and the noise contribution is consequently obtained by subtracting the contribution of the best linear estimate from the output as


\begin{equation*}
y_{t}\left(\omega_{k}\right)=y\left(\omega_{k}\right)-G_{\mathrm{bla}}\left(\omega_{k}\right) u\left(\omega_{k}\right)=y_{s}\left(\omega_{k}\right)+v\left(\omega_{k}\right) . \tag{10.13}
\end{equation*}


The contributions of $y_{s}\left(\omega_{k}\right)$ and $v\left(\omega_{k}\right)$ can be separated since the nonlinear distortion $y_{s}(t)$ is linearly uncorrelated with the input $\tilde{u}(t)$, i.e., $\left(\mathbb{E}_{u}\left\{y_{s}(t) \tilde{u}(\tau)\right\}=0 \forall t, \tau\right)$, however, it is not independent of $\tilde{u}(t)$. On the contrary, the noise distortion $v(t)$ is both uncorrelated, i.e., $\left(\mathbb{E}_{u}\{v(t) \tilde{u}(\tau)\}=0 \forall t, \tau\right)$, and independent of the input $\tilde{u}(t)$. This fundamental difference enables the separation of nonlinear effects. Hence, the nonlinear distortion and the noise distortion equal


\begin{gather*}
y_{s}\left(\omega_{k}\right)=y_{0}\left(\omega_{k}\right)-G_{\mathrm{bla}}\left(\omega_{k}\right) u\left(\omega_{k}\right),  \tag{10.14}\\
v\left(\omega_{k}\right)=y\left(\omega_{k}\right)-G_{\mathrm{bla}}\left(\omega_{k}\right) u\left(\omega_{k}\right)-y_{s}\left(\omega_{k}\right) . \tag{10.15}
\end{gather*}


The nonlinearity of a system can be quantified by the variance of the nonlinear distortion. For this variance estimate the robust BLA approach can be used [202].

\subsection*{10.3.3 Measuring the BLA: the robust approach}
Two main approaches are available in the literature to determine the BLA and obtain an estimate of the variances of the stochastic and noise distortion contributions: the so-called fast approach and robust approach [202]. This section briefly recapitulates the main aspects of the robust approach, as it forms the basis of the proposed nonlinearity location and quantification algorithm.

The key observation that enables the separation between noise and the nonlinear disturbances is that the noise behaves aperiodic, while the nonlinear disturbances behave periodic for PISPO systems that are excited by a periodic input. The robust approach relies on applying $M$ different realizations of a random phase multisine signal, see (10.6), with the same power spectrum and measure $P$ steady-state periods of each experiment. For each period of each realization, the frequency response function (FRF) is calculated as


\begin{equation*}
\hat{G}_{\mathrm{bla}}^{[m, p]}\left(\omega_{k}\right)=\frac{\tilde{y}^{[m, p]}\left(\omega_{k}\right)}{\tilde{u}^{[m, p]}\left(\omega_{k}\right)}, \tag{10.16}
\end{equation*}


where $\tilde{y}^{[m, p]}\left(\omega_{k}\right)$ and $\tilde{u}^{[m, p]}\left(\omega_{k}\right)$ denote the leakage-free discrete fourier transform (DFT) of the $p$-th period and the $m$-th realization of the input and output signals $\tilde{y}(t)$ and $\tilde{u}(t)$ respectively. The sample mean over the $P$ periods is then computed as


\begin{equation*}
\hat{G}_{\mathrm{bla}}^{[m]}\left(\omega_{k}\right)=\frac{1}{P} \sum_{p=1}^{P} \hat{G}_{\mathrm{bla}}^{[m, p]}\left(\omega_{k}\right) \tag{10.17}
\end{equation*}


Subsequently, taking the sample mean over the $M$ realizations gives the BLA estimate as


\begin{equation*}
\hat{G}_{\mathrm{bla}}\left(\omega_{k}\right)=\frac{1}{M} \sum_{m=1}^{M} \hat{G}_{\mathrm{bla}}^{[m]}\left(\omega_{k}\right) \tag{10.18}
\end{equation*}


The sample variance can be taken over the $P$ periods to obtain the variance as a result of noise for the $m$-th experiment as


\begin{equation*}
\sigma_{G_{\mathrm{bla}, v}}^{[m]}{ }^{2}\left(\omega_{k}\right)=\frac{1}{P(P-1)} \sum_{p=1}^{P}\left|\hat{G}_{\mathrm{bla}}^{[m, p]}\left(\omega_{k}\right)-\hat{G}_{\mathrm{bla}}^{[m]}\left(\omega_{k}\right)\right|^{2} \tag{10.19}
\end{equation*}


The sample variance can also be computed of the $M$ realizations in order to examine the combined effect due to noise and nonlinearities on the averaged BLA as


\begin{equation*}
\sigma_{G_{\mathrm{bla}, t}}^{2}\left(\omega_{k}\right)=\frac{1}{M(M-1)} \sum_{m=1}^{M}\left|\hat{G}_{\mathrm{bla}}^{[m]}\left(\omega_{k}\right)-\hat{G}_{\mathrm{bla}}\left(\omega_{k}\right)\right|^{2} \tag{10.20}
\end{equation*}


where the subscript of $\sigma_{G_{\text {bla }}, t}^{2}\left(\omega_{k}\right)$ refers to the total effect. Hence, taking the variance over the periods quantifies the noise level in the measurements. Taking the variance over the different realizations quantifies the combined influence of the nonlinearities and the noise on the FRF. The variance in (10.19) only considers one experiment. Taking the mean of this sample variance over the $M$ experiments gives the noise variance on the averaged BLA as


\begin{equation*}
\sigma_{G_{\mathrm{bla}, v}}^{2}\left(\omega_{k}\right)=\frac{1}{M^{2}} \frac{1}{P(P-1)} \sum_{m=1}^{M} \sum_{p=1}^{P}\left|\hat{G}_{\mathrm{bla}}^{[m, p]}\left(\omega_{k}\right)-\hat{G}_{\mathrm{bla}}^{[m]}\left(\omega_{k}\right)\right|^{2} \tag{10.21}
\end{equation*}


The estimate of the variance due to the stochastic nonlinear distortion $y_{s}$, can now be separated, and is given by

\[
\sigma_{G_{\mathrm{bla}, s}}^{2}\left(\omega_{k}\right)= \begin{cases}\left(\sigma_{G_{\mathrm{bla}, t}}^{2}\left(\omega_{k}\right)-\sigma_{G_{\mathrm{bla}, v}}^{2}\left(\omega_{k}\right)\right) & \text { if } \sigma_{G_{\mathrm{bla}, t}}^{2}\left(\omega_{k}\right)>\sigma_{G_{\mathrm{bla}, v}}^{2}\left(\omega_{k}\right)  \tag{10.22}\\ 0 & \text { if } \sigma_{G_{\mathrm{bla}, t}}^{2}\left(\omega_{k}\right) \leq \sigma_{G_{\mathrm{bla}, v}}^{2}\left(\omega_{k}\right)\end{cases}
\]

Remark 10.7. Note that the variances are so-called sample variances explaining the $P-1$ and $M-1$ factors. In order to quantify the variability of the mean BLA estimate (10.18), extra factors $M$ and $P$ have been introduced in the sample variances (10.19) and (10.20). These BLA estimate variances can be scaled back to the output level to obtain the estimates of the variance of $y_{s}$ and $v$ using


\begin{align*}
& \sigma_{y_{s}}^{2}\left(\omega_{k}\right)=M \sigma_{\text {bla }, s}^{2}\left|u\left(\omega_{k}\right)\right|^{2}  \tag{10.23}\\
& \sigma_{v}^{2}\left(\omega_{k}\right)=M P \sigma_{\text {bla }, v}^{2}\left|u\left(\omega_{k}\right)\right|^{2} \tag{10.24}
\end{align*}


The system under test is considered to be nonlinear in case a nonzero $\sigma_{y_{s}}^{2}\left(\omega_{k}\right)$ variance is present. Since this is a variance estimate over the different frequencies, one can also observe in which frequency range the system is most nonlinear.

\subsection*{10.4 Nonlinearity location methodology}
The BLA approach presented in Section 10.3 is posed for open-loop SISO systems and enables the detection and quantification of nonlinearities. In contrast, for pinpointing the origin of nonlinearities in MIMO systems, the system has to be considered in the framework of dynamic networks. A key assumption to properly apply the standard BLA framework is that the noise sources disturbing the input and output of each subsystem must be uncorrelated. This assumption is violated in a dynamic network because the noise sources propagate through the subsystems. Consequently, the BLA of the network cannot be calculated directly.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-268}
\captionsetup{labelformat=empty}
\caption{Figure 10.5: A fully populated nonlinear dynamic network (left) and its BLA (right). The BLA represent the nonlinear network modules \$G\_\{i j}\$ using LTI models denoted as $G_{\text {bla }, i j}$. Furthermore, $y_{i s}$ corresponds to a stochastic nonlinear contribution, and $v_{i}$ corresponds to a noise source.\}\end{center}
\end{figure}

To circumvent this issue, a three-step methodology is followed. Instead of estimating the BLA of an open-loop SISO system, the method aims to identify the BLA of the subsystems present in the nonlinear dynamic network. To identify the network nodes that are directly connected to nonlinear elements, the measured node-to-node dynamics are linearized using the BLA framework as conceptually visualized in Figure 10.5. This is achieved by combining the BLA framework for systems operating in closed loop [199] and the BLA framework for process noise [227], and the BLA framework for MIMO systems [66, 67]. The noise variance and the stochastic nonlinear distortion variance are computed of every network node signal. This multi-stage approach avoids the introduction of bias as a result of correlated noise contributions due to the presence of feedback loops in the dynamic network.

In summary, the proposed methodology for nonlinearity location comprises three primary steps, which will be elaborated upon in the following subsections.

\begin{enumerate}
  \item Calculate the BLA from inputs to nodes and simulate the noise-free linearised system outputs;
  \item Calculate the BLA from nodes to nodes and simulate the node-to-node approximate dynamics;
  \item Conduct a residual analysis along the frequency axis using the variance of the total distortions $\sigma_{t, i}^{2}\left(\omega_{k}\right)$ and the variance of the noise distortions $\sigma_{v, i}^{2}\left(\omega_{k}\right)$. Any difference is attributed to a nonlinearity-induced variance. In contrast to the standard BLA approach, the residual analysis is performed at the signal level instead of on the basis of FRFs.
\end{enumerate}

\subsection*{10.4.1 Step 0: Measurements and data generation}
To perform the networked BLA analysis, the dynamics between the network nodes have to be estimated. To this end, all the network nodes are excited simultaneously using orthogonal multisine excitations, as outlined in [52]. Note that the network nodes also function as the system outputs, and thus there is an equal amount of inputs and outputs, i.e., $n_{u}=n_{y}$.

Similar to the random phase multisine introduced in Section 10.3, the user first selects the frequencies to be excited and a random phase multisine signal is generated for each input $u_{i}$. To perform MIMO FRF identification with $n_{u}$ inputs and outputs, $n_{u}$ experiments are constructed of which the inputs are stacked as

\[
U\left(\omega_{k}\right)=\left[\begin{array}{cccc}
u_{1,1}\left(\omega_{k}\right) & u_{1,2}\left(\omega_{k}\right) & \ldots & u_{1, n_{u}}\left(\omega_{k}\right)  \tag{10.25}\\
u_{2,1}\left(\omega_{k}\right) & u_{2,2}\left(\omega_{k}\right) & \ldots & u_{2, n_{u}}\left(\omega_{k}\right) \\
\vdots & \vdots & \ddots & \vdots \\
u_{n_{u}, 1}\left(\omega_{k}\right) & u_{n_{u}, 2}\left(\omega_{k}\right) & \ldots & u_{n_{u}, n_{u}}\left(\omega_{k}\right)
\end{array}\right] \text {, }
\]

with $u_{i, c}=e^{-\frac{j 2 \pi(i-1)(c-1)}{n_{y}}} u_{i}$ describing the orthogonal multisine excitation, where $i$ refers to the input channel number and $c$ refers to the experiment number. Hence, each row denotes a particular input channel and each column represents a new experiment. The resulting output data is stacked similarly as

\[
Y\left(\omega_{k}\right)=\left[\begin{array}{cccc}
y_{1,1}\left(\omega_{k}\right) & y_{1,2}\left(\omega_{k}\right) & \ldots & y_{1, n_{y}}\left(\omega_{k}\right)  \tag{10.26}\\
y_{2,1}\left(\omega_{k}\right) & y_{2,2}\left(\omega_{k}\right) & \ldots & y_{2, n_{y}}\left(\omega_{k}\right) \\
\vdots & \vdots & \ddots & \vdots \\
y_{n_{y}, 1}\left(\omega_{k}\right) & y_{n_{y}, 2}\left(\omega_{k}\right) & \ldots & y_{n_{y}, n_{y}}\left(\omega_{k}\right)
\end{array}\right],
\]

where $y_{i, c}$ refers to the $i$-th output channel and the $c$-th experiment.

Remark 10.8. Note that two distinct notations are used in the subscripts. Comma-separated supscripts such as $y_{i, c}$ indicate the $i$-th output channel and the $c$-th experiment, whereas $y_{i j}$ indicates the internal signal from node $j$ to node $i$.

In total $M$ realizations and $\tilde{P}$ periods of this combined experiment will executed. Of the $\tilde{P}$ periods, the last $P=\tilde{P}-\tilde{P}_{\text {transient }}$ steady state periods are used, i.e., the first periods are discarded. Here, $Y^{[m, p]}\left(\omega_{k}\right)$ and $U^{[m]}\left(\omega_{k}\right)$ denote the $m$-th realization and $p$-th period of the combined experiment, and similarly $y_{i, c}^{[m, p]}\left(\omega_{k}\right)$ and $u_{i, c}^{[m]}\left(\omega_{k}\right)$ denote its entries. The $p$ index is dropped for the input signals, as they are identical over all periods. Hence, a total of $M n_{y}$ experiments are performed, each containing $P N$ steady-state samples.

\subsection*{10.4.2 Step 1: BLA analysis from inputs to nodes}
The first step consists of calculating the MIMO BLA from all external inputs to the node signals. The BLA of a subsystem, represented by $S_{\text {bla }, i j}\left(\omega_{k}\right)$, captures the relationship from input $j$ to node $i$ with a transfer function, offering the best linear approximation in a least-squares sense. The complete $n_{y} \times n_{u}$ BLA, labeled as $S_{\text {bla }}\left(\omega_{k}\right)$, is determined from


\begin{equation*}
S_{\mathrm{bla}}\left(\omega_{k}\right)=\underset{S\left(\omega_{k}\right)}{\arg \min } \mathbb{E}_{u, v}\left\{\sum_{i=1}^{n_{y}}\left|\tilde{y}_{i}\left(\omega_{k}\right)-\sum_{j=1}^{n_{u}} S_{i j}\left(\omega_{k}\right) \tilde{u}_{j}\left(\omega_{k}\right)\right|^{2}\right\} \tag{10.27}
\end{equation*}


The expectation $\mathbb{E}_{u, v}\{\cdot\}$ is taken with respect all possible realisations of the input signals $u_{j}$ and noise signals $v_{i}$, within the considered signal class.

In practice this is realized by computing


\begin{equation*}
\hat{S}_{\mathrm{bla}}\left(\omega_{k}\right)=\frac{1}{M P} \sum_{m=1}^{M} \sum_{p=1}^{P} \tilde{Y}^{[m, p]}\left(\omega_{k}\right) \tilde{U}^{[m, p]^{-1}}\left(\omega_{k}\right), \tag{10.28}
\end{equation*}


where $\tilde{Y}$ and $\tilde{U}$ are the zero-mean equivalent signals of (10.25) and (10.26), and thus

\[
\hat{S}_{\mathrm{bla}}\left(\omega_{k}\right)=\left[\begin{array}{cccc}
S_{\mathrm{bla}, 11}\left(\omega_{k}\right) & S_{\mathrm{bla}, 12}\left(\omega_{k}\right) & \ldots & S_{\mathrm{bla}, 1 n_{u}}\left(\omega_{k}\right)  \tag{10.29}\\
S_{\mathrm{bla}, 21}\left(\omega_{k}\right) & S_{\mathrm{bla}, 22}\left(\omega_{k}\right) & \ldots & S_{\mathrm{bla}, 2 n_{u}}\left(\omega_{k}\right) \\
\vdots & \vdots & \ddots & \vdots \\
S_{\mathrm{bla}, n_{y} 1}\left(\omega_{k}\right) & S_{\mathrm{bla}, n_{y} 2}\left(\omega_{k}\right) & \ldots & S_{\mathrm{bla}, n_{y} n_{u}}\left(\omega_{k}\right)
\end{array}\right]
\]

The obtained BLA estimate (10.27) allows to calculate a noise-free and nonlinear distortion-free linear approximation $\bar{y}_{i}\left(\omega_{k}\right)$ of the node $y_{i}\left(\omega_{k}\right)$. In particular, $\bar{y}_{i, c}\left(\omega_{k}\right)$ and $y_{i, c}\left(\omega_{k}\right)$ denote the frequency content of the signals corresponding to the $c^{\text {th }}$ experiment. The approximated incoming signals of node $i$ are computed through


\begin{equation*}
\bar{y}_{i, c}^{[m]}\left(\omega_{k}\right)=\sum_{j=1}^{n_{y}} \hat{S}_{\mathrm{bla}, i j}\left(\omega_{k}\right) u_{j, c}^{[m]}\left(\omega_{k}\right) \tag{10.30}
\end{equation*}


\subsection*{10.4.3 Step 2: BLA analysis from nodes to nodes}
A second FRF estimation is carried out to obtain the estimates between nodes, considering the noise-free node signals $\bar{y}_{i}$ as inputs, and taking into account the direct contributions of the forcing signals $u_{i}$. This results in BLA estimates $G_{\text {bla }, i j}\left(\omega_{k}\right)$ from node $j$ to node $i$ and $G_{\text {bla }, i u}\left(\omega_{k}\right)$ from the system input $i$ to\\
node $i$, obtained from


\begin{align*}
G_{\text {bla }, i j}\left(\omega_{k}\right), G_{\text {bla }, u_{i}}\left(\omega_{k}\right)= & \underset{G_{i j}, G_{u_{i}}}{\arg \min } \mathbb{E}_{u, v}\left\{\sum_{i=1}^{n_{y}} \mid \bar{y}_{i}\left(\omega_{k}\right)\right.  \tag{10.31}\\
& \left.-G_{u_{i}}\left(\omega_{k}\right) u_{i}\left(\omega_{k}\right)-\left.\sum_{j=1, j \neq i}^{n_{y}} G_{i j}\left(\omega_{k}\right) \bar{y}_{j}\left(\omega_{k}\right)\right|^{2}\right\} \tag{10.32}
\end{align*}


In practice, the estimates per node are obtained as


\begin{equation*}
\hat{G}_{\mathrm{bla}, i}\left(\omega_{k}\right)=\frac{1}{M} \sum_{m=1}^{M} \bar{Y}_{i}^{[m]}\left(\omega_{k}\right) Z_{i}^{[m]^{-1}}\left(\omega_{k}\right) \tag{10.33}
\end{equation*}


where


\begin{gather*}
\hat{G}_{\text {bla }, i}\left(\omega_{k}\right)=\left[\begin{array}{llll}
G_{\text {bla }, u_{i}}\left(\omega_{k}\right) & G_{\text {bla }, i 1}\left(\omega_{k}\right) & \ldots & G_{\text {bla }, i(i-1)}\left(\omega_{k}\right) \\
& G_{\text {bla }, i(i+1)}\left(\omega_{k}\right) & \ldots & G_{\text {bla }, i n_{y}}\left(\omega_{k}\right)
\end{array}\right]  \tag{10.34}\\
\bar{Y}_{i}^{[m]}\left(\omega_{k}\right)=\left[\begin{array}{llll}
\bar{y}_{i, 1}\left(\omega_{k}\right) & \bar{y}_{i, 2}\left(\omega_{k}\right) & \ldots & \bar{y}_{i, n_{y}}\left(\omega_{k}\right)
\end{array}\right] \\
Z_{i}^{[m]}\left(\omega_{k}\right)=\left[\begin{array}{cccc}
u_{i, 1}\left(\omega_{k}\right) & u_{i, 2}\left(\omega_{k}\right) & \ldots & u_{i, n_{y}}\left(\omega_{k}\right) \\
\bar{y}_{1,1}\left(\omega_{k}\right) & \bar{y}_{1,2}\left(\omega_{k}\right) & \ldots & \bar{y}_{1, n_{y}}\left(\omega_{k}\right) \\
\vdots & \vdots & \ddots & \vdots \\
\bar{y}_{i-1,1}\left(\omega_{k}\right) & \bar{y}_{i-1,2}\left(\omega_{k}\right) & \ldots & \bar{y}_{i-1, n_{y}}\left(\omega_{k}\right) \\
\bar{y}_{i+1,1}\left(\omega_{k}\right) & \bar{y}_{i+1,2}\left(\omega_{k}\right) & \ldots & \bar{y}_{i+1, n_{y}}\left(\omega_{k}\right) \\
\vdots & \vdots & \ddots & \vdots \\
\bar{y}_{n_{y}, 1}\left(\omega_{k}\right) & \bar{y}_{n_{y}, 2}\left(\omega_{k}\right) & \ldots & \bar{y}_{n_{y}, n_{y}}\left(\omega_{k}\right)
\end{array}\right] . \tag{10.35}
\end{gather*}


Note that (10.33) to (10.36) are calculated for each node separately.\\
The frequency content of the node signals can now be calculated using the node-to-node and input-to-node linear approximate description $G_{\text {bla }, i j}\left(\omega_{k}\right)$, $G_{\text {bla }, u_{i}}\left(\omega_{k}\right)$ of the network dynamics as


\begin{equation*}
\overline{\bar{y}}_{i, c}^{[m, p]}\left(\omega_{k}\right)=G_{\mathrm{bla}, u_{i}}\left(\omega_{k}\right) u_{i, c}\left(\omega_{k}\right)+\sum_{j=1, j \neq i}^{n_{y}} G_{\mathrm{bla}, i j}\left(\omega_{k}\right) y_{j, c}^{[m, p]}\left(\omega_{k}\right) \tag{10.37}
\end{equation*}


Observe that the difference between $\overline{\bar{y}}_{i, c}^{[m, p]}$ and $y_{i, c}^{[m, p]}$ is the noise and stochastic nonlinear contribution on that particular node, the noise and stochastic nonlinearity contributions on node $i$ coming from the other nodes have been taken into account as they are present in the measured $y_{j, c}^{[m, p]}$ signal. This allows for the nonlinearity location through residual analysis in the next subsection.

\subsection*{10.4.4 Step 3: Residual analysis and nonlinearity location}
By comparing the simulated node signals $\overline{\bar{y}}_{i, c}^{[m, p]}$ and the corresponding measurements $y_{i, c}^{[m, p]}$ over multiple periods and realisations, a residual analysis can be conducted. In contrast to the classical BLA analysis discussed in Section 10.3, where variances are computed based of the mean BLA, in this context, variances are computed of the residuals signals. For the analysis, the phase-rotated experiments, indicated with subscript $c$, are stacked for each realization $m$. To this end, consider $\overline{\bar{y}}_{i}^{[\tilde{m}, p]}$ and $y_{i}^{[\tilde{m}, p]}$, with $\tilde{m}=1, \ldots, \tilde{M}$ and $\tilde{M}=M n_{y}$.

Considering the residual $e_{i}^{[\tilde{m}, p]}\left(\omega_{k}\right)=\overline{\bar{y}}_{i}^{[\tilde{m}, p]}\left(\omega_{k}\right)-y_{i}^{[\tilde{m}, p]}\left(\omega_{k}\right)$ at node $i$, the variances of the total distortions $\sigma_{t, i}^{2}\left(\omega_{k}\right)$, the variance of the noise distortions $\sigma_{v, i}^{2}\left(\omega_{k}\right)$, and the variance of the nonlinear distortions $\sigma_{s, i}^{2}\left(\omega_{k}\right)$ are separated following a similar reasoning as in Section 10.3.3. Hence, averaging over the periods $P$ and realizations $\tilde{M}$ gives


\begin{align*}
e_{i}^{[\tilde{m}]}\left(\omega_{k}\right) & =\frac{1}{P} \sum_{p=1}^{P} e_{i}^{[\tilde{m}, p]}\left(\omega_{k}\right)  \tag{10.38}\\
e_{i}^{[p]}\left(\omega_{k}\right) & =\frac{1}{\tilde{M}} \sum_{\tilde{m}=1}^{\tilde{M}} e_{i}^{[\tilde{m}, p]}\left(\omega_{k}\right) \tag{10.39}
\end{align*}


The total distortion due to noise and stochastic nonlinearities, $\sigma_{t, i}^{2}\left(\omega_{k}\right)$, is obtained by taking the variance of the residuals over the realizations. The noise distortion, $\sigma_{n, i}^{2}\left(\omega_{k}\right)$, is obtained by taking the variance of the residuals over the periods. The stochastic contribution, $\sigma_{s, i}^{2}\left(\omega_{k}\right)$, can then be obtained by taking the difference of these two.

In frequency domain, the total distortion equals


\begin{equation*}
\sigma_{t, i}^{2}\left(\omega_{k}\right)=\frac{1}{P(\tilde{M}-1)} \sum_{p=1}^{P} \sum_{\tilde{m}=1}^{\tilde{M}}\left(e_{i}^{[\tilde{m}, p]}\left(\omega_{k}\right)-e_{i}^{[p]}\left(\omega_{k}\right)\right)^{2} \tag{10.40}
\end{equation*}


and the variance due to noise equals


\begin{equation*}
\sigma_{v, i}^{2}\left(\omega_{k}\right)=\frac{1}{\tilde{M}(P-1)} \sum_{\tilde{m}=1}^{\tilde{M}} \sum_{p=1}^{P}\left(e_{i}^{[\tilde{m}, p]}\left(\omega_{k}\right)-e_{i}^{[\tilde{m}]}\left(\omega_{k}\right)\right)^{2} . \tag{10.41}
\end{equation*}


With these, the variance due to stochastic nonlinearities is obtained from the difference, i.e.,

\[
\sigma_{s, i}^{2}\left(\omega_{k}\right)= \begin{cases}\sigma_{t, i}^{2}\left(\omega_{k}\right)-\sigma_{v, i}^{2}\left(\omega_{k}\right) & \text { if } \quad \sigma_{t, i}^{2}\left(\omega_{k}\right)>\sigma_{v, i}^{2}\left(\omega_{k}\right)  \tag{10.42}\\ 0 & \text { if } \quad \sigma_{t, i}^{2}\left(\omega_{k}\right) \leq \sigma_{v, i}^{2}\left(\omega_{k}\right)\end{cases}
\]

The level of the estimated nonlinear variance $\sigma_{s, i}^{2}$ quantifies the nonlinear distortion at each network node. With the nonlinearity levels at each node, the nonlinear part of the dynamic network can be pinpointed by comparison between the nodes. If no nonlinearity is present at a given node, the total distortion equals the noise distortion.

\subsection*{10.5 Simulation study}
This section demonstrates the proposed three-step nonlinearity location method from Section 10.4 applied to the three-mass system from Figure 10.3 where the nonlinear element between the third mass and the fixed world is a cubic spring. Before applying the three-step method, the classical MIMO BLA framework is applied to illustrate that the classical method does not allow nonlinearity location, but only detection.

The physical properties of the system are equal to $m_{1}=1 \mathrm{~kg}, m_{2}=0.8$ kg , and $m_{3}=1.2 \mathrm{~kg}$, with spring constants $k_{1}=2 \cdot 10^{4} \mathrm{~N} / \mathrm{m}, k_{2}=3.5 \cdot 10^{4} \mathrm{N} / \mathrm{m}, k_{3}=5 \cdot 10^{4} \mathrm{~N} / \mathrm{m}$, and $k_{4}=8 \cdot 10^{4} \mathrm{~N} / \mathrm{m}$. The nonlinear spring constant $k_{\mathrm{nl}}=1 \cdot 10^{8} \mathrm{~N} / \mathrm{m}^{3}$, and the damping constants are uniformly assigned as $c_{1}= c_{2}=c_{3}=c_{4}=8 \mathrm{Ns} / \mathrm{m}$.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-273}
\captionsetup{labelformat=empty}
\caption{Figure 10.6: The BLA from the inputs to nodes \$\textbackslash hat\{S}\_\{\text{bla}\}(\textbackslash cdot)\$ of the three mass-spring-damper system with cubic spring from Figure 10.3 and the true underlying system without nonlinearity, i.e., $S$ with $k_{\mathrm{nl}}=0$ (一).\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-274}
\captionsetup{labelformat=empty}
\caption{Figure 10.7: Nonlinearity detection in the three-mass-spring-damper system using the classical input-to-node dynamics. The spectra of the node signals \$y\_\{i}\textbackslash left(\textbackslash omega\_\{k\}\textbackslash right)(\textbackslash times)\$, the total distortion levels $\sigma_{t, i}^{2}\left(\omega_{k}\right)(\times)$, the distortions due to noise $\sigma_{v, i}^{2}\left(\omega_{k}\right)(x)$, and the nonlinear distortions $\sigma_{s, i}^{2}\left(\omega_{k}\right)(x)$, are depicted. Since the nonlinear distortion is above the noise floor, dominant nonlinearity is detected.\}\end{center}
\end{figure}

The system is excited with three multisines which are simultaneously applied to the three masses. These signals excite the system at all frequencies in the $5-100 \mathrm{~Hz}$ frequency range with a root-mean-square amplitude of 50 N and a random uniformly distributed phase. The position signals, i.e., the node signals, are generated using a nonlinear Newmark integration algorithm ([92]) with a sampling frequency of 5000 Hz . Each experiment has 4 periods with $N=8192$ samples each. The experiment is repeated for $M=10$ different realizations, i.e., $\tilde{M}=30$ experiments, of which the last $P=2$ steady-state periods are used for the following analysis. The resulting displacement is measured and corrupted with a white Gaussian noise signal such that the signal-to-noise ratio is 40 dB .

First, the BLA estimate $\hat{S}_{\text {bla }}$ is computed from the inputs to the nodes using the robust BLA method, i.e., (10.28) in practice. This BLA is depicted in Figure 10.6 together with the underlying system without nonlinearity.

Nonlinearities can be detected and quantified in the classical MIMO BLA setting from inputs to network nodes. This can be achieved using the analysis presented in Section 10.4.4, but with residuals based on $\bar{y}_{i, c}^{[m, p]}$ obtained from (10.30) instead of $\overline{\bar{y}}_{i, c}^{[m, p]}$. The resulting node signals and distortions are presented in Figure 10.7. The nonlinear distortion is above the noise distortion for all network nodes indicating dominant nonlinear behavior. Yet, only a single nonlinear element is present. Therefore, the traditional input-to-node BLA analysis is not adequate for pinpointing the location of the nonlinear element.

Next, the node-to-node BLA $\hat{G}_{\text {bla }}$ is estimated using (10.33). This BLA is depicted in Figure 10.8 together with the underlying true node-to-node dynamics corresponding to the imposed dynamic network structure from Figure 10.1. Note

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-275}
\captionsetup{labelformat=empty}
\caption{Figure 10.8: The BLA of the node-to-node dynamics \$\textbackslash hat\{G}\_\{\text{bla}\}(\textbackslash bullet)\$ of the three mass-spring-damper system from Figure 10.3 and the true underlying node-to-node dynamics with $k_{\mathrm{nl}}=0$ (-). Note that the imposed dynamic network structure from Figure 10.1 does not include diagonal entries, $\hat{G}_{\mathrm{bla}, 13}$, and $\hat{G}_{\mathrm{bla}, 31}$.\}\end{center}
\end{figure}

that these node-to-node dynamics are of low order compared to the dynamics from the inputs to the nodes in Figure 10.6, c.f. (10.3), and that the imposed dynamic network does not include diagonal entries, $\hat{G}_{\mathrm{bla}, 13}$, and $\hat{G}_{\mathrm{bla}, 31}$.

Finally, the approach for the detection, location and quantification of Section 10.4.4 is applied. The result is depicted in Figure 10.9, which reveals that the nonlinear distortion is smaller compared to the noise distortion at nodes 1 and 2 , suggesting predominantly linear system behavior of the connected elements. In sharp contrast, at node 3, the situation is reversed, indicating dominant nonlinear distortions compared to the noise distortion. In conclusion, from Figure 10.9 is inferred that a nonlinear element affects the three-mass system dynamics, and that it is attached to the third mass 3 on one side and grounded on the other side. Note that due to the nature of the elements present in the example, i.e., springs and dampers, it is evident that no nonlinear element is present between

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-276}
\captionsetup{labelformat=empty}
\caption{Figure 10.9: Nonlinearity detection in the three-mass-spring-damper system using the node-to-node dynamics. The spectra of the node signals \$y\_\{i}\textbackslash left(\textbackslash omega\_\{k\}\textbackslash right)(x)\$, the total distortion levels $\sigma_{t, i}^{2}\left(\omega_{k}\right)(x)$, the distortions due to noise $\sigma_{v, i}^{2}\left(\omega_{k}\right)(x)$, and the nonlinear distortions $\sigma_{s, i}^{2}\left(\omega_{k}\right)(x)$, are depicted. Dominant nonlinear dynamics are observed at the third node since the nonlinear distortion is larger than the noise distortion.\}\end{center}
\end{figure}

the second and third mass as such nonlinear element would result in a nonlinear distortion affecting the second node.

\subsection*{10.6 Experimental validation}
This section demonstrates the nonlinearity location methodology from Section 10.4 applied to the flexible beam system from Figure 10.10. The beam is $500 \times 20 \times 2 \mathrm{~mm}$, is suspended with wire flexures, and is actuated with three current-driven voice-coil actuators. The deflection is measured with five contactless fiberoptic sensors. The outer two and the middle sensor are used for the following experiment. A schematic drawing is depicted in Figure 10.11, which illustrates the free degrees of freedom. Next to the internal deformation, the beam can translate and is free to rotate, see [43] for further details.

By means of an artificial control loop, a nonlinear cubic spring force is added between the fixed world and the third actuator-sensor pair. This is done by manipulating the third input as


\begin{equation*}
u_{3}=u_{3, \mathrm{o}}-k_{\mathrm{nl}}\left(y_{3}-y_{3, \mathrm{r}}\right)^{3}, \tag{10.43}
\end{equation*}


where $u_{3}$ is the applied input, $u_{3, \mathrm{o}}$ is the non-manipulated input, and $y_{3, \mathrm{r}}$ is the relaxed position of the cubic spring. Similar to the numerical demonstration from Section 10.5, multisine signals are simultaneously applied to the three masses which excite the system in the $0.25-500 \mathrm{~Hz}$ band with a root-mean-square

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-277(1)}
\captionsetup{labelformat=empty}
\caption{Figure 10.10: Prototype experimental flexible beam setup. The moving part is indicated by (a) and is suspended by wire flexures (b). The deflection is measured with five contactless fiberoptic sensors of which three are used (c) and the setup is actuated with three current-driven voice coils (d). A cubic stiffness is artificially added between fixed world and the third actuator-sensor pair.}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-277}
\captionsetup{labelformat=empty}
\caption{Figure 10.11: Visualisation of the flexible beam setup from Figure 10.10. The actuator and sensor locations are indicated by \$u\_\{i}\$ and $y_{i}$ respectively. During excitation, the beam translates, rotates, and exhibits flexible behavior.\}\end{center}
\end{figure}

amplitude of 0.02 A . The system is measured at 4096 Hz and $\tilde{M}=6$ experiments are repeated over $\tilde{P}=6$ periods of 4 s of which the last $P=3$ steady state periods are used. These measurements are repeated for three different cubic spring constants where $k_{\mathrm{nl}} \in\{0,1,10\}$.

To be able to perform the nonlinearity location, the networked BLA setting is considered. As the system under test in this case is a distributed system, and not a lumped system such as the numerical example in Section 10.5, a dense network with dynamic interconnections between every node is considered. This particular network structure is depicted in Figure 10.5. Each of the output measurements corresponds to one of the network nodes and each node is excited by $u_{i}$ through the submodel $G_{u_{i}}$.

In the first step, the BLA estimate $\hat{S}_{\text {bla }}$ is calculated from inputs to the nodes using the robust BLA method, that is, using (10.28). This BLA is depicted in Figure 10.12 for the $k_{\mathrm{nl}}=1$ case. The first resonance relates to the wire flexures that suspend the beam and the second resonance around 32.5 Hz is the first internal mode of the beam.

A naive nonlinearity analysis from input to node, as is done by the classical MIMO BLA framework, results in the detection of nonlinearity in all three

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-278}
\captionsetup{labelformat=empty}
\caption{Figure 10.12: The BLA from the inputs to nodes \$\textbackslash hat\{S}\_\{\text{bla}\}(\textbackslash bullet)\$ obtained for the $k_{\mathrm{nl}}=1$ setting and a FRF measured of the system without nonlinearity, i.e., $\hat{S}$ with $k_{\mathrm{nl}}=0$ (一).\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-279}
\captionsetup{labelformat=empty}
\caption{Figure 10.13: Nonlinearity detection in the flexible beam system with \$k\_\{\textbackslash mathrm\{nl}\}=1\$ using the classical MIMO BLA approach with input-to-node dynamics. The spectra of the node signals $y_{i}\left(\omega_{k}\right)(\mathrm{x})$, the total distortion levels $\sigma_{t, i}^{2}\left(\omega_{k}\right)(\mathrm{x})$, the distortions due to noise $\sigma_{v, i}^{2}\left(\omega_{k}\right)(x)$, and the nonlinear distortions $\sigma_{s, i}^{2}\left(\omega_{k}\right)(x)$, are depicted. Since the nonlinear distortion is above the noise floor, dominant nonlinearity is detected in the system. Pinpointing the precise location is not possible using the classical approach.\}\end{center}
\end{figure}

measurement locations, as shown in Figure 10.13. Although such an analysis allows for the detection and quantification of the nonlinearity present in the system, it clearly does not allow to pinpoint the location of the nonlinearity since the nonlinear distortion is larger than the noise distortion at every node.

Finally, the three-step approach for detection, location, and quantification of Section 10.4.4 is applied. Using these networked BLAs, a detailed distortion analysis at the node level is possible in a networked setting. The results depicted in Figures 10.14 to 10.16 for $k_{\mathrm{nl}}=0, k_{\mathrm{nl}}=1$, and $k_{\mathrm{nl}}=10$ respectively.

Without nonlinear spring, i.e., $k_{\mathrm{nl}}=0$, no dominant source of nonlinearity is detected because the noise distortion is almost equal to the total distortion in Figure 10.14. Some weak low-frequency nonlinearity is detected. This distortion can most likely be attributed to other non-idealities that are present in the setup. When the nonlinear spring stiffness is increased to $k_{\mathrm{nl}}=1$, a $\sim 10 \mathrm{~dB}$ clearance is observed between the estimated noise level and the nonlinear distortion at the third node, see Figure 10.15. Furthermore, the nonlinearity is mainly active at the resonance frequency associated to the third node. This corresponds to the presence of a nonlinear spring at that location in the physical setup. In case $k_{\mathrm{nl}}=$ 10, see Figure 10.16, the magnitude is even larger indicating a nonlinearity of larger magnitude. In conclusion, the proposed algorithm does not only locate the nonlinearity, but also quantifies the nonlinearity over the considered frequency range.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-280}
\captionsetup{labelformat=empty}
\caption{Figure 10.14: Networked BLA nonlinearity analysis on the measurement nodes obtained for the \$k\_\{\textbackslash mathrm\{nl}\}=0\$ setting. The spectra of the node signals $y_{i}\left(\omega_{k}\right) (\mathrm{x})$, the total distortion levels $\sigma_{t, i}^{2}\left(\omega_{k}\right)(\mathrm{x})$, the distortions due to noise $\sigma_{v, i}^{2}\left(\omega_{k}\right) (\times)$, and the nonlinear distortions $\sigma_{s, i}^{2}\left(\omega_{k}\right)(\times)$, are depicted. No dominant nonlinear dynamics are observed since the noise distortion has a higher or equal contribution with respect to the nonlinear distortion.\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-280(1)}
\captionsetup{labelformat=empty}
\caption{Figure 10.15: Networked BLA nonlinearity analysis on the measurement nodes obtained for the \$k\_\{\textbackslash mathrm\{nl}\}=1\$ setting. The spectra of the node signals $y_{i}\left(\omega_{k}\right) (\times)$, the total distortion levels $\sigma_{t, i}^{2}\left(\omega_{k}\right)(\times)$, the distortions due to noise $\sigma_{v, i}^{2}\left(\omega_{k}\right) (x)$, and the nonlinear distortions $\sigma_{s, i}^{2}\left(\omega_{k}\right)(x)$, are depicted. Dominant nonlinear dynamics are observed at the third node because the nonlinear distortion is significantly above the noise distortion.\}\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=\textwidth]{2025_10_13_d0faf158053d0f7500c0g-281}
\captionsetup{labelformat=empty}
\caption{Figure 10.16: Networked BLA nonlinearity analysis on the measurement nodes obtained for the \$k\_\{\textbackslash mathrm\{nl}\}=10\$ setting. The spectra of the node signals $y_{i}\left(\omega_{k}\right) (\mathrm{x})$, the total distortion levels $\sigma_{t, i}^{2}\left(\omega_{k}\right)(\mathrm{x})$, the distortions due to noise $\sigma_{v, i}^{2}\left(\omega_{k}\right) (\times)$, and the nonlinear distortions $\sigma_{s, i}^{2}\left(\omega_{k}\right)(\times)$, are depicted. An increased nonlinear distortion is observed at the third node since the magnitude is higher compared to Figure 10.15.\}\end{center}
\end{figure}

\subsection*{10.7 Conclusion}
This chapter presents a non-parametric frequency domain approach to locate nonlinearities and quantify its magnitude. The nonlinear system is conceptualized as a dynamic network and a residual analysis is conducted based on residuals between the measurement data and the linearized node-to-node dynamics. The graphical output consists of frequency-domain graphs per measurement location, allowing to easily detect and pinpoint the origin of the nonlinear behavior, which is essential during the system design phase and after commissioning machines for the purpose of online fault detection and isolation. The efficacy of the proposed approach is demonstrated through numerical simulations and experimental validation. In conclusion, this approach offers a systematic and quantitative means to evaluate the impact of nonlinearity across a broad spectrum of nonlinear systems.

\section*{Part V}
\section*{Closing}
\section*{Chapter}
\section*{Conclusions and Recommendations}
\subsection*{11.1 Conclusions}
The presented model-based fault detection and isolation algorithms allow to leverage the accurate models, already created prior to commissioning a machine, along with the available vast data streams to create effective monitoring and diagnostics systems. These systems, in turn, enhance the reliability and economic value of mechatronic production equipment. The cornerstones of the presented approaches stem from the field of control theory, identification, and fault diagnosis and enable to address the challenges that appear when system complexity increases, including closed-loop aspects, MIMO aspects, and the simultaneous presence of modeling uncertainty, disturbances, and faults. Traditional fault diagnosis approaches have not been able to meet the demands of high-precision mechatronic systems, and their successful application had never been demonstrated. The challenge posed by the increased system complexity motivate the overarching research objective: to develop robust advanced fault diagnosis systems for closed-loop MIMO systems, while demonstrating practical applicability through proofs-of-concept and exploring new perspectives on identification. This thesis is divided into three parts, corresponding to the contributions I-III, IV-VI, and VII-IX as formulated in Section 1.6. Detailed conclusions for each part are provided next, followed by recommendations for future research.

\subsection*{11.1.1 Fault Diagnosis for Uncertain Systems}
A key aspect in fault diagnosis involves distinguishing faults from disturbances and modeling uncertainty. This thesis solves this challenge through robust fault diagnosis methods for uncertain systems. In Chapter 2, a novel approach for robust fault detection filter design is developed for continuous-time LTI MIMO uncertain systems operating in both open and closed-loop settings. The proposed solution involves solving a single Riccati equation that optimizes performance across all frequencies and is based on an upper-bound envelope that bounds the uncertainty and disturbance models. This method achieves an optimal compromise between sensitivity to faults and the rejection of disturbances and modeling uncertainties, effectively tackling the key challenge in fault-diagnosis system design to discriminate faults from disturbances and modeling uncertainty.

Since a user-friendly design framework for application of fault diagnosis approaches to uncertain mechatronic is lacking, this particular approach is applied to a next-generation prototype wafer stage in Chapter 3. The proposed procedure encompasses all steps for real-world application from identifying an uncertain plant model to the design of the fault detection filter and analyzing the time-domain residual responses. This marks the first application of such a robust fault detection method to high-precision production equipment, representing a significant advancement in fault diagnosis for mechatronics.

Additionally, an approach is proposed in Chapter 4 that builds upon the attractive strong foundations of $\mu$-synthesis to balance fault sensitivity and disturbance rejection amidst modeling uncertainties. This approach formulates the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ problem within an $\mathcal{H}_{\infty}$ framework using a structured complex perturbation for robust performance analysis and synthesis. The solution is used as an initial condition for fixed-structure synthesis, significantly reducing conservatism compared to traditional $\mu$-approaches. The approach is validated on a next-generation prototype wafer stage, demonstrating its practical applicability and potential to enhance fault detection in complex industrial systems.

\subsection*{11.1.2 Fault Diagnosis for Nominal Systems}
The vast majority of fault diagnosis methods is developed for open-loop sys- tems, despite often being applied in closed-loop environments. The implications of applying an open-loop formulation in such closed-loop setting have not been fully clarified. By investigating closed-loop operators, the consequences of this approach are examined, particularly in terms of identification and interaction between submodules. The findings conclude that, for optimal performance, it is crucial to account for the closed-loop configuration. This is thoroughly described in Chapter 5, which addresses these ambiguities and provides a deeper understanding of the closed-loop fault diagnosis.

Successful large-scale application is demonstrated on a prototype wafer stage in Chapter 6. The nullspace-based paradigm is adopted to find a fault detection\\
and isolation filter capable of discriminating 17 distinct fault scenarios. Additionally, the essentials for fault detection and isolation in closed-loop controlled mechatronic systems are presented, translating solvability requirements to expose fundamental limitations. Using this theory, a nullspace-based fault detection and isolation system is synthesized, and experimental results illustrate the effectiveness of the proposed method. This establishes the method's feasibility for large-scale industrial applications, such as wafer or reticle stages.

A new alternative method is proposed to solve the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ problem for nominal systems in Chapter 7. In particular, a method is proposed to shape the minimum and maximum singular value of the closed-loop performance channel and its effectiveness is illustrated in the context of fault diagnosis. A bilinear matrix inequality is derived that constrains the minimum gain and is directly be implemented in combination with various multiobjective matrix inequalities and applied to synthesize filters for a wide range of control and estimation problems.

\subsection*{11.1.3 New Perspectives on Identification for Fault Detection and Isolation}
A single (non-additive) transfer function is commonly employed in linear system identification, however, practical applications in flexible motion systems are more effectively conceptualized as a sum of transfer functions with distinct denominators. These additive model parametrizations bring benefits such as enhanced physical insight for fault diagnosis and improved numerical conditioning for high-order systems. Commonly encountered faults in mechanical systems such as wear and tear, fatigue, or loose components lead to shifting of resonance dynamics, which is precisely captured in these additive modal representations. In the Chapters 8 and 9 novel identification algorithms are developed to directly estimate continuous-time additive models from input-output data. First, in Chapter 8, a recursive method is developed to identify additive systems for both open and closed-loop setups. The proposed algorithms, based on a block-coordinate descent with refined instrumental variables, are capable to track relatively slow time-varying parameters of continuous-time systems in realtime. This allows tracking of more parsimonious and physically-relevant model representations such as modal systems.

Next, in Chapter 9 offline approaches are developed to identifying continuous- time models in this additive form, applicable to both open and closed-loop settings. This method is derived from the optimality conditions for both scenarios and extends the properties of well-known refined instrumental variable algorithms to address the identification of systems requiring more flexible model parameterizations. The open and closed-loop estimators are generically consistent and the approaches in both Chapters 8 and 9, have been successfully tested using experimental data.

Faults in mechatronic systems often result in nonlinear effects. A data-driven\\
approach for nonlinearity location and quantification by analyzing nonparametric frequency response functions is proposed in Chapter 10. To achieve this objective, measurement locations in mechanical systems are interpreted as nodes arranged in a dynamic network, and linearization techniques are employed on the frequency response functions formed from node to node. The efficacy of the proposed approach and the concept of nonlinearity localization and quantification are illustrated by numerical simulations and experiments on a flexible beam setup. In conclusion, this approach offers a systematic and quantitative means to evaluate the impact of nonlinearity across a broad spectrum of nonlinear systems.

\subsection*{11.2 Recommendations}
The developments in the field of fault diagnosis for nominal and uncertain systems, and the new perspectives on identification, have led to the following insights and recommendations for future research.

Robust fault isolation for uncertain systems: The robust fault detection methods in Chapters 2-4 are directly extendable such that these include the fault isolation task. To this end, standard approaches [61, 264] directly apply. The main idea is to enforce a signature on a set of residual signals. For each signal, a different selection of the faults are considered as disturbances. From this signature, the faults can be isolated. Such fault isolation approach is already used in Chapter 6 in this thesis.

Integrating approaches for uncertain systems in current FDI tools: The robust fault detection methods in Chapters 2-3 make use of an upper-bound envelope that encaptures the effect of exogenous disturbances and modeling uncertainty. Once this upper bound is computed, the fault detection filter follows from the co-inner-outer factorization, similar to optimal approaches for nominal systems. Hence, once the upper bounds are designed, the robust approaches from Chapters 2-3 can directly be integrated into tools for fault detection and isolation filter design for nominal systems.

\section*{Direct fault detection filter synthesis for uncertain systems in the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ framework:}
The robust fault detection approaches in Chapters 2-4 are both in some sense indirect. The approach in Chapter 2 requires the computation of an upper-bound envelope to encapture the exogenous disturbances and modeling uncertainty. The approach in Chapter 4 reformulates the $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ problem to an $\mathcal{H}_{\infty}$ framework. A direct fault detection filter synthesis approach is envisioned using a solution similar to $\mu$-synthesis, but where the generalized structured singular\\
value $\mu_{g}$ [178] is used in the D-step [233].\\[0pt]
Experiment design for fault diagnosis: To obtain models specifically tailored to achieve a high fault detection performance, dedicated experiments may be performed to obtain tailored models, similar to the approaches used in the field of experiment design for control [65]. This thesis and in particular Chapters 3-5 present guidelines to obtain models for fault diagnosis filter design.

Stacked recursive simplified refined instrumental variable method: The approach for recursive estimation in Chapter 8 integrates the recursive simplified refined instrumental variable method for additive systems with blockcoordinate descent to minimize an exponentially-weighted cost function. Application of the block-coordinate descent method can be omitted by stacking the instrument vectors for each of the modes as in the offline method presented in Chapter 9. In this case, the iteration loop related to the block-coordinate descent is omitted, possibly saving computational resources which is particularly relevant in online estimation approaches.

MIMO constrained additive simplified refined instrumental variable method: The approach for offline estimation of additive systems in Chapter 9 is based on a single-input single-output formulation. This approach is extendable to MIMO systems with a common matrix denominator. This involves parameterizing the MIMO system as a left matrix fraction decomposition [3] and manipulation to the standard regression form. To constrain the solution to modal or generally damped systems, both relevant in the field of mechatronics, the system is parameterized with common inverse transfer function matrix and rank 1 IPEM may be used to enforce the rank 1 condition in the numerator matrix coefficients [238].

Frequency-domain additive simplified refined instrumental variable method: The approach for offline estimation of additive systems in Chapter 9 relies on time-domain data. Alternative frequency-domain formulations also exist [22], and these can be similarly extended to include the additive formulation.

Nonlinearity location with single-input multi-output experiments: The approach for nonlinearity detection, location, and quantification in Chapter 10 requires simultaneous excitation of all nodes. This requirement may be relaxed through the concatenation of data sequentially measured in single-input conditions.

\section*{Bibliography}
[1] A. Abdi and C. Amrit, "A review of travel and arrival-time prediction methods on road networks: classification, challenges and opportunities," PeerJ Computer Science, vol. 7, no. e689, 2021.\\[0pt]
[2] P. Aghion, P. Howitt, and L. Bursztyn, The Economics of Growth, Cambridge, MA, USA: MIT Press, 2009.\\[0pt]
[3] M. Akroum and K. Hariche, "Extending the SRIV identification algorithm to MIMO LMFD models," Journal of Electrical Engineering and Technology, vol. 4, no. 1, pp. 135-142, 2009.\\[0pt]
[4] D. Antonio, D. H. Zanette, and D. López, "Frequency stabilization in nonlinear micromechanical oscillators," Nature Communications, vol. 3, no. 806, 2012.\\[0pt]
[5] P. Apkarian and D. Noll, "Nonsmooth $H_{\infty}$ synthesis," IEEE Transactions on Automatic Control, vol. 51, no. 1, pp. 71-86, 2006.\\[0pt]
[6] M. ApS, MOSEK optimization toolbox for MATLAB, 2024.\\[0pt]
[7] M. Armendia, M. Ghassempouri, E. Ozturk, and F. Peysson, editors, Twin-control: A digital twin approach to improve machine tools lifecycle, Springer, 2019.\\[0pt]
[8] K. J. Åström, "Maximum likelihood and prediction error methods," IFAC Proceedings Volumes, vol. 12, no. 8, pp. 551-574, 1979.\\[0pt]
[9] K. J. Åström, P. Hagander, and J. Sternby, "Zeros of sampled systems," Automatica, vol. 20, no. 1, pp. 31-38, 1984.\\[0pt]
[10] E. W. Bai, "Identification of nonlinear additive FIR systems," Automatica, vol. 41, no. 7, pp. 1247-1253, 2005.\\[0pt]
[11] E.-W. Bai and K.-S. Chan, "Identification of an additive nonlinear system and its applications in generalized Hammerstein models," Automatica, vol. 44, no. 2, pp. 430-436, 2008.\\[0pt]
[12] G. Balas, R. Chiang, A. Packard, and M. Safonov, Robust Control Toolbox User’s Guide, 2024.\\[0pt]
[13] G. J. Balas, J. C. Doyle, K. Glover, A. Packard, and R. Smith, $\mu$-Analysis and Synthesis Toolbox.\\[0pt]
[14] J. A. Ball, I. Gohberg, and L. Rodman, Interpolation of Rational Matrix Functions, Basel: Birkhäuser Basel, 1990.\\[0pt]
[15] E. C. Balta, D. M. Tilbury, and K. Barton, "A Digital Twin Framework for Performance Monitoring and Anomaly Detection in Fused Deposition Modeling," in 2019 IEEE 15th International Conference on Automation Science and Engineering (CASE), 2019.\\[0pt]
[16] R. J. Barro and X. Sala-i-Martin, "Technological Diffusion, Convergence, and Growth," Journal of Economic Growth, vol. 2, no. 1, pp. 1-26, 1997.\\[0pt]
[17] M. Basseville and I. V. Nikiforov, Detection of Abrupt Changes: Theory and Application, Prentice Hall, 1996.\\[0pt]
[18] G. Betta and A. Pietrosanto, "Instrument Fault Detection and Isolation: State of the Art and New Research Trends," IEEE Transactions on Instrumentation and Measurement, vol. 49, no. 1, pp. 100-107, 2000.\\[0pt]
[19] X. Bi, R. Qin, D. Wu, S. Zheng, and J. Zhao, "One Step Forward for Smart Chemical Process Fault Detection and Diagnosis," Computers 68 Chemical Engineering, vol. 164, pp. 107884, 2022.\\[0pt]
[20] P. Billingsley, Probability and Measure, John Wiley \& Sons, 1995.\\[0pt]
[21] M. Blanke, M. Kinnaert, J. Lunze, and M. Staroswiecki, Diagnosis and Fault-Tolerant Control, 3. ed, Berlin: Springer science, 2016, 695 pages.\\[0pt]
[22] R. S. Blom and P. M. J. Van den Hof, "Multivariable Frequency Domain Identification Using IV-based Linear Regression," in 49th IEEE Conference on Decision and Control (CDC), Atlanta, GA, USA: IEEE, 2010, pp. 1148-1153.\\[0pt]
[23] N. Boechler, G. Theocharis, and C. Daraio, "Bifurcation-Based Acoustic Switching and Rectification," Nature Materials, vol. 10, no. 9, pp. 665668, 2011.\\[0pt]
[24] F. Boeren, L. Blanken, D. Bruijnen, and T. Oomen, "Optimal estimation of rational feedforward control via instrumental variables: With application to a wafer stage," Asian Journal of Control, vol. 20, no. 3, pp. 975-992, 2018.\\[0pt]
[25] S. Boyd and L. Chua, "Fading Memory and the Problem of Approximating Nonlinear Operators with Volterra Series," IEEE Transactions on Circuits and Systems, vol. 32, no. 11, pp. 1150-1161, 1985.\\[0pt]
[26] R. Braatz, P. Young, J. Doyle, and M. Morari, "Computational Complexity of $\mu$ Calculation," IEEE Transactions on Automatic Control, vol. 39, no. 5, pp. 1000-1002, 1994.\\[0pt]
[27] M. J. Brauer et al., "Coordination of Growth Rate, Cell Cycle, Stress Response, and Metabolic Activity in Yeast," Molecular Biology of the Cell, vol. 19, no. 1, pp. 352-367, 2008.\\[0pt]
[28] L. J. Bridgeman and J. R. Forbes, "The Minimum Gain Lemma," International Journal of Robust and Nonlinear Control, vol. 25, no. 14, pp. 2515-2531, 2015.\\[0pt]
[29] A. L. Bruce, A. Goel, and D. S. Bernstein, "Convergence and consistency of recursive least squares with variable-rate forgetting," Automatica, vol. 119, pp. 109052, 2020.\\[0pt]
[30] M. Busch, "The Waddington Effect," Sport Aviation, 2011.\\[0pt]
[31] M. Butcher, A. Karimi, and R. Longchamp, "On the consistency of certain identification methods for linear parameter varying systems," IFAC Proceedings Volumes, vol. 41, no. 2, pp. 4018-4023, 2008.\\[0pt]
[32] R. J. Caverly, "Optimal Output Modification and Robust Control Using Minimum Gain and the Large Gain Theorem," Michigan: University of Michigan, 2018.\\[0pt]
[33] R. J. Caverly and J. R. Forbes, "Hınfty -Optimal Parallel Feedforward Control Using Minimum Gain," IEEE Control Systems Letters, vol. 2, no. 4, pp. 677-682, 2018.\\[0pt]
[34] R. J. Caverly and J. R. Forbes, "LMI Properties and Applications in Systems, Stability, and Control Theory," 2021.\\[0pt]
[35] J. Chen and R. J. Patton, Robust Model-Based Fault Diagnosis for Dynamic Systems, Boston, MA: Springer US, 1999, pages 1089-1091.\\[0pt]
[36] A. Chiuso and G. Pillonetto, "A Bayesian approach to sparse dynamic network identification," Automatica, vol. 48, no. 8, pp. 1553-1565, 2012.\\[0pt]
[37] K. Classens, W. P. M. H. Heemels, and T. Oomen, "Closed-loop Aspects in MIMO Fault Diagnosis with Application to Precision Mechatronics," in 2021 IEEE American Control Conference (ACC), New Orleans, LA, USA, 2021, pp. 1756-1761.\\[0pt]
[38] K. Classens, W. P. M. H. Heemels, and T. Oomen, "Digital Twins in Mechatronics: From Model-based Control to Predictive Maintenance," in 2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI), Beijing, China, 2021, pp. 336-339.\\[0pt]
[39] K. Classens, M. Mostard, J. van de Wijdeven, W. P. M. H. Heemels, and T. Oomen, "Fault Detection for Precision Mechatronics: Online Estimation of Mechanical Resonances," in 2nd Modeling, Estimation and Control Conference (MECC), Jersey City, NJ, USA, 2022, pp. 746-751.\\[0pt]
[40] K. Classens, J. van de Wijdeven, W. P. M. H. Heemels, and T. Oomen, "Opportunities of Digital Twins for High-tech Systems: From Fault Diagnosis and Predictive Maintenance to Control Reconfiguration," Mikroniek, vol. 63, no. 5, pp. 5-12, 2023.\\[0pt]
[41] K. Classens, S. de Rijk, J. van de Wijdeven, W. P. M. H. Heemels, and T. Oomen, "Robust $H_{-} / H_{\infty}$ Fault Detection for Closed-Loop systems with Application to a Prototype Wafer Stage," submitted for journal publication.\\[0pt]
[42] K. Classens, R. A. González, and T. Oomen, "Recursive Identification of Structured Systems: An Instrumental-Variable Approach Applied to Mechanical Systems," submitted for journal publication.\\[0pt]
[43] K. Classens, W. P. M. H. Heemels, and T. Oomen, "A Closed-Loop Perspective on Fault Detection for Precision Motion Control: With Application to an Overactuated System," in 2021 IEEE International Conference on Mechatronics (ICM), Kashiwa, Japan, 2021, pp. 1-6.\\[0pt]
[44] K. Classens, W. P. M. H. M. Heemels, and T. Oomen, "Direct Shaping of Minimum and Maximum Singular Values: An $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ Synthesis Approach for Fault Detection Filters," in Proceedings of the IFAC 22nd Triennial World Congress, Yokohama, Japan, 2023.\\[0pt]
[45] K. Classens, T. Ickenroth, K. Tiels, P. Tacx, J. van de Wijdeven, W. P. M. H. Heemels, and T. Oomen, "Robust Fault Detection with Application to a Prototype Wafer Stage," submitted for journal publication.\\[0pt]
[46] K. Classens, T. Ickenroth, J. van de Wijdeven, W. P. M. H. Heemels, and T. Oomen, "Closed-Loop Optimal Fault Detection for Uncertain Systems," submitted for journal publication.\\[0pt]
[47] K. Classens, T. Ickenroth, J. van de Wijdeven, W. P. M. H. Heemels, and T. Oomen, "Optimal Fault Detection for Closed-Loop Linear Uncertain Systems," in 63rd IEEE Conference on Decision and Control, Milan, Italy, 2024.\\[0pt]
[48] K. Classens, M. Schoukens, T. Oomen, and J. P. Noël, "Locating Nonlinearities in Mechanical Systems: A Frequency-Domain Dynamic Network Perspective," submitted for journal publication.\\[0pt]
[49] K. Classens, J. van de Wijdeven, W. P. M. H. Heemels, and T. Oomen, "Nullspace-based Fault Diagnosis for Closed-Loop Mechatronic Systems with Application to Semiconductor Equipment," submitted for journal publication.\\[0pt]
[50] A. Cooman, P. Bronders, and G. Vandersteen, "Distortion Contribution Analysis of Strongly Non-Linear Analog Circuits," in 2016 13th International Conference on Synthesis, Modeling, Analysis and Simulation Methods and Applications to Circuit Design (SMACD), Lisbon, Portugal: IEEE, 2016, pp. 1-4.\\[0pt]
[51] T. M. Cover and J. A. Thomas, Elements of Information Theory, 2nd Edition, John Wiley \& Sons, 2006.\\[0pt]
[52] P. Z. Csurcsia, B. Peeters, and J. Schoukens, "User-friendly nonlinear nonparametric estimation framework for vibro-acoustic industrial measurements with multiple inputs," Mechanical Systems and Signal Processing, vol. 145, pp. 106926, 2020.\\[0pt]
[53] R. S. da Silva de Aguiar, P. Apkarian, and D. Noll, "Structured Robust Control Against Mixed Uncertainty," IEEE Trans. Contr. Syst. Technol., vol. 26, no. 5, pp. 1771-1781, 2018.\\[0pt]
[54] M. Dahleh, M. A. Dahleh, and G. Verghese, Lectures on Dynamic Systems and Control, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, 2002.\\[0pt]
[55] A. Dakkoune, L. V. Hassimi, S. Leveneur, L. Estel, and D. Lefebvre, "Model-Based Fault Detection and Isolation for Chemical Processes: Application to the Prevention of Thermal Runaway," in 2018 IEEE Symposium Series on Computational Intelligence (SSCI), 2018 IEEE Symposium Series on Computational Intelligence (SSCI), Bangalore, India: IEEE, 2018, pp. 1352-1358.\\[0pt]
[56] A. Dankers, "System Identification in Dynamic Networks," PhD thesis, Eindhoven University of Technology, 2014.\\[0pt]
[57] A. Dankers, P. M. J. Van den Hof, X. Bombois, and P. Heuberger, "Errors-in-variables identification in dynamic networks - Consistency results for an instrumental variable approach," Automatica, vol. 62, pp. 39-50, 2015.\\[0pt]
[58] A. Dankers, P. M. J. Van den Hof, X. Bombois, and P. S. C. Heuberger, "Identification of Dynamic Models in Complex Networks With Prediction Error Methods: Predictor Input Selection," IEEE Trans. Automat. Contr., vol. 61, no. 4, pp. 937-952, 2016.\\[0pt]
[59] A. delli Carri, B. Weekes, D. Di Maio, and D. J. Ewins, "Extending Modal Testing Technology for Model Validation of Engineering Structures with Sparse Nonlinearities: A First Case Study," Mechanical Systems and Signal Processing, Recent Advances in Nonlinear System Identification vol. 84, pp. 97-115, 2017.\\[0pt]
[60] C. A. Desoer and M. Vidyasagar, Feedback Systems: Input-output Properties, Academic Press, 1975, 296 pages.\\[0pt]
[61] S. X. Ding, Data-Driven Design of Fault Diagnosis and Fault-tolerant Control Systems, Advances in Industrial Control, London: Springer London, 2014.\\[0pt]
[62] S. X. Ding, T. Jeinsch, P. M. Frank, and E. L. Ding, "A unified approach to the optimization of fault detection systems," International journal of adaptive control and signal processing, pp. 725-745, 2000.\\[0pt]
[63] S. X. Ding, Model-Based Fault Diagnosis Techniques: Design Schemes, Algorithms, and Tools, Berlin: Springer Nature, 2008, pages 1-473, 473 pages.\\[0pt]
[64] X. Ding and P. M. Frank, "Fault Detection via Factorization Approach," Systems $\mathcal{E}$ Control Letters, vol. 14, no. 5, pp. 431-436, 1990.\\[0pt]
[65] N. Dirkx, J. van de Wijdeven, and T. Oomen, "Frequency Response Function Identification for Multivariable Motion Control: Optimal Experiment Design with Element-Wise Constraints," Mechatronics, vol. 71, pp. 102440, 2020.\\[0pt]
[66] T. Dobrowiecki and J. Schoukens, "Linear approximation of weakly nonlinear MIMO systems," English, IEEE Transactions on Instrumentation and Measurement, vol. 56, no. 3, pp. 887-894, 2007.\\[0pt]
[67] T. Dobrowiecki and J. Schoukens, "Measuring a linear approximation to weakly nonlinear MIMO systems," Automatica, vol. 43, no. 10, pp. 1737-1751, 2007.\\[0pt]
[68] J. Doyle, "Analysis of Feedback Systems with Structured Uncertainties," IEE Proc. D Control Theory Appl. UK, vol. 129, no. 6, pp. 242, 1982.\\[0pt]
[69] G. E. Dullerud and F. Paganini, A Course in Robust Control Theory, redacted by J. E. Marsden, L. Sirovich, and S. S. Antman, Texts in Applied Mathematics, New York, NY: Springer New York, 2000.\\[0pt]
[70] A. Edelmayer, J. Bokor, and L. Keviczky, " $H_{\infty}$ Detection Filter Design for Linear Systems: Comparison of Two Approaches," IFAC Proceedings Volumes, vol. 29, no. 1, pp. 6359-6364, 1996.\\[0pt]
[71] A. Einstein, Relativity: The Special and General Theory, New York: H. Holt and Company, 1916.\\[0pt]
[72] A. Einstein, Time, Space, and Gravitation, The Times, 1919.\\[0pt]
[73] M. Enqvist and L. Ljung, "Linear approximations of nonlinear FIR systems for separable input processes," Automatica, vol. 41, no. 3, pp. 459473, 2005.\\[0pt]
[74] E. Evers, R. Voorhoeve, and T. Oomen, "On Frequency Response Function Identification for Advanced Motion Control," in IEEE 16th International Workshop on Advanced Motion Control, Norway, 2020, pp. 319324.\\[0pt]
[75] D. J. Ewins, B. Weekes, and A. delli Carri, "Modal Testing for Model Validation of Structures with Discrete Nonlinearities," Phil. Trans. R. Soc. A., vol. 373, no. 2051, pp. 20140410, 2015.\\[0pt]
[76] M. Fan and A. Tits, "Characterization and Efficient Computation of the Structured Singular Value," IEEE Trans. Automat. Contr., vol. 31, no. 8, pp. 734-743, 1986.\\[0pt]
[77] U. Forssell and L. Ljung, "Closed-loop identification revisited," Automatica, vol. 35, no. 7, pp. 1215-1241, 1999.\\[0pt]
[78] U. Forssell and L. Ljung, "Identification of unstable systems using output error and Box-Jenkins model structures," IEEE Transactions on Automatic Control, vol. 45, no. 1, pp. 137-141, 2000.\\[0pt]
[79] P. M. Frank and X. Ding, "Survey of robust residual generation and evaluation methods in observer-based fault detection systems," Journal of process control, vol. 7, pp. 403-424, 1997.\\[0pt]
[80] P. M. Frank, "Fault Diagnosis in Dynamic Systems Using Analytical and Knowledge-based Redundancy A Survey and Some New Results," Automatica, pp. 16, 1990.\\[0pt]
[81] E. Frisk and M. Nyberg, "A Minimal Polynomial Basis Solution to Residual Generation for Fault Diagnosis in Linear Systems," Automatica, 2001.\\[0pt]
[82] A. Fuller, Z. Fan, C. Day, and C. Barlow, "Digital Twin: Enabling Technologies, Challenges and Open Research," IEEE Access, vol. 8, 2020.\\[0pt]
[83] P. Gahinet and P. Apkarian, "A Linear Matrix Inequality Approach to Hinfty Control," Int. J. Robust Nonlinear Control, vol. 4, no. 4, pp. 421-448, 1994.\\[0pt]
[84] Z. Gao, C. Cecati, and S. X. Ding, "A Survey of Fault Diagnosis and Fault-Tolerant Techniques-Part I: Fault Diagnosis With Model-Based\\
and Signal-Based Approaches," IEEE Trans. Ind. Electron, vol. 62, no. 6, pp. 3757-3767, 2015.\\[0pt]
[85] Z. Gao, C. Cecati, and S. X. Ding, "A Survey of Fault Diagnosis and Fault-Tolerant Techniques-Part II: Fault Diagnosis With KnowledgeBased and Hybrid/Active Approaches," IEEE Trans. Ind. Electron, vol. 62, no. 6, pp. 3768-3774, 2015.\\[0pt]
[86] H. Garnier, R. R. Bitmead, and R. A. de Callafon, "Direct continuoustime model identification of high-powered light-emitting diodes from rapidly sampled thermal step response data," IFAC Proceedings Volumes, vol. 47, no. 3, pp. 6430-6435, 2014.\\[0pt]
[87] H. Garnier, M. Mensler, and A. Richard, "Continuous-time model identification from sampled data: implementation issues and performance evaluation," International Journal of Control, vol. 76, no. 13, pp. 1337-1357, 2003.\\[0pt]
[88] H. Garnier and L. Wang, Identification of Continuous-time Models from Sampled Data, Springer, 2008.\\[0pt]
[89] H. Garnier and P. C. Young, "The advantages of directly identifying continuous-time transfer function models in practical applications," International Journal of Control, vol. 87, no. 7, pp. 1319-1338, 2014.\\[0pt]
[90] H. Garnier, "Direct continuous-time approaches to system identification. Overview and benefits for practical applications," European Journal of Control, vol. 24, pp. 50-62, 2015.\\[0pt]
[91] W. K. Gawronski, Advanced Structural Dynamics and Active Control of Structures, Springer, 2004.\\[0pt]
[92] M. Geradin and D. Rixen, Mechanical Vibrations: Theory and Application to Structural Dynamics, Chichester, UK, Second Edition: John Wiley \& Sons, Ltd, 2015.\\[0pt]
[93] J. Gertler, "Diagnosing Parametric Faults: From Parameter Estimation to Parity Relations," in 1995 American Control Conference (ACC), volume 3, 1995, pp. 1615-1620.\\[0pt]
[94] J. Gertler, Fault Detection and Diagnosis in Engineering Systems, Boca Raton, 1998.\\[0pt]
[95] D. Ghosh, R. Sharman, H. Raghav Rao, and S. Upadhyaya, "Self-Healing Systems - Survey and Synthesis," Decision Support Systems, Decision Support Systems in Emerging Economies vol. 42, no. 4, pp. 2164-2185, 2007.\\[0pt]
[96] M. Gilson and H. Garnier, "Continuous-time model identification of systems operating in closed-loop," IFAC Proceedings Volumes, vol. 36, no. 16, pp. 405-410, 2003.\\[0pt]
[97] M. Gilson, H. Garnier, P. C. Young, and P. M. J. Van den Hof, "Instrumental variable methods for closed-loop continuous-time model identification," in H. Garnier and L. Wang (Eds.). Identification of Continuoustime Models from Sampled Data, Springer, 2008, pages 133-160.\\[0pt]
[98] M. Gilson, H. Garnier, P. C. Young, and P. M. J. Van den Hof, "Optimal instrumental variable method for closed-loop identification," IET Control Theory ${ }^{\mathcal{E}}$ Applications, vol. 5, no. 10, pp. 1147-1154, 2011.\\[0pt]
[99] M. Gilson and P. M. J. Van den Hof, "Instrumental variable methods for closed-loop system identification," Automatica, vol. 41, no. 2, pp. 241-249, 2005.\\[0pt]
[100] M. Gilson, J. S. Welsh, and H. Garnier, "A frequency localizing basis function-based IV method for wideband system identification," IEEE Transactions on Control Systems Technology, vol. 26, no. 1, pp. 329-335, 2017.\\[0pt]
[101] K. Glover and A. Varga, "On solving non-standard $\mathcal{H}_{-} / \mathcal{H}_{2 / \infty}$ fault detection problems," Proceedings of the IEEE Conference on Decision and Control, no. 2, pp. 891-896, 2011.\\[0pt]
[102] T. Goelles, B. Schlager, and S. Muckenhuber, "Fault Detection, Isolation, Identification and Recovery (FDIIR) Methods for Automotive Perception Sensors Including a Detailed Literature Survey for Lidar," Sensors, vol. 20, no. 13, pp. 3662, 2020.\\[0pt]
[103] L. H. Goetz and N. J. Schork, "Personalized Medicine: Motivation, Challenges and Progress," Fertil Steril, vol. 109, no. 6, pp. 952-963, 2018.\\[0pt]
[104] J. Goncalves and S. Warnick, "Necessary and Sufficient Conditions for Dynamical Structure Reconstruction of LTI Networks," IEEE Trans. Automat. Contr., vol. 53, no. 7, pp. 1670-1674, 2008.\\[0pt]
[105] R. A. González, K. Classens, C. R. Rojas, J. S. Welsh, and T. Oomen, "Statistical Analysis of Block Coordinate Descent Algorithms for Linear Continuous-time System Identification," IEEE Control Systems Letters, 2024.\\[0pt]
[106] R. A. González, S. Pan, C. R. Rojas, and J. S. Welsh, "Consistency analysis of refined instrumental variable methods for continuous-time system identification in closed-loop," Automatica, 2024.\\[0pt]
[107] R. A. González, C. R. Rojas, S. Pan, and J. S. Welsh, "On the Relation between Discrete and Continuous-time Refined Instrumental Vari-\\
able Methods," IEEE Control Systems Letters, vol. 7, pp. 2233-2238, 2023.\\[0pt]
[108] R. A. González, C. R. Rojas, S. Pan, and J. S. Welsh, "Parsimonious Identification of Continuous-Time Systems: A Block-Coordinate Descent Approach," in 22nd IFAC World Congress, Yokohama, Japan, 2023.\\[0pt]
[109] R. A. González, C. R. Rojas, S. Pan, and J. S. Welsh, "Refined instrumental variable methods for unstable continuous-time systems in closedloop," International Journal of Control, pp. 1-15, 2022.\\[0pt]
[110] R. A. González, C. R. Rojas, S. Pan, and J. S. Welsh, "The SRIVC algorithm for continuous-time system identification with arbitrary input excitation in open and closed loop," in Proceedings of the 60th IEEE Conference on Decision and Control, 2021, pp. 3004-3009.\\[0pt]
[111] R. A. González, C. R. Rojas, and J. S. Welsh, "An asymptotically optimal indirect approach to continuous-time system identification," in 57 th IEEE Conference on Decision and Control (CDC), 2018, pp. 638-643.\\[0pt]
[112] R. A. González, K. Classens, C. R. Rojas, J. S. Welsh, and T. Oomen, "Identification of Additive Continuous-time Systems in Open and Closed Loop," submitted for journal publication.\\[0pt]
[113] N. C. Grassly and C. Fraser, "Mathematical Models of Infectious Disease Transmission," Nat Rev Microbiol, vol. 6, no. 6, pp. 477-487, 2008.\\[0pt]
[114] P. Green, K. Worden, K. Atallah, and N. Sims, "The Benefits of Duffingtype Nonlinearities and Electrical Optimisation of a Mono-Stable Energy Harvester under White Gaussian Excitations," Journal of Sound and Vibration, vol. 331, no. 20, pp. 4504-4517, 2012.\\[0pt]
[115] F. Gustafsson, Adaptive Filtering and Change Detection: Gustafsson: Adaptive, Chichester, UK: John Wiley \& Sons, Ltd, 2001.\\[0pt]
[116] H. Ha and J. S. Welsh, "Ensuring stability in continuous time system identification instrumental variable method for over-parameterized models," in 53rd IEEE Conference on Decision and Control (CDC), 2014, pp. 2597-2602.\\[0pt]
[117] F. H. Hahn and R. C. O. Matthews, "The Theory of Economic Growth: A Survey," The Economic Journal, vol. 74, no. 296, pp. 779-902, 1964.\\[0pt]
[118] D. P. Hampshire, "A Derivation of Maxwell's Equations Using the Heaviside Notation," Philos Trans A Math Phys Eng Sci, vol. 376, no. 2134, pp. 20170447, 2018.\\[0pt]
[119] W. Härdle, S. Huet, E. Mammen, and S. Sperlich, "Bootstrap inference in semiparametric generalized additive models," Econometric Theory, vol. 20, no. 2, pp. 265-300, 2004.\\[0pt]
[120] A. Hassibi, J. How, and S. Boyd, "A Path-Following Method for Solving BMI Problems in Control," in Proceedings of the 1999 American Control Conference, Proceedings of the 1999 American Control Conference, San Diego, CA, USA: IEEE, 1999, pp. 1385-1389.\\[0pt]
[121] T. Hastie and R. Tibshirani, "Generalized additive models," Statistical Science, vol. 1, no. 3, pp. 297-310, 1986.\\[0pt]
[122] Z. Hausfather, H. F. Drake, T. Abbott, and G. A. Schmidt, "Evaluating the Performance of Past Climate Model Projections," Geophysical Research Letters, vol. 47, no. 1, pp. e2019GL085378, 2020.\\[0pt]
[123] O. Heaviside, Electromagnetic Theory, American Mathematical Soc., 2003, 682 pages.\\[0pt]
[124] D. Henry, A. Zolghadri, M. Monsion, and S. Ygorra, "Offline Robust Fault Diagnosis Using the Generalized Structured Singular Value," pp. 12, 2002.\\[0pt]
[125] D. Henry, "Theories for Design and Analysis of Robust $H_{\infty} / H_{-}$Fault Detectors," J. Franklin Inst., vol. 358, no. 1, pp. 1152-1183, 2021.\\[0pt]
[126] D. Henry and A. Zolghadri, "Design and Analysis of Robust Residual Generators for Systems under Feedback Control," Automatica, vol. 41, no. 2, pp. 251-264, 2005.\\[0pt]
[127] D. Henry and A. Zolghadri, "Design of Fault Diagnosis Filters: A MultiObjective Approach," Journal of the Franklin Institute, vol. 342, no. 4, pp. 421-446, 2005.\\[0pt]
[128] D. Henry, A. Zolghadri, F. Castang, and M. Monsion, "A New MultiObjective Filter Design for Guaranteed Robust FDI Performance," in Proceedings of the 40th IEEE Conference on Decision and Control, volume 1, Orlando, FL, USA, 2001, pp. 173-178.\\[0pt]
[129] R. Holland, P. Young, and C. Zhu, "Development of a Skew $\mu$ Lower Bound," International Journal of Robust and Nonlinear Control, vol. 15, no. 11, pp. 495-506, 2005.\\[0pt]
[130] R. Holland, P. Young, and C. Zhu, "Development of a Skew $\mu$ Upper Bound," International Journal of Robust and Nonlinear Control, vol. 15, no. 18, pp. 905-921, 2005.\\[0pt]
[131] F. Hoots, R. Glover, and P. Jr, "History of Analytical Orbit Modeling in the U. S. Space Surveillance System," Journal of Guidance Control and Dynamics, vol. 27, pp. 174-185, 2004.\\[0pt]
[132] R. A. Horn and C. R. Johnson, Topics in Matrix Analysis, Cambridge University Press, 1994, 620 pages.\\[0pt]
[133] R. A. Horn and C. R. Johnson, Matrix Analysis, 2nd Edition, Cambridge University Press, 2012.\\[0pt]
[134] M. Hou and R. J. Patton, "An LMI Approach to $H_{-} / H_{\infty}$ Fault Detection Observers," in UKACC International Conference on Control, 1996, pp. 305-310.\\[0pt]
[135] E. Huselstein and H. Garnier, "An approach to continuous-time model identification from non-uniformly sampled data," in Proceedings of the 41st IEEE Conference on Decision and Control, 2002, pp. 622-623.\\[0pt]
[136] I. Hwang, S. Kim, Y. Kim, and C. E. Seah, "A Survey of Fault Detection, Isolation, and Reconfiguration Methods," IEEE Trans. Control Syst. Technol., vol. 18, no. 3, pp. 636-653, 2010.\\[0pt]
[137] R. Isermann, Fault-Diagnosis Systems: An Introduction from Fault Detection to Fault Tolerance, Berlin; New York: Springer, 2006, 475 pages.\\[0pt]
[138] R. Isermann, "Supervision, Fault-Detection and Fault-Diagnosis Methods - An Introduction," Control Engineering Practice, vol. 5, no. 5, pp. 639-652, 1997.\\[0pt]
[139] R. Isermann, Fault-Diagnosis Applications: Model-Based Condition Monitoring: Actuators, Drives, Machinery, Plants, Sensors, and Faulttolerant Systems, Berlin, Heidelberg: Springer Berlin Heidelberg, 2011.\\[0pt]
[140] R. Isermann, "Model-Based Fault-Detection and Diagnosis - Status and Applications," Annu. Rev. Control, vol. 29, no. 1, pp. 71-85, 2005.\\[0pt]
[141] A. Isidori, Nonlinear Control Systems, Third edition, softcover reprint of the hardcover 3rd edition 2000, Communications and Control Engineering, London: Springer, 2013, 549 pages.\\[0pt]
[142] I. M. Jaimoukha, Z. Li, and V. Papakos, "A Matrix Factorization Solution to the $H_{-} / H_{\infty}$ Fault Detection Problem," Automatica, vol. 42, no. 11, 2006.\\[0pt]
[143] K. Janschek, Mechatronic Systems Design: Methods, Models, Concepts, Berlin, Heidelberg: Springer Berlin Heidelberg, 2012.\\[0pt]
[144] A. Josefsson, M. Magnevall, K. Ahlin, and G. Broman, "Spatial Location Identification of Structural Nonlinearities from Random Data," Mechanical Systems and Signal Processing, vol. 27, pp. 410-418, 2012.\\[0pt]
[145] E. Katsoulakis et al., "Digital Twins for Health: A Scoping Review," npj Digital Medicine, vol. 7, no. 1, pp. 1-11, 2024.\\[0pt]
[146] G. Kerschen, K. Worden, A. F. Vakakis, and J. Golinval, "Past, Present and Future of Nonlinear System Identification in Structural Dynamics," Mechanical Systems and Signal Processing, vol. 20, no. 3, pp. 505-592, 2006.\\[0pt]
[147] M. Khosrowjerdi, R. Nikoukhah, and N. Safari-Shad, "A Mixed $H_{2} / H_{\infty}$ Approach to Simultaneous Fault Detection and Control," Automatica, vol. 40, no. 2, pp. 261-267, 2004.\\[0pt]
[148] H. Kimura, "Geometric Structure of Observers for Linear Feedback Control Laws," IEEE Transactions on Automatic Control, vol. 22, no. 5, pp. 846-855, 1977.\\[0pt]
[149] M. Kinnaert, "Fault Diagnosis Based on Analytical Models for Linear and Nonlinear Systems - a Tutorial," IFAC Proceedings Volumes, 5th IFAC Symposium on Fault Detection, Supervision and Safety of Technical Processes 2003, Washington DC, 9-11 June 1997 vol. 36, no. 5, pp. 3750, 2003.\\[0pt]
[150] E. Kivits, "Modelling and identification of physical linear networks," PhD thesis, Eindhoven University of Technology, 2024.\\[0pt]
[151] G. Kosova, E. Di Lorenzo, B. Peeters, and G. Kerschen, "Locating Structural Nonlinearities Using Linear Frequency Response Functions and Nonlinear Orthogonal Projections," Mechanical Systems and Signal Processing, vol. 202, pp. 110585, 2023.\\[0pt]
[152] R. Lee, "The Outlook for Population Growth," Science, vol. 333, no. 6042, pp. 569-573, 2011.\\[0pt]
[153] Y. Lei, B. Yang, X. Jiang, F. Jia, N. Li, and A. K. Nandi, "Applications of Machine Learning to Machine Fault Diagnosis: A Review and Roadmap," Mech. Syst. Signal Process., vol. 138, pp. 106587, 2020.\\[0pt]
[154] J. Li, K. Zhou, and Z. Ren, "Robust fault diagnosis for linear time invariant uncertain systems," Proceedings of the World Congress on Intelligent Control and Automation (WCICA), pp. 1170-1173, 2008.\\[0pt]
[155] LIGO Scientific Collaboration and Virgo Collaboration et al., "Observation of Gravitational Waves from a Binary Black Hole Merger," Phys. Rev. Lett., vol. 116, no. 6, pp. 061102, 2016.\\[0pt]
[156] J. Liu, J. L. Wang, and G. Yang, "An LMI Approach to Minimum Sensitivity Analysis with Application to Fault Detection," Automatica, vol. 41, no. 11, pp. 1995-2004, 2005.\\[0pt]
[157] N. Liu and K. Zhou, "Optimal Solutions to Multi-Objective Robust Fault Detection Problems," in 2007 46th IEEE Conference on Decision and Control, 2007 46th IEEE Conference on Decision and Control, 2007, pp. 981-988.\\[0pt]
[158] L. Ljung and R. Singh, "Version 8 of the MATLAB system identification toolbox," IFAC Proceedings Volumes, vol. 45, no. 16, pp. 1826-1831, 2012.\\[0pt]
[159] L. Ljung and T. Söderström, Theory and Practice of Recursive Identification, MIT Press, 1983.\\[0pt]
[160] L. Ljung, "Experiments with identification of continuous time models," in 15th IFAC Symposium on System Identification, Saint Malo, France, volume 42, 10, Elsevier, 2009, pp. 1175-1180.\\[0pt]
[161] L. Ljung, System Identification: Theory for the User, 2nd Edition, Prentice-Hall, 1987.\\[0pt]
[162] J. Löfberg, YALMIP: A Toolbox for Modeling and Optimization in MAT$L A B$, Taiwan, 2004.\\[0pt]
[163] D. G. Luenberger and Y. Ye, Linear and Nonlinear Programming, 3rd Edition, Springer, 2008.\\[0pt]
[164] J. Lunze and J. Richter, "Reconfigurable Fault-tolerant Control: A Tutorial Introduction," European Journal of Control, vol. 14, no. 5, pp. 359-386, 2008.\\[0pt]
[165] R. Mangoubi, B. Appleby, and J. Farrell, "Robust Estimation in Fault Detection," in 1992 Proceedings of the 31st IEEE Conference on Decision and Control, Tucson, AZ, USA: IEEE, 1992, pp. 2317-2322.\\[0pt]
[166] R. Mangoubi, "Robust Estimation and Failure Detection for Linear Systems," Massachusetts Institute of Technology, 1995.\\[0pt]
[167] R. S. Mangoubi, Robust Estimation and Failure Detection, redacted by M. J. Grimble and M. A. Johnson, Advances in Industrial Control, London: Springer London, 1998.\\[0pt]
[168] S. Marchesiello and L. Garibaldi, "A time domain approach for identifying nonlinear vibrating structures by subspace methods," Mechanical Systems and Signal Processing, vol. 22, pp. 81-101, 2008.\\[0pt]
[169] G. Marion et al., "Modelling: Understanding Pandemics and How to Control Them," Epidemics, vol. 39, pp. 100588, 2022.\\[0pt]
[170] J. Marzat, H. Piet-Lahanier, F. Damongeot, and E. Walter, "Model-Based Fault Diagnosis for Aerospace Systems: A Survey," Proceedings of the In-\\
stitution of Mechanical Engineers, Part G: Journal of Aerospace Engineering, vol. 226, no. 10, pp. 1329-1360, 2012.\\[0pt]
[171] D. Materassi and G. Innocenti, "Topological Identification in Networks of Dynamical Systems," IEEE Trans. Automat. Contr., vol. 55, no. 8, pp. 1860-1871, 2010.\\[0pt]
[172] S. Mathur and J. Sutton, "Personalized Medicine Could Transform Healthcare," Biomedical Reports, vol. 7, no. 1, pp. 3-5, 2017.\\[0pt]
[173] J. C. Maxwell, A Treatise on Electricity and Magnetism, Clarendon Press, 1873, 616 pages.\\[0pt]
[174] N. Mooren, G. Witvoet, and T. Oomen, "On-line instrumental variablebased feedforward tuning for non-resetting motion tasks," International Journal of Robust and Nonlinear Control, pp. 1-19, 2023.\\[0pt]
[175] J. Moyne, Y. Qamsane, E. C. Balta, I. Kovalenko, J. Faris, K. Barton, and D. M. Tilbury, "A Requirements Driven Digital Twin Framework: Specification and Opportunities," IEEE Access, vol. 8, 2020.\\[0pt]
[176] C. L. Navier, Sur Les Lois Des Mouvements Des Fluides, Paris: Mémoires de L'Académie Royale des Sciences de L'Institut de France, 1827, 389-440.\\[0pt]
[177] C. L. Navier, Sur Les Lois Des Mouvements Des Fluides, an Ayant Égard à l'adhésion Des Molecules, Paris: Annales de Chimie et de Physique, 1821, 244-260.\\[0pt]
[178] M. P. Newlin and R. S. Smith, "A Generalization of the Structured Singular Value and Its Application to Model Validation," IEEE Transactions on Automatic Control, vol. 43, no. 7, pp. 901-907, 1998.\\[0pt]
[179] I. Newton, N. W. L. o. S. I. N. Chittenden, D. Adee, A. Motte, and T. P. E. A. m. b. C.-B. Hill, Newton's Principia: The Mathematical Principles of Natural Philosophy, New-York : Published by Daniel Adee, 1846, 600 pages.\\[0pt]
[180] H. Nijmeijer and A. van der Schaft, Nonlinear Dynamical Control Systems, New York, NY: Springer New York, 1990.\\[0pt]
[181] J. P. Noël, L. Renson, and G. Kerschen, "Complex Dynamics of a Nonlinear Aerospace Structure: Experimental Identification and Modal Interactions," Journal of Sound and Vibration, vol. 333, no. 12, pp. 2588-2607, 2014.\\[0pt]
[182] J. Noël and G. Kerschen, "Frequency-domain subspace identification for nonlinear mechanical systems," Mechanical Systems and Signal Processing, vol. 40, pp. 701-717, 2013.\\[0pt]
[183] J. Noël and G. Kerschen, "Nonlinear system identification in structural dynamics: 10 more years of progress," Mechanical Systems and Signal Processing, vol. 83, pp. 2-35, 2017.\\[0pt]
[184] T. Oomen, "Advanced Motion Control for Precision Mechatronics: Control, Identification, and Learning of Complex Systems," IEEJ Journal IA, vol. 7, no. 2, pp. 127-140, 2018.\\[0pt]
[185] J. D. Opsomer and D. Ruppert, "A root-n consistent backfitting estimator for semiparametric additive modeling," Journal of Computational and Graphical Statistics, vol. 8, no. 4, pp. 715-732, 1999.\\[0pt]
[186] N. S. Ottosen and H. Petersson, Introduction to the Finite Element Method, Prentice-Hall, 1992.\\[0pt]
[187] A. Packard, G. Balas, R. Liu, and J. Shin, "Results on Worst-Case Performance Assessment," in Proceedings of the 2000 American Control Conference. ACC, Proceedings of the 2000 American Control Conference. ACC, 2000, pp. 2425-2427.\\[0pt]
[188] A. Packard and J. Doyle, "The Complex Structured Singular Value," Automatica, vol. 29, no. 1, pp. 71-109, 1993.\\[0pt]
[189] A. Padilla, H. Garnier, P. C. Young, F. Chen, and J. Yuz, "Identification of continuous-time models with slowly time-varying parameters," Control Engineering Practice, vol. 93, pp. 104165, 2019.\\[0pt]
[190] A. Padilla, H. Garnier, P. C. Young, and J. Yuz, "Recursive online IV method for identification of continuous-time slowly time-varying models in closed loop," IFAC-PapersOnLine, vol. 50, no. 1, pp. 4008-4013, 2017.\\[0pt]
[191] S. Pan, R. A. González, J. S. Welsh, and C. R. Rojas, "Consistency analysis of the Simplified Refined Instrumental Variable method for Continuous-time systems," Automatica, vol. 113, 2020.\\[0pt]
[192] S. Pan, J. S. Welsh, R. A. González, and C. R. Rojas, "Efficiency analysis of the simplified refined instrumental variable method for continuous-time systems," Automatica, vol. 121, Article 109196, 2020.\\[0pt]
[193] S. Pan, J. S. Welsh, and M. Fu, "Identification of Continuous-time Linear Time-varying Systems with Abrupt Changes in Parameters," IFACPapersOnLine, vol. 54, no. 7, pp. 339-344, 2021.\\[0pt]
[194] Y.-J. Park, S.-K. S. Fan, and C.-Y. Hsu, "A Review on Fault Detection and Process Diagnostics in Industrial Processes," Processes, vol. 8, no. 9, pp. 1123, 2020.\\[0pt]
[195] B. Patartics, P. Seiler, B. Takarics, and B. Vanek, "Worst Case Uncertainty Construction via Multifrequency Gain Maximization With Ap-\\
plication to Flutter Control," IEEE Trans. Contr. Syst. Technol., vol. 31, no. 1, pp. 155-165, 2023.\\[0pt]
[196] B. Patartics, P. Seiler, and B. Vanek, "Construction of an Uncertainty to Maximize the Gain at Multiple Frequencies," in 2020 American Control Conference (ACC), 2020 American Control Conference (ACC), Denver, CO, USA, 2020, pp. 2643-2648.\\[0pt]
[197] R. Patton, R. Clark, and P. M. Frank, Issues of Fault Diagnosis for Dynamic Systems, 2010.\\[0pt]
[198] R. Pintelon, P. Guillaume, Y. Rolain, J. Schoukens, and H. Van Hamme, "Parametric Identification of Transfer Functions in the Frequency Domain-a Survey," IEEE Transactions on Automatic Control, vol. 39, no. 11, 1994.\\[0pt]
[199] R. Pintelon and J. Schoukens, "FRF Measurement of Nonlinear Systems Operating in Closed Loop," IEEE Transactions on Instrumentation and Measurement, vol. 62, no. 5, pp. 1334-1345, 2013.\\[0pt]
[200] R. Pintelon and J. Schoukens, "Measurement and modelling of linear systems in the presence of non-linear distortions," Mechanical Systems and Signal Processing, vol. 16, no. 5, pp. 785-801, 2002.\\[0pt]
[201] R. Pintelon and J. Schoukens, "The best linear approximation of nonlinear systems operating in feedback," in, 2012, pp. 2092-2097.\\[0pt]
[202] R. Pintelon and J. Schoukens, System Identification: A Frequency Domain Approach, 2nd, Hoboken, New Jersey: John Wiley \& Sons, 2012.\\[0pt]
[203] J. Ploeg, B. T. M. Scheepers, E. Van Nunen, N. Van de Wouw, and H. Nijmeijer, "Design and experimental evaluation of cooperative adaptive cruise control," in 14th International IEEE Conference on Intelligent Transportation Systems (ITSC), IEEE, 2011, pp. 260-265.\\[0pt]
[204] A. Preumont, Vibration Control of Active Structures: An Introduction, 3rd Edition, Springer, 2018.\\[0pt]
[205] Y. Qamsane et al., "A Unified Digital Twin Framework for Real-time Monitoring and Evaluation of Smart Manufacturing Systems," in 2019 IEEE 15th International Conference on Automation Science and Engineering (CASE), 2019.\\[0pt]
[206] D. D. Quinn, A. L. Triplett, A. F. Vakakis, and L. A. Bergman, "Energy Harvesting From Impulsive Loads Using Intentional Essential Nonlinearities," Journal of Vibration and Acoustics, vol. 133, no. 1, pp. 011004, 2011.\\[0pt]
[207] M. L. Rank and H. Niemann, "Norm Based Design of Fault Detectors," International Journal of Control, vol. 72, no. 9, pp. 773-783, 1999.\\[0pt]
[208] G. P. Rao and H. Unbehauen, "Identification of continuous-time systems," IEE Proceedings-Control Theory and Applications, vol. 153, no. 2, pp. 185-220, 2006.\\[0pt]
[209] A. Rasheed, O. San, and T. Kvamsdal, "Digital Twin: Values, Challenges and Enablers," ArXiv, 2019.\\[0pt]
[210] L. Renson, A. Gonzalez-Buelga, D. Barton, and S. Neild, "Robust identification of backbone curves using control-based continuation," Journal of Sound and Vibration, vol. 367, pp. 145-158, 2016.\\[0pt]
[211] E. Reynders, "System identification methods for (operational) modal analysis: review and comparison," Archives of Computational Methods in Engineering, vol. 19, pp. 51-124, 2012.\\[0pt]
[212] C. Roos, "Systems Modeling, Analysis and Control (SMAC) Toolbox: An Insight into the Robustness Analysis Library," in 2013 IEEE Conference on Computer Aided Control System Design (CACSD), Hyderabad, India: IEEE, 2013, pp. 176-181.\\[0pt]
[213] M. Ruderman, T. Bertram, and M. Iwasaki, "Modeling, observation, and control of hysteresis torsion in elastic robot joints," Mechatronics, vol. 24, no. 5, pp. 407-415, 2014.\\[0pt]
[214] W. J. Rugh, Linear System Theory, Prentice-Hall, 1996.\\[0pt]
[215] A. Saberi, A. A. Stoorvogel, P. Sannuti, and H. Niemann, "Fundamental Problems in Fault Detection and Identification," Int. J. Robust Nonlinear Control, pp. 28, 2000.\\[0pt]
[216] M. A. Sadrnia, R. J. Patton, and J. Chen, "Robust $H_{-} / \mu$ Fault Diagnosis Observer Design," in 1997 European Control Conference (ECC), 1997 European Control Conference (ECC), 1997, pp. 1502-1507.\\[0pt]
[217] K. Safarzynska and J. van den Bergh, "Evolutionary Models in Economics: A Survey of Methods and Building Blocks," Journal of Evolutionary Economics, vol. 20, pp. 329-373, 2010.\\[0pt]
[218] D. Saha and G. P. Rao, "A general algorithm for parameter identification in lumped continuous systems-the Poisson moment functional approach," IEEE Transactions on Automatic Control, vol. 27, no. 1, pp. 223-225, 1982.\\[0pt]
[219] C. Scherer, P. Gahinet, and M. Chilali, "Multiobjective Output-Feedback Control via LMI Optimization," IEEE Transactions on Automatic Control, vol. 42, no. 7, pp. 896-911, 1997.\\[0pt]
[220] C. Scherer and S. Weiland, Linear Matrix Inequalities in Control, 2020.\\[0pt]
[221] J. Schewe et al., "State-of-the-Art Global Models Underestimate Impacts from Climate Extremes," Nat Commun, vol. 10, no. 1, pp. 1005, 2019.\\[0pt]
[222] J. Schoukens, T. Dobrowiecki, and R. Pintelon, "Parametric and nonparametric identification of linear systems in the presence of nonlinear distortions. A frequency domain approach," IEEE Transactions on Automatic Control, vol. 43, no. 2, pp. 176-190, 1998.\\[0pt]
[223] J. Schoukens, J. Lataire, R. Pintelon, G. Vandersteen, and T. Dobrowiecki, "Robustness Issues of the Best Linear Approximation of a Nonlinear System," IEEE Transactions on Instrumentation and Measurement, vol. 58, no. 5, pp. 1737-1745, 2009.\\[0pt]
[224] J. Schoukens, M. Vaes, and R. Pintelon, "Linear system identification in a nonlinear setting: Nonparametric analysis of the nonlinear distortions and their impact on the best linear approximation," IEEE Control Systems Magazine, vol. 36(3), pp. 38-69, 2016.\\[0pt]
[225] J. Schoukens et al., "Structure discrimination in block-oriented models using linear approximations: A theoretic framework," Automatica, vol. 53, pp. 225-234, 2015.\\[0pt]
[226] J. Schoukens, G. Vandersteen, Y. Rolain, and R. Pintelon, "Frequency Response Function Measurements Using Concatenated Subrecords With Arbitrary Length," IEEE Transactions on Instrumentation and Measurement, vol. 61, no. 10, 2012.\\[0pt]
[227] M. Schoukens, R. Pintelon, T. P. Dobrowiecki, and J. Schoukens, "Extending the Best Linear Approximation Framework to the Process Noise Case," IEEE Transactions on Automatic Control, vol. 65, no. 4, pp. 1514-1524, 2020.\\[0pt]
[228] M. Schoukens and K. Tiels, "Identification of block-oriented nonlinear systems starting from linear approximations: A survey," Automatica, vol. 85, pp. 272-292, 2017.\\[0pt]
[229] M. Schoukens and P. M. J. Van den Hof, "Detecting nonlinear modules in a dynamic network: A step-by-step procedure," IFAC-PapersOnLine, vol. 51(15), pp. 593-597, 2018.\\[0pt]
[230] E. Seedhouse, "Next-Generation Spacecraft," in XCOR, Developing the Next Generation Spaceplane, Cham: Springer International Publishing, 2016, pages 53-73.\\[0pt]
[231] R. M. Sibly and J. Hone, "Population Growth Rate and Its Determinants: An Overview.," Philos Trans R Soc Lond B Biol Sci, vol. 357, no. 1425, pp. 1153-1170, 2002.\\[0pt]
[232] S. Simani, C. Fantuzzi, and R. J. Patton, Model-Based Fault Diagnosis in Dynamic Systems Using Identification Techniques, redacted by M. J. Grimble and M. A. Johnson, Advances in Industrial Control, London: Springer London, 2003.\\[0pt]
[233] S. Skogestad and I. Postlethwaite, Multivariable Feedback Control Analysis and Design, Second Edition, John Wiley \& Sons, 2005.\\[0pt]
[234] T. Söderström and P. Stoica, Instrumental Variable Methods for System Identification, Springer, 1983.\\[0pt]
[235] T. Söderström, "Ergodicity results for sample covariances," Problems of Control and Information Theory, vol. 4, no. 2, pp. 131-138, 1975.\\[0pt]
[236] T. Söderström and P. Stoica, System Identification, Prentice-Hall, 2001.\\[0pt]
[237] T. Söderström, P. Stoica, and E. Trulsson, "Instrumental variable methods for closed loop systems," IFAC Proceedings Volumes, vol. 20, no. 5, pp. 363-368, 1987.\\[0pt]
[238] T. Söderström, P. Stoica, and B. Friedlander, "An Indirect Prediction Error Method for System Identification," Automatica, vol. 27, no. 1, pp. 183-188, 1991.\\[0pt]
[239] P. Stoica and T. Söderström, "On the parsimony principle," International Journal of Control, vol. 36, no. 3, pp. 409-418, 1982.\\[0pt]
[240] P. Stoica and T. Söderström, "Optimal instrumental variable estimation and approximate implementations," IEEE Transactions on Automatic Control, vol. 28, no. 7, pp. 757-772, 1983.\\[0pt]
[241] A. Stoorvogel, H. Niemann, A. Saberi, and P. Sannuti, "Optimal Fault Signal Estimation," International Journal of Robust and Nonlinear Control, vol. 12, no. 8, pp. 697-727, 2002.\\[0pt]
[242] J. Stoustrup and H. H. Niemann, "Fault Estimation - a Standard Problem Approach," Int. J. Robust Nonlinear Control, vol. 12, no. 8, pp. 649673, 2002.\\[0pt]
[243] J. Stoustrup, M. Grimble, and H. Niemann, "Design of Integrated Systems for the Control and Detection of Actuator/Sensor Faults," Sensor Review, vol. 17, no. 2, pp. 138-149, 1997.\\[0pt]
[244] B. S. Strachan, S. W. Shaw, and O. Kogan, "Subharmonic Resonance Cascades in a Class of Coupled Resonators," Journal of Computational and Nonlinear Dynamics, vol. 8, no. 4, pp. 041015, 2013.\\[0pt]
[245] J. Swevers, F. Al-Bender, C. Ganseman, and T. Prajogo, "An integrated friction model structure with improved presliding behavior for accurate\\
friction compensation," IEEE Transactions on Automatic Control, vol. 45, no. 4, pp. 675-686, 2000.\\[0pt]
[246] P. Tacx and T. Oomen, "Comparing Multivariable Uncertain Model Structures for Data-Driven Robust Control: Visualization and Application to a Continuously Variable Transmission," International Journal of Robust and Nonlinear Control, vol. 33, no. 16, pp. 9636-9664, 2023.\\[0pt]
[247] P. Tacx and T. Oomen, "A One-step Approach for Centralized Overactuated Motion Control of a Prototype Reticle Stage," IFAC-PapersOnLine, vol. 55, no. 37, pp. 308-313, 2022.\\[0pt]
[248] F. Tao and Q. Zhao, "Fault Detection Observer Design with Unknown Inputs," in Proceedings of 2005 IEEE Conference on Control Applications, 2005 IEEE Conference on Control Applications, 2005. CCA 2005. Toronto, Canada, 2005, pp. 1275-1280.\\[0pt]
[249] M. Thirumarimurugan, N. Bagyalakshmi, and P. Paarkavi, "Comparison of Fault Detection and Isolation Methods: A Review," in 2016 10th International Conference on Intelligent Systems and Control (ISCO), 2016, pp. 1-6.\\[0pt]
[250] R. Tóth, Modeling and Identification of Linear Parameter-Varying Systems, Springer, 2010.\\[0pt]
[251] L. Travé-Massuyès, "Bridging Control and Artificial Intelligence Theories for Diagnosis: A Survey," Engineering Applications of Artificial Intelligence, vol. 27, pp. 1-16, 2014.\\[0pt]
[252] M. Vaes et al., "Nonlinear ground vibration identification of an F-16 aicraft - Part 1: Fast nonparametric analysis of distortions in FRF measurements," in Proceedings of the International Forum on Aeroelasticity and Structural Dynamics (IFASD), Saint Petersburg, Russia, 2015.\\[0pt]
[253] A. F. Vakakis, O. V. Gendelman, L. A. Bergman, D. M. McFarland, G. Kerschen, and Y. S. Lee, Nonlinear Targeted Energy Transfer in Mechanical and Structural Systems, Springer Science \& Business Media, 2008, 1034 pages.\\[0pt]
[254] R. van Herpen, T. Oomen, E. Kikken, M. van de Wal, W. Aangenent, and M. Steinbuch, "Exploiting Additional Actuators and Sensors for NanoPositioning Robust Motion Control," Mechatronics, vol. 24, no. 6, pp. 619-631, 2014.\\[0pt]
[255] R. van Herpen, T. Oomen, and M. Steinbuch, "Optimally Conditioned Instrumental Variable Approach for Frequency-Domain System Identification," Automatica, vol. 50, no. 9, 2014.\\[0pt]
[256] P. M. J. Van den Hof, "Closed-Loop Issues in System Identification," Annual Reviews in Control, vol. 22, pp. 173-186, 1998.\\[0pt]
[257] P. M. J. Van den Hof, A. Dankers, P. Heuberger, and X. Bombois, "Identification of dynamic models in complex networks with prediction error methods - Basic methods for consistent module estimates," Automatica, vol. 49, no. 10, pp. 2994-3006, 2013.\\[0pt]
[258] P. M. J. Van den Hof, A. G. Dankers, and H. H. Weerts, "Identification in Dynamic Networks," Computers 63 Chemical Engineering, vol. 109, pp. 23-29, 2018.\\[0pt]
[259] A. Varga, "Reliable Algorithms for Computing Minimal Dynamic Covers," in 42nd IEEE International Conference on Decision and Control, 2003, pp. 1873-1878.\\[0pt]
[260] A. Varga, "On Computing Achievable Fault Signatures," IFAC Proceedings Volumes, vol. 42, no. 8, pp. 935-940, 2009.\\[0pt]
[261] A. Varga, "On Computing Nullspace Bases - a Fault Detection Perspective," IFAC Proceedings Volumes, vol. 41, no. 2, pp. 6295-6300, 2008.\\[0pt]
[262] A. Varga, "Fault Detection and Isolation Tools (FDITOOLS) User's Guide," 2017.\\[0pt]
[263] A. Varga, "New Computational Paradigms in Solving Fault Detection and Isolation Problems," Annu. Rev. Control, vol. 37, no. 1, 2013.\\[0pt]
[264] A. Varga, Solving Fault Diagnosis Problems, Springer Nature, 2017.\\[0pt]
[265] V. Venkatasubramanian, R. Rengaswamy, and S. N. Ka, "A Review of Process Fault Detection and Diagnosis Part II: Qualitative Models and Search Strategies," Computers and Chemical Engineering, pp. 14, 2003.\\[0pt]
[266] V. Venkatasubramanian, R. Rengaswamy, S. N. Ka, and K. Yin, "A Review of Process Fault Detection and Diagnosis Part III: Process History Based Methods," Computers and Chemical Engineering, pp. 20, 2003.\\[0pt]
[267] V. Venkatasubramanian, R. Rengaswamy, K. Yin, and S. N. Ka, "A Review of Process Fault Detection and Diagnosis Part I: Quantitative Model-Based Methods," Computers and Chemical Engineering, pp. 19, 2003.\\[0pt]
[268] R. Voorhoeve, R. de Rozario, W. Aangenent, and T. Oomen, "Identifying position-dependent mechanical systems: A modal approach applied to a flexible wafer stage," IEEE Transactions on Control Systems Technology, vol. 29, no. 1, pp. 194-206, 2020.\\[0pt]
[269] H. Wang, J. Wang, J. Liu, and J. Lam, "Iterative LMI Approach for Robust Fault Detection Observer Design," in 42nd IEEE International Conference on Decision and Control, Maui, Hawaii, USA, 2003, pp. 19741979.\\[0pt]
[270] J. L. Wang, G. Yang, and J. Liu, "An LMI Approach to $H_{-}$Index and Mixed $H_{-} / H_{\infty}$ Fault Detection Observer Design," Automatica, vol. 43, no. 9, pp. 1656-1665, 2007.\\[0pt]
[271] P. A. Weber, S. Merola, A. Wasielewski, and G. H. Ballantyne, "Telerobotic-Assisted Laparoscopic Right and Sigmoid Colectomies for Benign Disease," Dis Colon Rectum, vol. 45, no. 12, pp. 1689-1694, discussion 1695-1696, 2002.\\[0pt]
[272] H. H. M. Weerts, P. M. J. Van den Hof, and A. G. Dankers, "Identification of Dynamic Networks with Rank-Reduced Process Noise," in IFACPapersOnLine, 20th IFAC World Congress, 2017, pp. 10562-10567.\\[0pt]
[273] H. H. Weerts, P. M. J. Van den Hof, and A. G. Dankers, "Prediction error identification of linear dynamic networks with rank-reduced noise," Automatica, vol. 98, pp. 256-268, 2018.\\[0pt]
[274] P. Welch, "The Use of Fast Fourier Transform for the Estimation of Power Spectra: A Method Based on Time Averaging over Short, Modified Periodograms," IEEE Transactions on Audio and Electroacoustics, vol. 15, no. 2, 1967.\\[0pt]
[275] E. Wernholt and S. Gunnarsson, "Estimation of nonlinear effects in frequency domain identification of industrial robots," IEEE Transactions on Instrumentation and Measurement, vol. 57, no. 4, pp. 856-863, 2008.\\[0pt]
[276] S. Yin, S. X. Ding, X. Xie, and H. Luo, "A Review on Basic Data-Driven Approaches for Industrial Process Monitoring," IEEE Trans. Ind. Electron, vol. 61, no. 11, pp. 6418-6428, 2014.\\[0pt]
[277] P. C. Young, Recursive Estimation and Time-Series Analysis: An Introduction for the Student and Practitioner, 2nd Edition, Springer, 2012.\\[0pt]
[278] P. C. Young, "Refined instrumental variable estimation: Maximum Likelihood optimization of a unified Box-Jenkins model," Automatica, vol. 52, pp. 35-46, 2015.\\[0pt]
[279] P. C. Young, "The determination of the parameters of a dynamic process," IERE Journal of Radio and Electronic Engineering, vol. 29, pp. 345-361, 1965.\\[0pt]
[280] P. C. Young, H. Garnier, and M. Gilson, "Simple refined IV methods of closed-loop system identification," in 15th IFAC Symposium on System Identification, Saint Malo, France, 2009, pp. 1151-1156.\\[0pt]
[281] P. C. Young and A. J. Jakeman, "Refined instrumental variable methods of recursive time-series analysis Part III. Extensions," International Journal of Control, vol. 31, no. 4, pp. 741-764, 1980.\\[0pt]
[282] Y. Yuan, G.-B. Stan, S. Warnick, and J. Goncalves, "Robust Dynamical Network Structure Reconstruction," Automatica, vol. 47, no. 6, pp. 1230-1235, 2011.\\[0pt]
[283] P. Zhang and S. X. Ding, "An Integrated Trade-off Design of Observer Based Fault Detection Systems," Automatica, vol. 44, no. 7, pp. 18861894, 2008.\\[0pt]
[284] Q. Zhang, S. E. Li, and K. Deng, Automotive Air Conditioning, Cham: Springer International Publishing, 2016.\\[0pt]
[285] M. Zhong, S. X. Ding, J. Lam, and H. Wang, "An LMI Approach to Design Robust Fault Detection Filter for Uncertain LTI Systems," Automatica, 2003.\\[0pt]
[286] K. Zhou, J. Doyle, and K. Glover, Robust and optimal control, Prentice Hall, 1996.\\[0pt]
[287] T. Zhu, Y. Ran, X. Zhou, and Y. Wen, A Survey of Predictive Maintenance: Systems, Purposes and Approaches, 2024, preprint.\\[0pt]
[288] A. Zolghadri, D. Henry, J. Cieslak, D. Efimov, and P. Goupil, Fault Diagnosis and Fault-Tolerant Control and Guidance for Aerospace Vehicles: From Theory to Application, Advances in Industrial Control, London: Springer, 2014.

\section*{List of publications}
\section*{Peer-reviewed journal articles}
\begin{itemize}
  \item Koen Classens, Tjeerd Ickenroth, Jeroen van de Wijdeven, Maurice Heemels, and Tom Oomen, "Closed-Loop Optimal Fault Detection for Uncertain Systems," submitted for journal publication.
  \item Koen Classens, Tjeerd Ickenroth, Paul Tacx, Koen Tiels, Jeroen van de Wijdeven, Maurice Heemels, and Tom Oomen, "Robust Fault Detection with Application to a Prototype Wafer Stage," submitted for journal publication.
  \item Koen Classens, Stan de Rijk, Jeroen van de Wijdeven, Maurice Heemels, and Tom Oomen, "Robust $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ Fault Detection with Application to a Prototype Wafer Stage," submitted for journal publication.
  \item Koen Classens, Jeroen van de Wijdeven, Maurice Heemels, and Tom Oomen, "Nullspace-based Fault Diagnosis for Closed-Loop Mechatronic Systems with Application to Semiconductor Equipment," submitted for journal publication.
  \item Koen Classens, Rodrigo González, and Tom Oomen, "Recursive Identification of Structured Systems: An Instrumental-Variable applied to Mechanical Systems," submitted for journal publication.
  \item Rodrigo González, Koen Classens, and Tom Oomen, "Identification of Additive Continuous-time Systems in Open and Closed loop," submitted for journal publication.
  \item Koen Classens, Maarten Schoukens, Tom Oomen, and Jean-Philippe Noël, "Locating Nonlinearities in Mechanical Systems: A Frequency-Domain Dynamic Network Approach," submitted for journal publication.
\end{itemize}

\section*{Contribution to professional journal}
\begin{itemize}
  \item Koen Classens, Jeroen van de Wijdeven, Maurice Heemels, and Tom Oomen, "Opportunities of Digital Twins for High-tech Systems: From
\end{itemize}

Fault Diagnosis and Predictive Maintenance to Control Reconfiguration," Mikroniek, vol. 63, no 5, pp. 5-12, 2023.

\section*{Unrelated peer-reviewed journal articles}
\begin{itemize}
  \item Rodrigo González, Koen Classens, Cristian Rojas, Tom Oomen, and Håkan Hjalmarsson, "Finite Sample Multi-Input Multi-Output System Identification with Multisine Excitation: From Nonparametric to Parametric Models," in preparation for journal publication.
  \item Hoang Chu, Koen Classens, Sebastiaan van den Eijnden, Marcel Heertjes, and Maurice Heemels, "Filtered Projection-Based Integrator: Stability and Performance Analysis using Linear Matrix Inequalities," in preparation for journal publication.
  \item Max van Haren, Lennart Blanken, Koen Classens, and Tom Oomen, "Local Rational Modeling for Identification Beyond the Nyquist Frequency: Applied to a Prototype Wafer Stage," in preparation for journal publication.
  \item Masahiro Mae, Max van Haren, Koen Classens, Wataru Ohnishi, Tom Oomen, and Hiroshi Fujimoto, "Fixed-Structure Sampled-Data Feedforward Control for Multivariable Motion System," submitted for journal publication.
  \item Rodrigo González, Koen Classens, Cristian Rojas, James Welsh, and Tom Oomen, "Statistical Analysis of Block Coordinate Descent Algorithms for Linear Continuous-time System Identification," IEEE Control Systems Letters, vol. 8, pp. 388-393, 2024.
  \item Merijn Floren, Koen Classens, Tom Oomen, and Jean-Philippe Noel, "Feedback Linearisation of Mechanical Systems using Data-Driven Models," Journal of Sound and Vibration, vol. 577, pp. 118335, 2024.
  \item Leontine Aarnoudse, Johan Kon, Koen Classens, Max van Meer, Maurice Poot, Paul Tacx, Nard Strijbosch, and Tom Oomen, "Cross-Coupled Iterative Learning Control: A Computationally Efficient Approach Applied to an Industrial Flatbed Printer," Mechatronics, vol. 99, pp. 103170, 2024.
  \item Koen Classens, Thomas Hafkamp, Steyn Westbeek, Joris Remmers, and Siep Weiland, "Multiphysical Modeling and Optimal Control of Material Properties for Photopolymerization Processes," Additive Manufacturing, vol. 38, pp. 101520, 2021.
  \item Mohammadali Javaheri Koopaee, Christopher Pretty, Koen Classens, and XiaoQi Chen, "Dynamical Modeling and Control of Modular Snake Robots with Series Elastic Actuators for Pedal Wave Locomotion on Uneven Terrain," Journal of Mechanical Design, vol. 142, no. 3, pp. 031120, 2020.
\end{itemize}

\section*{Peer-reviewed articles in conference proceedings}
\begin{itemize}
  \item Koen Classens, Tjeerd Ickenroth, Jeroen van de Wijdeven, Maurice Heemels, and Tom Oomen, "Optimal Fault Detection for Closed-Loop Linear Uncertain Systems," in 63rd IEEE Conference on Decision and Control, Milan, Italy, 2024.
  \item Koen Classens, Maurice Heemels, and Tom Oomen, "Direct Shaping of Minimum and Maximum Singular Values: An $\mathcal{H}_{-} / \mathcal{H}_{\infty}$ Synthesis," in Proceedings of the IFAC 22st Triennial World Congress, Yokohama, Japan, 2023.
  \item Koen Classens, Mike Mostard, Jeroen van de Wijdeven, Maurice Heemels, and Tom Oomen, "Fault Detection for Precision Mechatronics: Online Estimation of Mechanical Resonances," in 2nd Modeling, Estimation and Control Conference, Jersey City, New Jersey, USA, 2022.
  \item Koen Classens, Stan Verbeek, Maurice Heemels, and Tom Oomen, "Joint Estimation of Additive and Parametric Faults: A Model-based Fault Diagnosis Approach Towards Predictive Maintenance," in 11th IFAC Symposium on Fault Detection, Supervision, and Safety of Technical Processes, Pafos, Cyprus, 2022.
  \item Koen Classens, Maurice Heemels, and Tom Oomen, "Digital Twins in Mechatronics: From Model-based Control to Predictive Maintenance," in 2021 IEEE International Conference on Digital Twins and Parallel Intelligence, Beijing, China, 2021.
  \item Koen Classens, Maurice Heemels, and Tom Oomen, "Closed-loop Aspects in MIMO Fault Diagnosis with Application to Precision Mechatronics," in 2021 IEEE American Control Conference, New Orleans, Louisiana, USA, 2021.
  \item Koen Classens, Maurice Heemels, and Tom Oomen, "A Closed-Loop Perspective on Fault Detection for Precision Motion Control: With Application to an Overactuated System," in 2021 IEEE International Conference on Mechatronics, Kashiwa, Japan, 2021.
\end{itemize}

\section*{Unrelated peer-reviewed articles in conference proceedings}
\begin{itemize}
  \item Leontine Aarnoudse, Johan Kon, Koen Classens, Max van Meer, Maurice Poot, Paul Tacx, Nard Strijbosch, and Tom Oomen, "Cross-Coupled Iterative Learning Control for Complex Systems: A Monotonically Convergent and Computationally Efficient Approach," in 61st IEEE Conference on Decision and Control, Cancun, Mexico, 2022.
  \item Koen Classens, Thomas Hafkamp, Steyn Westbeek, Joris Remmers, and Siep Weiland, "Real-Time Nonlinear Tracking Control of Photopolymer-\\
ization for Additive Manufacturing," in 2021 IEEE American Control Conference, New Orleans, Louisiana, USA, 2021.
  \item Koen Classens, Mohammadali Javaheri Koopaee, Christopher Pretty, Siep Weiland, and XiaoQi Chen, "Dynamical Modeling and Gait Optimization of a 2-D Modular Snake Robot in a Confined Space," in Proceedings of the IFAC 21st Triennial World Congress, Berlin, Germany, 2020.
  \item Mohammadali Javaheri Koopaee, Christopher Pretty, Koen Classens, and XiaoQi Chen, "Dynamical Modelling and Control of Snake-Like Motion in Vertical Plane for Locomotion in Unstructured Environments," in 15th IEEE/ASME International Conference on Mechatronic and Embedded Systems and Applications, Anaheim, California, USA, 2019.
\end{itemize}

\section*{Non peer-reviewed abstracts in conference proceedings}
\begin{itemize}
  \item Tjeerd Ickenroth, Koen Classens, Jeroen van de Wijdeven, Maurice Heemels, and Tom Oomen, "On Robust Fault Detection for Precision Mechatronics," in 43rd Benelux Meeting on Systems and Control, Blankenberge, Belgium, 2024.
  \item Rodrigo González, Koen Classens, Cristian Rojas, James Welsh, and Tom Oomen, "Additive Continuous-time Identification: With Application to Modal Mechanical Systems," in 43rd Benelux Meeting on Systems and Control, Blankenberge, Belgium, 2024.
  \item Koen Classens, Maurice Heemels, and Tom Oomen, "Fault Diagnosis for High-tech Precision Mechatronics: With Application to a Prototype Wafer Stage," in DSPE 5th Conference on Precision Mechatronics, SintMichielsgestel, The Netherlands, 2023.
  \item Koen Classens, Maurice Heemels, and Tom Oomen, "Fault Diagnosis for Precision Mechatronics," in JSPS-NWO Seminar Research Network on Learning in Machines, Tokyo, Japan, 2023.
  \item Tom Oomen, Leontine Aarnoudse, Lennart Blanken, Koen Classens, Mathyn van Dael, Nic Dirkx, Rodrigo González, Max van Haren, Johan Kon, Max van Meer, Maurice Poot, Paul Tacx, Koen Tiels, and Gert Witvoet, "Learning in Machines: From Data to Models, Control Performance, and Monitoring," in JSPS-NWO Seminar Research Network on Learning in Machines, Tokyo, Japan, 2023.
  \item Koen Classens, Maurice Heemels, and Tom Oomen, "On Nullspace-based Fault Diagnosis of Complex Mechatronic Systems," in 42nd Benelux Meeting on Systems and Control, Elspeet, The Netherlands, 2023.
  \item Koen Classens, Maurice Heemels, and Tom Oomen, "Digital Twins for Precision Mechatronics: Predictive Maintenance via Fault Detection and
\end{itemize}

Isolation," in Second Euspen Special Interest Group Meeting on Precision Motion Systems 86 Control, 's-Hertogenbosch, The Netherlands, 2022.

\begin{itemize}
  \item Koen Classens, Maurice Heemels, and Tom Oomen, "Signal-to-noise Aspects in Closed-loop Fault Identification," in 30th ERNSI Workshop on System Identification, poster presentation, Leuven, Belgium, 2022.
  \item Koen Classens, Maurice Heemels, and Tom Oomen, "On Robust Fault Diagnosis of Complex Mechatronic Systems," in 41st Benelux Meeting on Systems and Control, Brussels, Belgium, 2022.
  \item Koen Classens, Maurice Heemels, and Tom Oomen, "Identifying Faults: A Closed-loop Perspective," in 29th ERNSI Workshop on System Identification, poster presentation, Rennes, France, 2021.
  \item Koen Classens, Maurice Heemels, and Tom Oomen, "On Closed-loop Fault Diagnosis of Complex Mechatronic Systems," in Benelux Workshop on Systems and Control, Rotterdam, The Netherlands, 2021.
  \item Koen Classens, Tom Oomen, Maurice Heemels, Jeroen van de Wijdeven, Marc van de Wal, and Wouter Aangenent, "Digital Twins in Control From Fault Detection to Predictive Maintenance," in First Euspen Special Interest Group Meeting on Precision Motion Systems ${ }^{\mathcal{E}}$ Control, Veldhoven, The Netherlands, 2020.
  \item Koen Classens, Tom Oomen, Maurice Heemels, Jeroen van de Wijdeven, Marc van de Wal, and Wouter Aangenent, "Towards predictive maintenance via digital twinning: Bridging data-driven and model-based fault detection and isolation," in DSPE 4 th Conference on Precision Mechatronics, Sint Michielsgestel, The Netherlands, 2020.
\end{itemize}

\section*{Unrelated non peer-reviewed abstracts in conference proceedings}
\begin{itemize}
  \item Koen Classens, Thomas Hafkamp, Steyn Westbeek, Joris Remmers, and Siep Weiland, "Quadratic Tracking Control of Photopoly-merization for Additive Manufacturing," in 39th Benelux Meeting on Systems and Control, Elspeet, The Netherlands, 2020.
  \item Koen Classens, Mohammadali Javaheri Koopaee, and Siep Weiland, "Generalized Dynamical Modeling of 2-D Modular Snake Robots," in 38th Benelux Meeting on Systems and Control, Lommel, Belgium, 2019.
\end{itemize}

\section*{Dankwoord}
Deze pagina's vormen het afsluitende deel van dit proefschrift. Wanneer ik terugblik op de afgelopen jaren, denk ik vooral aan de vele onvergetelijke momenten en verbaas ik me hoe snel de tijd voorbij is gevlogen. Ik kan niet anders concluderen dan het een prachtige en leerzame reis is geweest, voornamelijk te danken aan de fantastische mensen om me heen. Daarom wil ik graag iedereen bedanken die op welke manier dan ook heeft bijgedragen aan mijn promotie en de weg daarnaartoe.

Tom en Maurice, onze eerste samenwerking gaat al een flink aantal jaren terug, naar onze tijd in de opleidingscommissie. Toen zich de unieke kans voordeed om een promotietraject te starten onder jullie begeleiding, heb ik die met beide handen aangegrepen. Bedankt voor het vertrouwen en de enorme vrijheid die jullie me hebben gegeven. Tom, het is ontzettend leerzaam geweest om mee te mogen kijken hoe jij je beweegt in de academische wereld. Naast je open houding en motiverend advies was er altijd ruimte om te sparren over gezamenlijke persoonlijke interesses zoals home-automation en DIY-klussen, of over zonnepanelen, warmtepompen en energiecontracten. Maurice, ik bewonder je onuitputtelijke enthousiasme en waardeer persoonlijke betrokkenheid. Jullie vullen elkaar aan waar nodig, maar hebben ook overeenkomsten, zoals de compleet met rode pen volgeschreven documenten die jullie in een mum van tijd wisten te produceren op basis van mijn steevast op het laatste moment aangeleverde drafts.

Ik wil iedereen binnen ASML research bedanken voor de prettige werkomgeving, de goede gesprekken tijdens de lunchwandelingen rondom Veldhoven, Flight Forum en de High Tech Campus, het altijd klaarstaan voor een kritische discussie en de vruchtbare voedingsbodem voor nieuwe ideeën. In het bijzonder, Jeroen, bedankt voor de mooie samenwerking, adviezen, oog voor detail, het altijd klaar staan voor studenten, en je lekker praktische insteek. Marc, Wouter, en Joost, bedankt voor het vertrouwen en het verzorgen van de organisatorische rompslomp waar nodig. George en Rohan, thanks for supporting me in the rebuild of the OAT and the move from Veldhoven to the TU/e. Tot slot, dank aan ieder die aansloot tijdens de Q-meetings om te sparren over ons onderzoek.

I would also like to express my gratitude to the other members of the PhD committee, Paul van den Hof, Bayu Jayawardhana, Riccardo Ferrari, and Daniel

Jung for reviewing this dissertation and taking part in my PhD defense ceremony.\\
Over the past years, I have had the privilege to collaborate with many fellow researchers. Jean-Philippe, Maarten, thank you for the enjoyable partnership and for introducing me to networked systems. Rodrigo, I am grateful for your guidance on SRIVC and other complex identification-related acronyms; I truly admire your theoretical expertise and look forward to continuing our fruitful collaboration. I am sure there is much more to come. Beyond this thesis, I have been fortunate to explore various related fields, and I want to thank everyone who has been a part of these experiences. In particular, I am grateful to Joris, Siep, Thomas, Steyn, and Mohammadali for sparking my interest in research during my MSc phase, giving me a taste of PhD life at the Benelux Meeting, and guiding me towards my first academic publications.

De meeste energie en voldoening haalde ik uit het begeleiden van getalenteerde en ambitieuze MSc-studenten. Stan (2x), Paul, Mike, Tjeerd en Maarten, jullie hebben een enorme bijdrage geleverd aan de totstandkoming van dit proefschrift. Naast de technische discussies en whiteboard-sessies was er ook altijd ruimte om het afgelopen en komend weekend te evalueren. Het is geweldig om te zien hoe jullie je hebben ontwikkeld en waar jullie nu terecht zijn gekomen. Jullie zijn voor mij meer dan alleen MSc-studenten; jullie zijn vrienden en collega's geworden. Merijn en Victor, hoewel ik jullie misschien niet officieel heb begeleid, heb ik altijd veel plezier beleefd aan onze experimentele sessies op het balkje en de OAT.

Dan richt ik mij tot iedereen in de kelders van Gemini. Met de CST en D\&C collega's is gezelligheid bij de koffiecorners, groepuitjes, en Benelux meetings gegarandeerd. In het bijzonder koester ik mooie herinneringen met de 'Oomen groep': Enzo, Joey, Johan, Leontine, Mathyn, Maurice, Max (2x), Nard, Nic, Noud, Paul, Tjeerd, Gert, Koen, Lennart, Rodrigo, Sebastiaan. Onze trips naar Lommel, Leuven, Cyprus, New York, Mexico, en Japan zullen me lang blijven heugen, met onder andere HoY, Coco Bongo, de begijnen, the Womb, de roadtrip door Yucatán en de beklimming van Mt. Fuji als highlights. I would like to thank our international friends Wataru, Masahiro, Kentaro, Zhihe, and Deokjin for their contribution in creating these memories. Tomas, bedankt voor de mooie reis door Mexico en de fijne samenwerking omtrent Systems Theory. Dank aan iedereen voor de fijne koffiepauzes en voorspelbare lunchwandelingen, ook dank aan alle verdwaalde CST- en D\&C'ers die zich hierbij aansloten.

Een speciaal woord van dank aan de laatste bewoners van Gemini Zuid -1.126. Johan bedankt voor je rapsessies, je enthousiasme en je interesse in alles wat met regeltechniek te maken heeft. Leontine, bedankt voor de filosofische gesprekken over carrièreperspectieven, hypotheken, en het kopen van een huis. Maurice, van de opleidingscommissie tot het samen volgen van vakken en het doorlopen van een PhD, wat hebben we een reis afgelegd. Met jou als kantoorgenoot, gevreesd dartopponent, je aanstekelijke enthousiasme en positieve instelling zijn de afgelopen jaren werkelijk voorbijgevlogen.

Niet onbelangrijk, er is natuurlijk ook een leven buiten de high-tech wereld van Eindhoven. Hertog Jetje, dank voor de afleiding in de weekenden en vakanties, de vele gezellige feestjes, en voor de pogingen om te begrijpen waar ik nu eigenlijk mee bezig was. Ook wil ik mijn familie en schoonfamilie bedanken voor alle support, interesse, en momenten van ontspanning.

Pap, mam en Sjoerd, bedankt voor jullie steun en voor het bieden van een thuis waar ik altijd op kan terugvallen. Mam, dank je voor alle zorg en ondersteuning de afgelopen jaren. Pap, ik ben blij dat je me met je boerenverstand en twee rechterhanden de belangrijke technische zaken leert die niet in de boeken staan. Ook wil ik je bedanken voor de waardevolle gesprekken over alles wat er zich in Eindhoven en in de rest van ons leven afspeelt.

Tot slot, Janine, door mijn ambities zijn voor jou de afgelopen jaren ook uitdagend geweest. Omdat die PhD nog niet genoeg was zijn we nog een mega project gestart in Venray. Bedankt voor je geduld en ontzorging waar nodig, zonder dit was het me niet gelukt. Je hebt me op de juiste momenten gemotiveerd en helpen ontspannen wanneer ik weer te veel aan de laptop zat of te druk was met klussen. Terugkijkend, ben ik trots op wat we samen hebben overwonnen en ik verheug me op wat de toekomst ons zal brengen op ons mooie plekje.

Koen Classens\\
Venray, juli 2024

\section*{Curriculum Vitae}
Koen Classens was born on September 9th, 1995 in Venray, the Netherlands. After finishing secondary education in 2013 at the Dendron College in Horst, the Netherlands, he studied Mechanical Engineering at Eindhoven University of Technology in Eindhoven, the Netherlands, where he received the Bachelor of Science degree (cum laude) in 2016. Consequently, he studied both Mechanical Engineering and Systems and Control at the Eindhoven University of Technology, where he received both Master of Science degrees (cum laude) in 2019. During his studies, he was a research intern at the University of Canterbury (New\\
\includegraphics[max width=\textwidth, center]{2025_10_13_d0faf158053d0f7500c0g-324}\\
Zealand) where he researched the mechatronic design, dynamic modeling, and motion controller design for locomotion of modular snake robots. The research for his master's thesis, entitled 'Multiphysical Modeling and Control of Photopolymerization for Additive Manufacturing' was conducted under supervision of Joris Remmers and Siep Weiland. This research contributed to a new process control paradigm where material properties are closed-loop controlled in realtime and was awarded with the MSc thesis award (Mechanical Engineering) and the Unilever Research Prize.

In 2020, Koen started his Ph.D. research in the Control Systems Technology group at the Department of Mechanical Engineering at the Eindhoven University of Technology, under the supervision of Maurice Heemels and Tom Oomen. His research is conducted in collaboration with the ASML Research Mechatronics \& Control group. The main results are included in this thesis and focus on the development of robust fault detection and isolation systems for high-precision motion applications.


\end{document}